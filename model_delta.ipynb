{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "textile-replica",
   "metadata": {},
   "source": [
    "## Simple model using Bx, By, Bz, jy, vz\n",
    " classifications: o_structures, null\n",
    " \n",
    " fixed length timeseries informed by d_per_de ~ 4\n",
    " \n",
    " similar to gamma, but with only plasmoid/non-plasmoid classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "advised-population",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/analysis/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib\n",
    "matplotlib.use('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "# get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/loss_fns.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/metrics.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "start = dt.datetime.now(dt.timezone.utc)  # for timing\n",
    "time_str = start.strftime(\"%H%M%S\")\n",
    "date_str = start.strftime(\"%d-%m-%y\")\n",
    "start_str = date_str + time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-grill",
   "metadata": {},
   "source": [
    "### Assemble a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspected-jimmy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bx (InputLayer)                 [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "by (InputLayer)                 [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bz (InputLayer)                 [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "jy (InputLayer)                 [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vz (InputLayer)                 [(None, 30, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 28, 32)       128         bx[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 28, 32)       128         by[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 28, 32)       128         bz[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 28, 32)       128         jy[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 28, 32)       128         vz[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 14, 32)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 14, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 14, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 14, 32)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 14, 32)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 14, 32)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 12, 64)       6208        average[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 6, 64)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           7700        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10, 2)        0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 10, 2)        0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,548\n",
      "Trainable params: 14,548\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "model_name = \"delta\"\n",
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "filters = 32\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "epochs = 5\n",
    "thinning_factor = [0.8, None]\n",
    "hyperparams = {'learning_rate':learning_rate, 'filters':filters, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor}\n",
    "\n",
    "# input\n",
    "bx_input = keras.Input(shape=(input_length,1), name=\"bx\") \n",
    "by_input = keras.Input(shape=(input_length,1), name=\"by\") \n",
    "bz_input = keras.Input(shape=(input_length,1), name=\"bz\") \n",
    "jy_input = keras.Input(shape=(input_length,1), name=\"jy\") \n",
    "vz_input = keras.Input(shape=(input_length,1), name=\"vz\") \n",
    "\n",
    "\n",
    "# convolve and pool separately\n",
    "bx_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid', activation='relu')(bx_input)\n",
    "by_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid', activation='relu')(by_input)\n",
    "bz_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid', activation='relu')(bz_input)\n",
    "jy_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid', activation='relu')(jy_input)\n",
    "vz_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid', activation='relu')(vz_input)\n",
    "\n",
    "bx_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(bx_conv)\n",
    "by_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(by_conv)\n",
    "bz_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(bz_conv)\n",
    "jy_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(jy_conv)\n",
    "vz_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(vz_conv)\n",
    "\n",
    "# merge the layers together\n",
    "avg = keras.layers.Average()([bx_pool, by_pool, bz_pool, jy_pool, vz_pool])\n",
    "# convolve and pool\n",
    "avg_conv = keras.layers.Conv1D(filters=2*filters, kernel_size=kernel_size, padding='valid', activation='relu')(avg)\n",
    "avg_pool = keras.layers.MaxPooling1D(pool_size=2)(avg_conv)\n",
    "\n",
    "\n",
    "# use dense layer to output\n",
    "flat_pool = keras.layers.Flatten()(avg_pool)\n",
    "flat_logits = keras.layers.Dense(stride*2, activation='relu')(flat_pool)\n",
    "logits = keras.layers.Reshape((stride, 2))(flat_logits)\n",
    "probs = keras.layers.Softmax()(logits)\n",
    "# throw together the model\n",
    "model = keras.Model(\n",
    "    inputs=[bx_input, by_input, bz_input, jy_input, vz_input],\n",
    "    outputs=[probs])\n",
    "\n",
    "# show the model\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, \"/scratch/gpfs/kendrab/model_\"+model_name+\".png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-vertical",
   "metadata": {},
   "source": [
    "### Get 1d sampling (If training/testing only, not building!)\n",
    "Generated by [1d_sampling](./1d_sampling.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heated-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    }
   ],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "basedir = '/tigress/kendrab/21032023/'\n",
    "readpaths = []\n",
    "\n",
    "for i in range(10):\n",
    "    totdir = basedir+str(i)+'/'\n",
    "    for j in range(5,60,5):\n",
    "        readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvz.hdf5\")\n",
    "\n",
    "idx_list = []  # to keep track of which file what sample came from\n",
    "s_list = []\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "jy_list = []\n",
    "vz_list = []\n",
    "x0_list = []\n",
    "x1_list = []\n",
    "topo_list = []\n",
    "\n",
    "train_idx = None\n",
    "\n",
    "for idx, filepath in enumerate(readpaths):\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_smooth'][:])\n",
    "        by_list += list(file['by'][:])\n",
    "        bz_list += list(file['bz_smooth'][:])\n",
    "        jy_list += list(file['jy'][:])\n",
    "        vz_list += list(file['vz'][:]) \n",
    "        x0_list += list(file['x0'][:])\n",
    "        x1_list += list(file['x1'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = topo_list_tmp[i] % 2  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = keras.utils.to_categorical(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "\n",
    "        if idx == 70:  # trying for 70-30 train test split\n",
    "            train_idx = len(bx_list)\n",
    "print(len(bx_list))\n",
    "# do train test split\n",
    "idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "s_train_list = s_list[:train_idx] \n",
    "bx_train_list = bx_list[:train_idx] \n",
    "by_train_list = by_list[:train_idx] \n",
    "bz_train_list = bz_list[:train_idx] \n",
    "jy_train_list = jy_list[:train_idx] \n",
    "vz_train_list = vz_list[:train_idx] \n",
    "x0_train_list = x0_list[:train_idx] \n",
    "x1_train_list = x1_list[:train_idx] \n",
    "topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "idx_test_list = idx_list[train_idx:] \n",
    "s_test_list = s_list[train_idx:] \n",
    "bx_test_list = bx_list[train_idx:] \n",
    "by_test_list = by_list[train_idx:] \n",
    "bz_test_list = bz_list[train_idx:] \n",
    "jy_test_list = jy_list[train_idx:] \n",
    "vz_test_list = vz_list[train_idx:] \n",
    "x0_test_list = x0_list[train_idx:] \n",
    "x1_test_list = x1_list[train_idx:] \n",
    "topo_test_list = topo_list[train_idx:] \n",
    "\n",
    "# BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 50-50 between train and test\n",
    "# lots of magic numbers here but we don't have time to make the code nice rn\n",
    "noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "noplasmoids_paths = []\n",
    "for j in range(5,55,5):\n",
    "        noplasmoids_paths.append(noplasmoids_dir+f\"100samples_idx{j}_bxbybzjyvz.hdf5\")\n",
    "        \n",
    "for k in range(5):\n",
    "    # training part\n",
    "    with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "        idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_train_list += list(file['s'][:])\n",
    "        bx_train_list += list(file['bx_smooth'][:])\n",
    "        by_train_list += list(file['by'][:])\n",
    "        bz_train_list += list(file['bz_smooth'][:])\n",
    "        jy_train_list += list(file['jy'][:])\n",
    "        vz_train_list += list(file['vz'][:]) \n",
    "        x0_train_list += list(file['x0'][:])\n",
    "        x1_train_list += list(file['x1'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = topo_list_tmp[i] % 2  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = keras.utils.to_categorical(topo_list_tmp[i], num_classes=2)\n",
    "        topo_train_list += topo_list_tmp    \n",
    "        \n",
    "    # testing part\n",
    "    with h5py.File(noplasmoids_paths[k+5], 'r') as file:\n",
    "        idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_test_list += list(file['s'][:])\n",
    "        bx_test_list += list(file['bx_smooth'][:])\n",
    "        by_test_list += list(file['by'][:])\n",
    "        bz_test_list += list(file['bz_smooth'][:])\n",
    "        jy_test_list += list(file['jy'][:])\n",
    "        vz_test_list += list(file['vz'][:]) \n",
    "        x0_test_list += list(file['x0'][:])\n",
    "        x1_test_list += list(file['x1'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = topo_list_tmp[i] % 2  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = keras.utils.to_categorical(topo_list_tmp[i], num_classes=2)\n",
    "        topo_test_list += topo_list_tmp        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-vermont",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "magnetic-congo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406293, 30, 1)\n",
      "Total batch: 406293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/analysis/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c8f277e5d39d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train],\n\u001b[0;32m---> 41\u001b[0;31m                          topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-44fc411820d0>\u001b[0m in \u001b[0;36mrebalance_ctrl_group\u001b[0;34m(inputs_list, labels, null_label, thinning_factor, seed)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mnon_batch_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnon_null_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnull_label_compat\u001b[0m  \u001b[0;31m# null_label_compat gets broadcast from (1,1,categorization)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mnon_null_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_null_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_batch_axes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should be 1d now. Values positive integers or 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mnull_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_null_samples\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# now we know where we need to thin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# !!! remember np.nonzero returns tuple of tuples !!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/analysis/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2242\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/analysis/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "# chunk into sliding windows\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "s_train = batch_subsects(s_train_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "bx_train = batch_subsects(bx_train_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "by_train = batch_subsects(by_train_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "bz_train = batch_subsects(bz_train_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "jy_train = batch_subsects(jy_train_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "vz_train = batch_subsects(vz_train_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "topo_train = batch_unpadded_subsects(topo_train_list, padding_length, stride)\n",
    "\n",
    "print(bx_train.shape)\n",
    "idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "s_test = batch_subsects(s_test_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "bx_test = batch_subsects(bx_test_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "by_test = batch_subsects(by_test_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "bz_test = batch_subsects(bz_test_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "jy_test = batch_subsects(jy_test_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "vz_test = batch_subsects(vz_test_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "topo_test = batch_unpadded_subsects(topo_test_list, padding_length, stride)\n",
    "\n",
    "\n",
    "idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train, topo_train = \\\n",
    "    shuffle(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test, topo_test = \\\n",
    "    shuffle(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "# (idx_train, idx_test, s_train, s_test, bx_train, bx_test, by_train, by_test, bz_train, bz_test, jy_train, jy_test, vz_train, vz_test, \n",
    "#      x0_train, x0_test, x1_train, x1_test, topo_train, topo_test) = \\\n",
    "#                        train_test_split(idx_segs, s_segs, bx_segs, by_segs, bz_segs, jy_segs, vz_segs,\n",
    "#                                         x0_segs, x1_segs, topo_segs)\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train], topo_train = \\\n",
    "    rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train],\n",
    "                         topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-symbol",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comfortable-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# loss_fn = tfa.losses.SigmoidFocalCrossEntropy(gamma=10)  # gamma must be an integer apparently (in int form)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "loss = gen_loss_per_pt(loss_fn=loss_fn)\n",
    "metric = gen_metric_per_cat()\n",
    "metrics = [\"acc\"]  # loss_fn keyword left default\n",
    "# for i in range(4):\n",
    "#     metrics.append(gen_metric_per_cat(mask_layer=mask_layer, cat_idx=i))\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics,\n",
    "             run_eagerly = True)  # run eagerly to get .numpy() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moving-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3630/3630 [==============================] - 172s 47ms/step - loss: 0.4166 - acc: 0.7813\n",
      "Epoch 2/5\n",
      "3630/3630 [==============================] - 170s 47ms/step - loss: 0.3810 - acc: 0.8323\n",
      "Epoch 3/5\n",
      "3630/3630 [==============================] - 159s 44ms/step - loss: 0.3674 - acc: 0.8437\n",
      "Epoch 4/5\n",
      "3630/3630 [==============================] - 160s 44ms/step - loss: 0.3642 - acc: 0.8450\n",
      "Epoch 5/5\n",
      "3630/3630 [==============================] - 164s 45ms/step - loss: 0.3630 - acc: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ad8efe2e5d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x={'bx': bx_train, 'by': by_train, 'bz': bz_train, 'jy': jy_train, 'vz': vz_train},\n",
    "          y = topo_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-charter",
   "metadata": {},
   "source": [
    "### Make output directories if they do not exist and set up output file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "choice-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file, cf_file, samplefile_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                model_name, date_str, time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-justice",
   "metadata": {},
   "source": [
    "### Observe the results, dump information to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "resistant-listing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance\n",
      "cat_breakdown\t\t[748348, 413022]\n",
      "[1.0, 1.0]\n",
      "[1.0, 1.0]\n",
      "Category 0 had recall 0.8930243849754333\n",
      "Category 1 had recall 0.7645403742790222\n",
      "Testing performance\n",
      "cat_breakdown\t\t[2106305, 252175]\n",
      "[1.0, 1.0]\n",
      "[1.0, 1.0]\n",
      "Category 0 had recall 0.913077175617218\n",
      "Category 1 had recall 0.7661465406417847\n"
     ]
    }
   ],
   "source": [
    "with open(log_file, 'w') as log:\n",
    "    log.write(f\"Model {model_name} trained on {start_str}\\n\")\n",
    "    log.write(f\"loss function \\t\\t{loss.__name__}\\n\")\n",
    "    log.write(\"Hyperparameters:\\n\")\n",
    "    for key in hyperparams.keys():\n",
    "        log.write(f\"{key}\\t\\t{hyperparams[key]}\\n\")\n",
    "        \n",
    "    log.write(\"Training performance\\n\")        \n",
    "    print(\"Training performance\")\n",
    "    train_topo_pred = model(inputs={'bx': bx_train, 'by': by_train, 'bz': bz_train, 'jy': jy_train, 'vz': vz_train}, training=False)\n",
    "    train_1d = np.argmax(topo_train.reshape(-1,2), axis=1) # for confusion matrix\n",
    "    train_1d_pred = np.argmax(train_topo_pred.numpy().reshape(-1,2), axis=1)  \n",
    "    num_per_cat = [np.sum(topo_train[...,i] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "    print([np.max(topo_train[:,:,i]) for i in range(2)])\n",
    "    print([np.max(train_topo_pred[:,:,i]) for i in range(2)])\n",
    "    for i in range(2):\n",
    "        acc = gen_metric_per_cat(cat_idx=i)(tf.convert_to_tensor(topo_train), train_topo_pred)\n",
    "        log.write(f\"cat{i}recall\\t\\t{acc}\\n\")\n",
    "        print(f\"Category {i} had recall {acc}\")\n",
    "\n",
    "    log.write(\"Testing performance\\n\")\n",
    "    print(\"Testing performance\")\n",
    "    test_topo_pred = model(inputs={'bx': bx_test, 'by': by_test, 'bz': bz_test, 'jy': jy_test, 'vz': vz_test}, training=False)\n",
    "    test_1d = np.argmax(topo_test.reshape(-1,2), axis=1) # for confusion matrix\n",
    "    test_1d_pred = np.argmax(test_topo_pred.numpy().reshape(-1,2), axis=1)  \n",
    "    num_per_cat = [np.sum(topo_test[...,i] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "    print([np.max(topo_test[:,:,i]) for i in range(2)])\n",
    "    print([np.max(test_topo_pred[:,:,i]) for i in range(2)])\n",
    "    for i in range(2):\n",
    "        acc = gen_metric_per_cat(cat_idx=i)(tf.convert_to_tensor(topo_test), test_topo_pred)\n",
    "        log.write(f\"cat{i}recall\\t\\t{acc}\\n\")\n",
    "        print(f\"Category {i} had recall {acc}\")    \n",
    "    end = dt.datetime.now(dt.timezone.utc)    \n",
    "    log.write(f\"runtime_seconds\\t\\t{(end-start).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-tomorrow",
   "metadata": {},
   "source": [
    "### Save confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "historical-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_traintest_cf_matrices(train_1d, train_1d_pred, test_1d, test_1d_pred, cf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-concert",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "familiar-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(samplefile_start+\"_modelfile.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-trailer",
   "metadata": {},
   "source": [
    "### Plot summaries of a selection of segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suspended-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reps([bx_train, by_train, bz_train, jy_train, vz_train], ['bx','by','bz','jy','vz'], s_train, topo_train, train_topo_pred, \n",
    "          samplefile_start, inputs_padding=padding_length, true_coords=np.stack([x0_train, x1_train], axis=-1), exs_per_cat=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-clone",
   "metadata": {},
   "source": [
    "### Put temporary stuff here, delete later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dirty-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2)\n",
    "# ax[0].set(title=\"Training Confusion\")\n",
    "# ax[1].set(title=\"Testing Confusion\")\n",
    "# cf_train_p = ConfusionMatrixDisplay(confusion_matrix(train_1d, train_1d_pred))\n",
    "# cf_test_p = ConfusionMatrixDisplay(confusion_matrix(test_1d, test_1d_pred))\n",
    "# cf_train_p.plot(ax=ax[0], colorbar=None, values_format='d', cmap='Greys')\n",
    "# cf_test_p.plot(ax=ax[1], colorbar=None, values_format='d', cmap='Greys')\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(\"/tigress/kendrab/analysis-notebooks/model_outs/\" + date_str + \"/deltacfmatrix\"+time_str+\"_nonnormal.svg\")\n",
    "# plt.close(fig='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "annual-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161370\n",
      "2358480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.352552790720729"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(748348+413022)\n",
    "print(2106305+252175)\n",
    "2106305/252175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-factor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
