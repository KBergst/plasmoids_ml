{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stock-french",
   "metadata": {},
   "source": [
    "### Testing optuna with a model, trying not to hog resources on tigress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from optuna.trial import TrialState\n",
    "import matplotlib.pyplot as plt\n",
    "# get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/loss_fns.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/metrics.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-testimony",
   "metadata": {},
   "source": [
    "### Optimize via optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    \"\"\" Assemble a model\"\"\"\n",
    "    #######################\n",
    "    model_name = \"zeta\"\n",
    "    # hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True)\n",
    "    filters = trial.suggest_categorical(\"filters\", [16,32])\n",
    "    kernel_size = 3\n",
    "    pool_size = 2\n",
    "    padding_length = trial.suggest_int(\"padding_length\", 10, 30)  \n",
    "                                       # amount of data on each side of each segment for additional info\n",
    "    stride = trial.suggest_int(\"stride\", 1, 7)  # size (and therefore spacing) of each segment\n",
    "    input_length = stride + 2*padding_length\n",
    "    mask_value = int(-10.0)\n",
    "    epochs = 1\n",
    "    thinning_factor = [.9, None]\n",
    "    hyperparams = {'learning_rate':learning_rate, 'filters':filters, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "                  'padding_length':padding_length, 'stride':stride, 'input_length':input_length, 'epochs':epochs,\n",
    "                   'thinning_factor':thinning_factor}\n",
    "\n",
    "    # input\n",
    "    bx_input = keras.Input(shape=(input_length,1), name=\"bx\") \n",
    "    by_input = keras.Input(shape=(input_length,1), name=\"by\") \n",
    "    bz_input = keras.Input(shape=(input_length,1), name=\"bz\") \n",
    "    jy_input = keras.Input(shape=(input_length,1), name=\"jy\") \n",
    "    vz_input = keras.Input(shape=(input_length,1), name=\"vz\") \n",
    "\n",
    "    # convolve and pool separately\n",
    "    bx_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(bx_input)\n",
    "    by_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(by_input)\n",
    "    bz_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(bz_input)\n",
    "    jy_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(jy_input)\n",
    "    vz_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(vz_input)\n",
    "\n",
    "    bx_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(bx_conv)\n",
    "    by_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(by_conv)\n",
    "    bz_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(bz_conv)\n",
    "    jy_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(jy_conv)\n",
    "    vz_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(vz_conv)\n",
    "\n",
    "    # merge the layers together\n",
    "    avg = keras.layers.Average()([bx_pool, by_pool, bz_pool, jy_pool, vz_pool])\n",
    "    # convolve and pool\n",
    "    avg_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(avg)\n",
    "    avg_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(avg_conv)\n",
    "\n",
    "\n",
    "    # use dense layer to output\n",
    "    flat_pool = keras.layers.Flatten()(avg_pool)\n",
    "    flat_logits = keras.layers.Dense(stride*2, activation='relu')(flat_pool)\n",
    "    logits = keras.layers.Reshape((stride, 2))(flat_logits)\n",
    "    probs = keras.layers.Softmax()(logits)\n",
    "    # throw together the model\n",
    "    model = keras.Model(\n",
    "        inputs=[bx_input, by_input, bz_input, jy_input, vz_input],\n",
    "        outputs=[probs])\n",
    "\n",
    "    # show the model\n",
    "    model.summary()\n",
    "    keras.utils.plot_model(model, \"/scratch/gpfs/kendrab/model_\"+model_name+\".png\", show_shapes=True)\n",
    "\n",
    "\n",
    "    \"\"\"Get 1d sampling\"\"\"\n",
    "    #####################\n",
    "    readpaths = ['/tigress/kendrab/03082021/'+\"1000samples_idx22_bxbybzjyvz.hdf5\",\n",
    "                 '/tigress/kendrab/03082021/'+\"1000samples_idx18_bxbybzjyvz.hdf5\",\n",
    "                 '/tigress/kendrab/03082021/'+\"1000samples_idx15_bxbybzjyvz.hdf5\"]\n",
    "\n",
    "    idx_list = []  # to keep track of which file what sample came from\n",
    "    s_list = []\n",
    "    bx_list = []\n",
    "    by_list = []\n",
    "    bz_list = []\n",
    "    jy_list = []\n",
    "    vz_list = []\n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    topo_list = []\n",
    "\n",
    "    for idx, filepath in enumerate(readpaths):\n",
    "        file = h5py.File(filepath, 'r')\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_smooth'][:])\n",
    "        by_list += list(file['by'][:])\n",
    "        bz_list += list(file['bz_smooth'][:])\n",
    "        jy_list += list(file['jy'][:])\n",
    "        vz_list += list(file['vz'][:]) \n",
    "        x0_list += list(file['x0'][:])\n",
    "        x1_list += list(file['x1'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = topo_list_tmp[i] % 2  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = keras.utils.to_categorical(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "        file.close()\n",
    "\n",
    "\n",
    "    \"\"\" Preprocess data\"\"\"\n",
    "    ######################\n",
    "    idx_segs = batch_unpadded_subsects(idx_list, padding_length, stride)\n",
    "    s_segs = batch_subsects(s_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "    bx_segs = batch_subsects(bx_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "    by_segs = batch_subsects(by_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "    bz_segs = batch_subsects(bz_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "    jy_segs = batch_subsects(jy_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "    vz_segs = batch_subsects(vz_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "    x0_segs = batch_unpadded_subsects(x0_list, padding_length, stride)\n",
    "    x1_segs = batch_unpadded_subsects(x1_list, padding_length, stride)\n",
    "    topo_segs = batch_unpadded_subsects(topo_list, padding_length, stride)\n",
    "\n",
    "    (idx_train, idx_test, s_train, s_test, bx_train, bx_test, by_train, by_test, bz_train, bz_test, jy_train, jy_test, vz_train, vz_test, \n",
    "         x0_train, x0_test, x1_train, x1_test, topo_train, topo_test) = \\\n",
    "                           train_test_split(idx_segs, s_segs, bx_segs, by_segs, bz_segs, jy_segs, vz_segs,\n",
    "                                            x0_segs, x1_segs, topo_segs)\n",
    "    # try to do some rebalancing in the training set\n",
    "    # model is struggling on plasmoids, which are underrepresented\n",
    "    [idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train], topo_train = \\\n",
    "        rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train],\n",
    "                             topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n",
    "    [idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test], topo_test = \\\n",
    "        rebalance_ctrl_group([idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test],\n",
    "                             topo_test, null_label=[1,0], thinning_factor = thinning_factor[0])\n",
    "    \n",
    "\n",
    "    \"\"\" Compile and train model \"\"\"\n",
    "    ###############################\n",
    "    weights = {i:np.sum(topo_train)/np.sum(topo_train[...,i]) for i in range(topo_train.shape[-1])}\n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "    loss = LossPerPt(loss_fn=loss_fn, class_weights=weights)\n",
    "    metric = gen_metric_per_cat()\n",
    "    metrics = [\"acc\", keras.metrics.AUC(name='prc', curve='PR'), keras.metrics.Precision(class_id=1), avg_metric(keras.metrics.Precision)]  # loss_fn keyword left default\n",
    "    # for i in range(4):\n",
    "    #     metrics.append(gen_metric_per_cat(mask_layer=mask_layer, cat_idx=i))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=metrics,\n",
    "                 run_eagerly = True)  # run eagerly to get .numpy() method\n",
    "\n",
    "    model.fit(x={'bx': bx_train, 'by': by_train, 'bz': bz_train, 'jy': jy_train, 'vz': vz_train},\n",
    "              y = topo_train, epochs=epochs, callbacks=[TFKerasPruningCallback(trial, \"prc\")])\n",
    "    score = model.evaluate(x={'bx': bx_test, 'by': by_test, 'bz': bz_test, 'jy': jy_test, 'vz': vz_test}, y=topo_test, verbose=0,\n",
    "                          return_dict=True)\n",
    "    return score[\"prc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-engineering",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=2))\n",
    "study.optimize(objective, n_trials=10, timeout=1800)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "ax = optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vulnerable-brunei",
   "metadata": {},
   "source": [
    "Best trial:\n",
    "  Value: 0.9498381018638611\n",
    "  Params: \n",
    "    learning_rate: 0.0123973459832124\n",
    "    filters: 32\n",
    "    padding_length: 30\n",
    "    stride: 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
