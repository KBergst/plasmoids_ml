{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e030ff-19ef-4da5-a052-98905278507f",
   "metadata": {},
   "source": [
    "## Simple model using Bx, By, Bz, Ex, Ey, Ez, jy\n",
    "classifications: o_structures, null\n",
    "\n",
    "fixed length timeseries informed by d_per_de ~ 4\n",
    "\n",
    "Model E, but with Optuna optimization put in and modified number of layers\n",
    "\n",
    "Also changing output to *actual* logits (-logit, logit) instead of needing softmax to get probability see https://stats.stackexchange.com/questions/207049/neural-network-for-binary-classification-use-1-or-2-output-neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a10f508-9e73-4a91-a634-8fc144de904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "import torcheval.metrics as tem  # YAY METRICS\n",
    "import optuna\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dtype = torch.double\n",
    "   \n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "\n",
    "start = dt.datetime.now(dt.timezone.utc)  # for timing\n",
    "time_str = start.strftime(\"%H%M%S\")\n",
    "date_str = start.strftime(\"%d-%m-%y\")\n",
    "start_str = date_str + time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045a9c8-4242-4aa1-ad0a-e08d2d256305",
   "metadata": {},
   "source": [
    "### Define the training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ad40ec-e97e-47c2-9e6c-39ef5f6049a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)  # the length of a tensordataset is the shared first dim\n",
    "    for batch, (_, _, bx, by, bz, ex, ey, ez, jy, _, _, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(bx, by, bz, ex, ey, ez, jy)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current_sample = (batch+1)*bx.shape[0]\n",
    "            print(f\"mean loss: {loss}, sample {current_sample}/{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b56ed98-c90f-4df8-8df9-2889bea3529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, threshold = 0.5):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    size = len(dataloader.dataset)  # number of samples\n",
    "    stride = dataloader.dataset[0][0].shape[-1]\n",
    "    tot_points = size*stride\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss_sum, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, bx, by, bz, ex, ey, ez, jy, _, _, y in dataloader:\n",
    "            pred = model(bx, by, bz, ex, ey, ez, jy)\n",
    "            pred_list.append(pred.cpu().numpy())\n",
    "            test_loss_sum += loss_fn(pred, y).item()  # .item() fetches the python scalar\n",
    "            # number of correct per-point predictions\n",
    "            correct += ((pred > threshold) == y).type(torch.float).sum().item()\n",
    "    tot_pred = np.concatenate(pred_list, axis=0)\n",
    "    test_loss_sum /= num_batches\n",
    "    correct /= tot_points\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss_sum:>8f} \\n\")    \n",
    "    return tot_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f832b49-5f69-4070-bf10-30f597c2951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_layers_n_times(layer_list, n):  # this instead of something simpler to be absolutely sure the layers are different objects and not repeating the same one\n",
    "    new_layer_list = []\n",
    "    for i in range(n):\n",
    "        for layer in layer_list:\n",
    "            new_layer_list.append(copy.deepcopy(layer))\n",
    "    return new_layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5971f091-5d80-4fd9-ad65-f27dad363625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    ''' MAKE THE MODEL'''\n",
    "    model_name = \"F\"\n",
    "    # hyperparameters\n",
    "    padding_length = trial.suggest_int('padding_length', 10, 40)  # amount of data on each side of each segment for additional info\n",
    "    stride = trial.suggest_int('stride', 10, 40)  # size (and therefore spacing) of each segment\n",
    "    input_length = stride + 2*padding_length\n",
    "    num_conv = trial.suggest_int('num_conv', 1, 2)  # for separate layers but also for end layer (so really there are num_conv*2 layers)\n",
    "    kp_limit = int(input_length**(1/(num_conv+1)))  # a non-maximal upper bound on sizes to avoid running out of length\n",
    "    print(f\"kernel eqn limit kp={kp_limit}\")\n",
    "    kernel_size = trial.suggest_int('kernel_size', 2, min(kp_limit - 1, 10))  # max size 10 or lower\n",
    "    pool_size = trial.suggest_int('pool_size', 1, min(kp_limit - kernel_size, 5))\n",
    "    out_channels = trial.suggest_int('out_channels', 8, 64)  # like 'filters' in keras\n",
    "    thinning_factor = trial.suggest_float('thinning_factor', 0.5, 0.95)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.2, log=True)\n",
    "    dropout_fraction = trial.suggest_float('dropout', 0, 0.3)\n",
    "    epochs = 1 \n",
    "    batch_size = trial.suggest_int('batch_size', 4, 256)  # idk what this should be for best performance \n",
    "    \n",
    "    # padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "    # stride = 10  # size (and therefore spacing) of each segment\n",
    "    # input_length = stride + 2*padding_length\n",
    "    # kp_limit = None\n",
    "    # num_conv = 1  # for separate layers but also for end layer (so really there are num_conv*2 layers)\n",
    "    # kernel_size = 3  # max size 10 or lower\n",
    "    # pool_size = 2\n",
    "    # out_channels = 16  # like 'filters' in keras\n",
    "    # thinning_factor = 0.9\n",
    "    # learning_rate = 0.01\n",
    "    # dropout_fraction = 0\n",
    "    # epochs = 3 \n",
    "    # batch_size = 128  # idk what this should be for best performance \n",
    "    \n",
    "    \n",
    "    hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "                  'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor,\n",
    "                  'batch_size':batch_size, 'num_conv':num_conv, 'dropout_fraction':dropout_fraction, 'kp_limit':kp_limit}\n",
    "\n",
    "\n",
    "\n",
    "    #TODO feed hyperparameters into __init__\n",
    "    class ModelF(nn.Module):\n",
    "        \"\"\" 1D CNN Model \"\"\"\n",
    "        def __init__(self):  # TODO change optuna params from globals to inputs to init\n",
    "            super().__init__()\n",
    "            # define these all separately because they will get different weights\n",
    "            # consider smooshing these together into one convolution with in_channels=6. Idk if a good idea\n",
    "            self.bx_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "            self.by_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "            self.bz_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "            self.ex_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "            self.ey_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "            self.ez_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "            self.jy_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                           nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "\n",
    "            self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                                         padding='valid'), nn.ReLU(),\n",
    "                                                               nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction),\n",
    "                                                    nn.Flatten(),\n",
    "                                                    nn.LazyLinear(stride))\n",
    "\n",
    "\n",
    "        def forward(self, bx, by, bz, ex, ey, ez, jy):\n",
    "            bx_proc = self.bx_layers(bx)\n",
    "            by_proc = self.by_layers(by)\n",
    "            bz_proc = self.bz_layers(bz)\n",
    "            ex_proc = self.ex_layers(ex)\n",
    "            ey_proc = self.ey_layers(ey)\n",
    "            ez_proc = self.ez_layers(ez)\n",
    "            jy_proc = self.jy_layers(jy)\n",
    "            combined = (bx_proc + by_proc + bz_proc + ex_proc + ey_proc + ez_proc + jy_proc)/6.\n",
    "            logits = self.post_merge_layers(combined)\n",
    "\n",
    "            return logits\n",
    "\n",
    "\n",
    "    model = ModelF().to(device=device, dtype=torch.double)\n",
    "    print(model)\n",
    "\n",
    "    ''' LOAD AND PREPROCESS THE DATA'''\n",
    "    basedir = '/tigress/kendrab/21032023/'\n",
    "    readpaths = []\n",
    "\n",
    "    for i in range(10):\n",
    "        totdir = basedir+str(i)+'/'+'new_better/'\n",
    "        for j in range(5,60,5):\n",
    "            readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "\n",
    "    idx_list = []  # to keep track of which file what sample came from\n",
    "    s_list = []\n",
    "    bx_list = []\n",
    "    by_list = []\n",
    "    bz_list = []\n",
    "    ex_list = []\n",
    "    ey_list = []\n",
    "    ez_list = []\n",
    "    jy_list = []\n",
    "    x0_list = []\n",
    "    x1_list = []\n",
    "    topo_list = []\n",
    "\n",
    "    train_idx = None\n",
    "    test_idx = None\n",
    "\n",
    "    for idx, filepath in enumerate(readpaths):\n",
    "        with h5py.File(filepath, 'r') as file:\n",
    "            idx_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "            s_list += list(file['s'][:])\n",
    "            bx_list += list(file['bx_mms_smooth'][:])\n",
    "            by_list += list(file['by_mms'][:])\n",
    "            bz_list += list(file['bz_mms_smooth'][:])\n",
    "            ex_list += list(file['ex_mms'][:]) \n",
    "            ey_list += list(file['ey_mms'][:])\n",
    "            ez_list += list(file['ez_mms'][:])  # vx_mms is simulation vz  thus the filename \n",
    "            jy_list += list(file['jy_mms'][:])\n",
    "            x0_list += list(file['x_mms'][:])\n",
    "            x1_list += list(file['z_mms'][:])\n",
    "            topo_list_tmp = list(file['topo'][:])\n",
    "            for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "                topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list += topo_list_tmp\n",
    "\n",
    "            if idx == int(.7*len(readpaths)):  # roughly 70-20-10 train-test-validation split for now.\n",
    "                train_idx = len(bx_list)\n",
    "            elif idx == int(.9*len(readpaths)):\n",
    "                test_idx = len(bx_list)\n",
    "\n",
    "    print(len(bx_list))\n",
    "    # do train test valid split NOT PROCESSING VALIDATION NOW BC DO THAT AT VERY END OF HYPERPARAMETER OPTIMIZATION.\n",
    "    # RUN 9 IS FOR VALIDATION ONLY\n",
    "    idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "    s_train_list = s_list[:train_idx] \n",
    "    bx_train_list = bx_list[:train_idx] \n",
    "    by_train_list = by_list[:train_idx] \n",
    "    bz_train_list = bz_list[:train_idx] \n",
    "    ex_train_list = ex_list[:train_idx] \n",
    "    ey_train_list = ey_list[:train_idx] \n",
    "    ez_train_list = ez_list[:train_idx] \n",
    "    jy_train_list = jy_list[:train_idx] \n",
    "    x0_train_list = x0_list[:train_idx] \n",
    "    x1_train_list = x1_list[:train_idx] \n",
    "    topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "    idx_test_list = idx_list[train_idx:test_idx] \n",
    "    s_test_list = s_list[train_idx:test_idx] \n",
    "    bx_test_list = bx_list[train_idx:test_idx] \n",
    "    by_test_list = by_list[train_idx:test_idx] \n",
    "    bz_test_list = bz_list[train_idx:test_idx] \n",
    "    ex_test_list = ex_list[train_idx:test_idx] \n",
    "    ey_test_list = ey_list[train_idx:test_idx] \n",
    "    ez_test_list = ez_list[train_idx:test_idx]\n",
    "    jy_test_list = jy_list[train_idx:test_idx]\n",
    "    x0_test_list = x0_list[train_idx:test_idx] \n",
    "    x1_test_list = x1_list[train_idx:test_idx] \n",
    "    topo_test_list = topo_list[train_idx:test_idx]     \n",
    "\n",
    "    print(len(bx_train_list))\n",
    "    print(len(bx_test_list))\n",
    "    # BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 70-20-10 train-test-validation\n",
    "    # lots of magic numbers here but we don't have time to make the code nice rn\n",
    "    noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "    noplasmoids_paths = []\n",
    "    for j in range(5,55,5):\n",
    "            noplasmoids_paths.append(noplasmoids_dir+'new_better/'+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "\n",
    "    for k in range(10):\n",
    "        # training part\n",
    "        if k < 7:\n",
    "            with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "                idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "                s_train_list += list(file['s'][:])\n",
    "                bx_train_list += list(file['bx_mms_smooth'][:])\n",
    "                by_train_list += list(file['by_mms'][:])\n",
    "                bz_train_list += list(file['bz_mms_smooth'][:])\n",
    "                ex_train_list += list(file['ex_mms'][:]) \n",
    "                ey_train_list += list(file['ey_mms'][:])\n",
    "                ez_train_list += list(file['ez_mms'][:]) \n",
    "                jy_train_list += list(file['jy_mms'][:])\n",
    "                x0_train_list += list(file['x_mms'][:])\n",
    "                x1_train_list += list(file['z_mms'][:])\n",
    "                topo_list_tmp = list(file['topo'][:])\n",
    "                for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "                    topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "                topo_train_list += topo_list_tmp    \n",
    "        \n",
    "        elif k < 9:\n",
    "            # testing part\n",
    "            with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "                idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "                s_test_list += list(file['s'][:])\n",
    "                bx_test_list += list(file['bx_mms_smooth'][:])\n",
    "                by_test_list += list(file['by_mms'][:])\n",
    "                bz_test_list += list(file['bz_mms_smooth'][:])\n",
    "                ex_test_list += list(file['ex_mms'][:]) \n",
    "                ey_test_list += list(file['ey_mms'][:])\n",
    "                ez_test_list += list(file['ez_mms'][:])\n",
    "                jy_test_list += list(file['jy_mms'][:])\n",
    "                x0_test_list += list(file['x_mms'][:])\n",
    "                x1_test_list += list(file['z_mms'][:])\n",
    "                topo_list_tmp = list(file['topo'][:])\n",
    "                for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "                    topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "                topo_test_list += topo_list_tmp           \n",
    "        # validation set is not processed at this time\n",
    "    print(len(bx_train_list))\n",
    "    print(len(bx_test_list))\n",
    "\n",
    "    '''CHONK UP THE DATA'''\n",
    "    # chunk into sliding windows\n",
    "    # NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "    idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "    s_train = batch_subsects(s_train_list, input_length, stride)  # not going through training so don't need to shape right\n",
    "    bx_train = np.expand_dims(batch_subsects(bx_train_list, input_length, stride),1)\n",
    "    by_train = np.expand_dims(batch_subsects(by_train_list, input_length, stride),1)\n",
    "    bz_train = np.expand_dims(batch_subsects(bz_train_list, input_length, stride),1)\n",
    "    ex_train = np.expand_dims(batch_subsects(ex_train_list, input_length, stride),1)\n",
    "    ey_train = np.expand_dims(batch_subsects(ey_train_list, input_length, stride),1)\n",
    "    ez_train = np.expand_dims(batch_subsects(ez_train_list, input_length, stride),1)\n",
    "    jy_train = np.expand_dims(batch_subsects(jy_train_list, input_length, stride),1)\n",
    "    x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "    x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "    topo_train = batch_unpadded_subsects(topo_train_list, padding_length, stride)\n",
    "\n",
    "    print(bx_train.shape)\n",
    "\n",
    "    idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "    s_test = np.expand_dims(batch_subsects(s_test_list, input_length, stride),1)\n",
    "    bx_test = np.expand_dims(batch_subsects(bx_test_list, input_length, stride),1)\n",
    "    by_test = np.expand_dims(batch_subsects(by_test_list, input_length, stride),1)\n",
    "    bz_test = np.expand_dims(batch_subsects(bz_test_list, input_length, stride),1)\n",
    "    ex_test = np.expand_dims(batch_subsects(ex_test_list, input_length, stride),1)\n",
    "    ey_test = np.expand_dims(batch_subsects(ey_test_list, input_length, stride),1)\n",
    "    ez_test = np.expand_dims(batch_subsects(ez_test_list, input_length, stride),1)\n",
    "    jy_test = np.expand_dims(batch_subsects(jy_test_list, input_length, stride),1)\n",
    "    x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "    x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "    topo_test = batch_unpadded_subsects(topo_test_list, padding_length, stride)\n",
    "\n",
    "    # shuffle the segments so they aren't adjacent to overlapping/similar segments\n",
    "    idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train, topo_train = \\\n",
    "        shuffle(idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "    idx_test, s_test, bx_test, by_test, bz_test, ex_test, ey_test, ez_test, jy_test, x0_test, x1_test, topo_test = \\\n",
    "        shuffle(idx_test, s_test, bx_test, by_test, bz_test, ex_test, ey_test, ez_test, jy_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "    # try to do some rebalancing in the training set\n",
    "    # model is struggling on plasmoids, which are underrepresented\n",
    "    [idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train], topo_train = \\\n",
    "        rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train],\n",
    "                             topo_train, null_label=0, thinning_factor = thinning_factor)  # TODO fix this\n",
    "    # numpy arrays to torch tensors (while crying about how many lines of code this is surely there is a better way)\n",
    "    idx_train = torch.from_numpy(idx_train).to(device, dtype=dtype)\n",
    "    s_train = torch.from_numpy(s_train).to(device, dtype=dtype)\n",
    "    bx_train = torch.from_numpy(bx_train).to(device, dtype=dtype)\n",
    "    by_train = torch.from_numpy(by_train).to(device, dtype=dtype)\n",
    "    bz_train = torch.from_numpy(bz_train).to(device, dtype=dtype)\n",
    "    ex_train = torch.from_numpy(ex_train).to(device, dtype=dtype)\n",
    "    ey_train = torch.from_numpy(ey_train).to(device, dtype=dtype)\n",
    "    ez_train = torch.from_numpy(ez_train).to(device, dtype=dtype)\n",
    "    jy_train = torch.from_numpy(jy_train).to(device, dtype=dtype)\n",
    "    x0_train = torch.from_numpy(x0_train).to(device, dtype=dtype)\n",
    "    x1_train = torch.from_numpy(x1_train).to(device, dtype=dtype)\n",
    "    topo_train = torch.from_numpy(topo_train).to(device, dtype=dtype)\n",
    "\n",
    "    idx_test = torch.from_numpy(idx_test).to(device, dtype=dtype)\n",
    "    s_test = torch.from_numpy(s_test).to(device, dtype=dtype)\n",
    "    bx_test = torch.from_numpy(bx_test).to(device, dtype=dtype)\n",
    "    by_test = torch.from_numpy(by_test).to(device, dtype=dtype)\n",
    "    bz_test = torch.from_numpy(bz_test).to(device, dtype=dtype)\n",
    "    ex_test = torch.from_numpy(ex_test).to(device, dtype=dtype)\n",
    "    ey_test = torch.from_numpy(ey_test).to(device, dtype=dtype)\n",
    "    ez_test = torch.from_numpy(ez_test).to(device, dtype=dtype)\n",
    "    jy_test = torch.from_numpy(jy_test).to(device, dtype=dtype)\n",
    "    x0_test = torch.from_numpy(x0_test).to(device, dtype=dtype)\n",
    "    x1_test = torch.from_numpy(x1_test).to(device, dtype=dtype)\n",
    "    topo_test = torch.from_numpy(topo_test).to(device, dtype=dtype)\n",
    "\n",
    "    # collect data into Datasets\n",
    "    train_dset = TensorDataset(idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train,\n",
    "                                  jy_train, x0_train, x1_train, topo_train)\n",
    "    test_dset =  TensorDataset(idx_test, s_test, bx_test, by_test, bz_test, ex_test, ey_test, ez_test,\n",
    "                                  jy_test, x0_test, x1_test, topo_test)\n",
    "    # Make DataLoaders for the training and test data\n",
    "    train_dl = DataLoader(train_dset, batch_size = batch_size)\n",
    "    test_dl = DataLoader(test_dset, batch_size = batch_size)\n",
    "\n",
    "    '''COMPILE AND TRAIN MODEL'''\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='mean')  # We are allowing reduction bc backward() \n",
    "                                                # needs a scalar or to specify a different gradient.\n",
    "                                                    # Easier this way. Probably.\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    metric = tem.BinaryAUPRC(device=device)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dl, model, loss_fn, opt)\n",
    "        test_probs = torch.nn.functional.sigmoid(torch.Tensor(test_loop(test_dl, model, loss_fn))).flatten().to(device)\n",
    "        test_classes = topo_test.flatten()  # labels aka true probability point is class 1\n",
    "        metric.update(test_probs, test_classes)\n",
    "        trial.report(metric.compute(), t)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    # save if we are retrieving a specific trial\n",
    "    if type(trial) is optuna.trial.FrozenTrial:\n",
    "        # Make output directories if they do not exist and set up output file names\n",
    "        log_file, cf_file, samplefile_start, file_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                model_name, date_str, time_str)\n",
    "        # Dump information to file\n",
    "        with open(log_file, 'w') as log:\n",
    "            log.write(f\"Model {model_name} trained on {start_str}\\n\")\n",
    "            log.write(f\"loss function \\t\\t{loss_fn.__repr__()}\\n\")\n",
    "            log.write(\"Hyperparameters:\\n\")\n",
    "            for key in hyperparams.keys():\n",
    "                log.write(f\"{key}\\t\\t{hyperparams[key]}\\n\")\n",
    "\n",
    "            log.write(\"Training performance\\n\")     \n",
    "            print(\"Training performance\")\n",
    "            train_1d_pred = test_loop(train_dl, model, loss_fn).flatten() \n",
    "            train_1d = topo_train.cpu().numpy().flatten() # for confusion matrix \n",
    "            num_per_cat = [np.sum(train_1d == i) for i in range(2)]\n",
    "            log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "            print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "\n",
    "            log.write(\"Testing performance\\n\")\n",
    "            print(\"Testing performance\")\n",
    "            test_1d_pred = test_loop(test_dl, model, loss_fn).flatten()\n",
    "            test_1d = topo_test.cpu().numpy().flatten() # for confusion matrix \n",
    "            num_per_cat = [np.sum(test_1d == i) for i in range(2)]\n",
    "            log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "            print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "\n",
    "            end = dt.datetime.now(dt.timezone.utc)    \n",
    "            log.write(f\"runtime_seconds\\t\\t{(end-start).total_seconds()}\")\n",
    "            \n",
    "        # Plot confusion matrices\n",
    "        train_1d_pred_int = train_1d_pred > 0  # must be more complicated if using threshold != 0.5\n",
    "        test_1d_pred_int = test_1d_pred > 0 \n",
    "        plt_traintest_cf_matrices(train_1d, train_1d_pred_int, test_1d, test_1d_pred_int, cf_file)\n",
    "        # plot precision-recall curve for test.\n",
    "        test_1d_prob = torch.nn.functional.sigmoid(torch.Tensor(test_1d_pred).to(device))  # to probability rather than logit to see if torcheval really supports logits for this function (it doesn't seem like they do)\n",
    "        precision, recall, thresh = tem.functional.binary_binned_precision_recall_curve(test_1d_prob.to(device),\n",
    "                                                                                        torch.Tensor(test_1d).to(device), threshold=100)\n",
    "        # last point for thresh = 1 is repeated, so we delete it\n",
    "        plot_prc(precision[:-1], recall[:-1], thresh, file_start+\"prc_test\", title=\"Precision-Recall curve, test\")\n",
    "        \n",
    "        # Save model\n",
    "        torch.save({\n",
    "                    'epoch': epochs,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': opt.state_dict(),\n",
    "                    'loss_fn': loss_fn}, file_start+\"_modelfile.tar\")\n",
    "\n",
    "        ## To load:\n",
    "        # model = TheModelClass(*args, **kwargs)\n",
    "        # optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "        # checkpoint = torch.load(PATH)\n",
    "        # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # epoch = checkpoint['epoch']\n",
    "        # loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "        # model.eval()\n",
    "        # # - or -\n",
    "        # model.train()\n",
    "        \n",
    "        # # Plot samples (CURRENTLY BROKEN)\n",
    "        # plot_reps([bx_train.cpu().detach().numpy(), \n",
    "        #    by_train.cpu().detach().numpy(),\n",
    "        #    bz_train.cpu().detach().numpy(),\n",
    "        #    ex_train.cpu().detach().numpy(),\n",
    "        #    ey_train.cpu().detach().numpy(),\n",
    "        #    ez_train.cpu().detach().numpy(),\n",
    "        #    jy_train.cpu().detach().numpy()], ['bx','by','bz', 'ex', 'ey','ez', 'jy'], s_train.cpu().detach().numpy(),\n",
    "        #   topo_train.cpu().detach().numpy(),\n",
    "        #   train_topo_pred, samplefile_start, inputs_padding=padding_length, \n",
    "        #   true_coords=np.stack([x0_train.cpu().detach().numpy(), x1_train.cpu().detach().numpy()], axis=-1), exs_per_cat=3 )\n",
    "    \n",
    "    # end of objective function\n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c0575-d14a-45fb-956f-d6ab2f618e0a",
   "metadata": {},
   "source": [
    "### Actually do the thingy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4290a845-88eb-4102-89dd-045a74a68a70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel eqn limit kp=9\n",
      "ModelF(\n",
      "  (bx_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (by_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (bz_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (ex_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (ey_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (ez_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (jy_layers): Sequential(\n",
      "    (0): LazyConv1d(0, 36, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "  )\n",
      "  (post_merge_layers): Sequential(\n",
      "    (0): Conv1d(36, 72, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.11224778904622225, inplace=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LazyLinear(in_features=0, out_features=11, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/torch-env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "7800\n",
      "2200\n",
      "8500\n",
      "2400\n",
      "(301878, 1, 89)\n",
      "[[0]]\n",
      "(301878, 11)\n",
      "Total batch: 301878\n",
      "Number of null samples: 200177\n",
      "Number of non-null samples: 101701\n",
      "With thinning factor 0.5445523804834298 will remove 109006 null samples\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "mean loss: 0.6948952661864375, sample 11/192872\n",
      "mean loss: 0.7008518384358716, sample 1111/192872\n",
      "mean loss: 0.7010104040902188, sample 2211/192872\n",
      "mean loss: 0.6933893964040573, sample 3311/192872\n",
      "mean loss: 0.6915438156980047, sample 4411/192872\n",
      "mean loss: 0.6945290501086864, sample 5511/192872\n",
      "mean loss: 0.6941812805918549, sample 6611/192872\n",
      "mean loss: 0.6942408110153867, sample 7711/192872\n",
      "mean loss: 0.693469956649072, sample 8811/192872\n",
      "mean loss: 0.698803036802497, sample 9911/192872\n",
      "mean loss: 0.6987239074630005, sample 11011/192872\n",
      "mean loss: 0.6932220873788618, sample 12111/192872\n",
      "mean loss: 0.6918383583678042, sample 13211/192872\n",
      "mean loss: 0.6927334418876772, sample 14311/192872\n",
      "mean loss: 0.6876818226428661, sample 15411/192872\n",
      "mean loss: 0.6941310710131369, sample 16511/192872\n",
      "mean loss: 0.6981519708297842, sample 17611/192872\n",
      "mean loss: 0.6861443266652223, sample 18711/192872\n",
      "mean loss: 0.6948272101702314, sample 19811/192872\n",
      "mean loss: 0.6931904710730679, sample 20911/192872\n",
      "mean loss: 0.6936603546273452, sample 22011/192872\n",
      "mean loss: 0.6932816161499021, sample 23111/192872\n",
      "mean loss: 0.6994334679433415, sample 24211/192872\n",
      "mean loss: 0.6826712383696774, sample 25311/192872\n",
      "mean loss: 0.6840245034403106, sample 26411/192872\n",
      "mean loss: 0.6911759299742221, sample 27511/192872\n",
      "mean loss: 0.6949042150570538, sample 28611/192872\n",
      "mean loss: 0.6892620416191292, sample 29711/192872\n",
      "mean loss: 0.6934756224348098, sample 30811/192872\n",
      "mean loss: 0.6796575075236868, sample 31911/192872\n",
      "mean loss: 0.6930871304571593, sample 33011/192872\n",
      "mean loss: 0.6933001064126557, sample 34111/192872\n",
      "mean loss: 0.6940614043392598, sample 35211/192872\n",
      "mean loss: 0.6960912189140387, sample 36311/192872\n",
      "mean loss: 0.6911663885267992, sample 37411/192872\n",
      "mean loss: 0.6902726579871397, sample 38511/192872\n",
      "mean loss: 0.6919581835958443, sample 39611/192872\n",
      "mean loss: 0.6947679915159943, sample 40711/192872\n",
      "mean loss: 0.6944984799315742, sample 41811/192872\n",
      "mean loss: 0.6919723875510606, sample 42911/192872\n",
      "mean loss: 0.6883544050970645, sample 44011/192872\n",
      "mean loss: 0.6907095672875335, sample 45111/192872\n",
      "mean loss: 0.6909242779966834, sample 46211/192872\n",
      "mean loss: 0.6929237569548689, sample 47311/192872\n",
      "mean loss: 0.6919722746307111, sample 48411/192872\n",
      "mean loss: 0.6946927286040677, sample 49511/192872\n",
      "mean loss: 0.6912656650300121, sample 50611/192872\n",
      "mean loss: 0.6926527619907781, sample 51711/192872\n",
      "mean loss: 0.6865726827653557, sample 52811/192872\n",
      "mean loss: 0.6905777421069057, sample 53911/192872\n",
      "mean loss: 0.6925087112765188, sample 55011/192872\n",
      "mean loss: 0.7026418557649712, sample 56111/192872\n",
      "mean loss: 0.6941802157087459, sample 57211/192872\n",
      "mean loss: 0.6930086778868979, sample 58311/192872\n",
      "mean loss: 0.6916882429855286, sample 59411/192872\n",
      "mean loss: 0.6937057939635546, sample 60511/192872\n",
      "mean loss: 0.6887345313611577, sample 61611/192872\n",
      "mean loss: 0.7084324192230913, sample 62711/192872\n",
      "mean loss: 0.6922018438557895, sample 63811/192872\n",
      "mean loss: 0.6854125358903923, sample 64911/192872\n",
      "mean loss: 0.68669967588806, sample 66011/192872\n",
      "mean loss: 0.6994878273405721, sample 67111/192872\n",
      "mean loss: 0.6805166154314977, sample 68211/192872\n",
      "mean loss: 0.6906022937430817, sample 69311/192872\n",
      "mean loss: 0.7024667303853223, sample 70411/192872\n",
      "mean loss: 0.7020202784933517, sample 71511/192872\n",
      "mean loss: 0.694860071025267, sample 72611/192872\n",
      "mean loss: 0.6926370701338819, sample 73711/192872\n",
      "mean loss: 0.6902316497872227, sample 74811/192872\n",
      "mean loss: 0.6934765857351104, sample 75911/192872\n",
      "mean loss: 0.6950841839977816, sample 77011/192872\n",
      "mean loss: 0.6927183986282708, sample 78111/192872\n",
      "mean loss: 0.691554131121309, sample 79211/192872\n",
      "mean loss: 0.6862295880154364, sample 80311/192872\n",
      "mean loss: 0.689395378495524, sample 81411/192872\n",
      "mean loss: 0.6973054136868899, sample 82511/192872\n",
      "mean loss: 0.6906130048894107, sample 83611/192872\n",
      "mean loss: 0.6876705223999939, sample 84711/192872\n",
      "mean loss: 0.6966351571026632, sample 85811/192872\n",
      "mean loss: 0.6834339012535481, sample 86911/192872\n",
      "mean loss: 0.6997396797977252, sample 88011/192872\n",
      "mean loss: 0.6826473301755535, sample 89111/192872\n",
      "mean loss: 0.6783917055665237, sample 90211/192872\n",
      "mean loss: 0.6951021221600414, sample 91311/192872\n",
      "mean loss: 0.6919496409853326, sample 92411/192872\n",
      "mean loss: 0.6961350829692607, sample 93511/192872\n",
      "mean loss: 0.6960544001148847, sample 94611/192872\n",
      "mean loss: 0.694860143236385, sample 95711/192872\n",
      "mean loss: 0.6873295541360436, sample 96811/192872\n",
      "mean loss: 0.6944895886303457, sample 97911/192872\n",
      "mean loss: 0.6943193203552079, sample 99011/192872\n",
      "mean loss: 0.6942248122214455, sample 100111/192872\n",
      "mean loss: 0.692812895192047, sample 101211/192872\n",
      "mean loss: 0.686895784500812, sample 102311/192872\n",
      "mean loss: 0.7155568841854003, sample 103411/192872\n",
      "mean loss: 0.7004543065978097, sample 104511/192872\n",
      "mean loss: 0.7046406155716366, sample 105611/192872\n",
      "mean loss: 0.6945857060021069, sample 106711/192872\n",
      "mean loss: 0.6936304939836805, sample 107811/192872\n",
      "mean loss: 0.6914472022461988, sample 108911/192872\n",
      "mean loss: 0.6979316955107556, sample 110011/192872\n",
      "mean loss: 0.6929321730860598, sample 111111/192872\n",
      "mean loss: 0.6944143642937124, sample 112211/192872\n",
      "mean loss: 0.6944511100420843, sample 113311/192872\n",
      "mean loss: 0.6939817029483772, sample 114411/192872\n",
      "mean loss: 0.6931653468578877, sample 115511/192872\n",
      "mean loss: 0.694663234076089, sample 116611/192872\n",
      "mean loss: 0.70051808358492, sample 117711/192872\n",
      "mean loss: 0.6874595350993155, sample 118811/192872\n",
      "mean loss: 0.6898062174002156, sample 119911/192872\n",
      "mean loss: 0.6902585332193679, sample 121011/192872\n",
      "mean loss: 0.6938716425460876, sample 122111/192872\n",
      "mean loss: 0.6917883541454587, sample 123211/192872\n",
      "mean loss: 0.6903178942964524, sample 124311/192872\n",
      "mean loss: 0.6836775713162906, sample 125411/192872\n",
      "mean loss: 0.700873788280281, sample 126511/192872\n",
      "mean loss: 0.686126977462505, sample 127611/192872\n",
      "mean loss: 0.6989684583001669, sample 128711/192872\n",
      "mean loss: 0.6970323630589117, sample 129811/192872\n",
      "mean loss: 0.6914895030730396, sample 130911/192872\n",
      "mean loss: 0.6862559296137788, sample 132011/192872\n",
      "mean loss: 0.6828017940841563, sample 133111/192872\n",
      "mean loss: 0.6883916227898889, sample 134211/192872\n",
      "mean loss: 0.8880810100295345, sample 135311/192872\n",
      "mean loss: 0.48611113969362635, sample 136411/192872\n",
      "mean loss: 0.5481037745948049, sample 137511/192872\n",
      "mean loss: 0.69967693774381, sample 138611/192872\n",
      "mean loss: 0.4441517710633694, sample 139711/192872\n",
      "mean loss: 0.7252998269656645, sample 140811/192872\n",
      "mean loss: 0.5662359826610005, sample 141911/192872\n",
      "mean loss: 0.42401350428340834, sample 143011/192872\n",
      "mean loss: 0.8067874872081233, sample 144111/192872\n",
      "mean loss: 0.612528139600707, sample 145211/192872\n",
      "mean loss: 0.7020437937129551, sample 146311/192872\n",
      "mean loss: 0.714708545392646, sample 147411/192872\n",
      "mean loss: 0.7161982137361407, sample 148511/192872\n",
      "mean loss: 0.3993757276689288, sample 149611/192872\n",
      "mean loss: 0.7479557402506335, sample 150711/192872\n",
      "mean loss: 0.532237228851342, sample 151811/192872\n",
      "mean loss: 0.6164046953721138, sample 152911/192872\n",
      "mean loss: 0.5196449475575898, sample 154011/192872\n",
      "mean loss: 0.519249857944592, sample 155111/192872\n",
      "mean loss: 0.33863588345070256, sample 156211/192872\n",
      "mean loss: 0.7399293878073032, sample 157311/192872\n",
      "mean loss: 0.6538497491170308, sample 158411/192872\n",
      "mean loss: 0.5251887379962611, sample 159511/192872\n",
      "mean loss: 0.464617523257154, sample 160611/192872\n",
      "mean loss: 0.5487284052091839, sample 161711/192872\n",
      "mean loss: 0.6323511594131033, sample 162811/192872\n",
      "mean loss: 0.8646571985237893, sample 163911/192872\n",
      "mean loss: 0.397502418286161, sample 165011/192872\n",
      "mean loss: 0.6559821720688606, sample 166111/192872\n",
      "mean loss: 0.6293186479785391, sample 167211/192872\n",
      "mean loss: 0.5707756988840464, sample 168311/192872\n",
      "mean loss: 0.45650290227321005, sample 169411/192872\n",
      "mean loss: 0.5566282567784736, sample 170511/192872\n",
      "mean loss: 0.6858183137779508, sample 171611/192872\n",
      "mean loss: 0.46978312673852574, sample 172711/192872\n",
      "mean loss: 0.4480275091338798, sample 173811/192872\n",
      "mean loss: 0.8295186292539526, sample 174911/192872\n",
      "mean loss: 0.7678941141900615, sample 176011/192872\n",
      "mean loss: 0.7554378145201956, sample 177111/192872\n",
      "mean loss: 0.6054973541357196, sample 178211/192872\n",
      "mean loss: 0.817093375064081, sample 179311/192872\n",
      "mean loss: 0.7251954865707984, sample 180411/192872\n",
      "mean loss: 0.5035069815864357, sample 181511/192872\n",
      "mean loss: 0.5819233171524341, sample 182611/192872\n",
      "mean loss: 0.45049167083276853, sample 183711/192872\n",
      "mean loss: 0.361147608144696, sample 184811/192872\n",
      "mean loss: 0.34741027914450484, sample 185911/192872\n",
      "mean loss: 0.5757270757712869, sample 187011/192872\n",
      "mean loss: 0.697166660961792, sample 188111/192872\n",
      "mean loss: 0.4000122092699692, sample 189211/192872\n",
      "mean loss: 0.5508531113763061, sample 190311/192872\n",
      "mean loss: 0.5680997325157674, sample 191411/192872\n",
      "mean loss: 0.4935849703887488, sample 192511/192872\n",
      "Test Error: \n",
      " Accuracy: 76.49446%, Avg loss: 0.562706 \n",
      "\n",
      "Training performance\n",
      "Test Error: \n",
      " Accuracy: 68.82544%, Avg loss: 0.560609 \n",
      "\n",
      "cat_breakdown\t\t[1045270, 1076322]\n",
      "Testing performance\n",
      "Test Error: \n",
      " Accuracy: 76.49446%, Avg loss: 0.562706 \n",
      "\n",
      "cat_breakdown\t\t[652828, 287980]\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6279)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assume study already made\n",
    "\"\"\" study = optuna.create_study(study_name='model_f_study',storage=\"mysql+mysqldb://optunauser:Frikkenoptuna@stellar-intel.princeton.edu:47793/model_f\",\n",
    "        pruner=optuna.pruners.HyperbandPruner(), direction='maximize')\"\"\"\n",
    "# use hyperband pruner \n",
    "study = optuna.load_study(study_name='model_f_study',storage=\"mysql+mysqldb://optunauser:Frikkenoptuna@stellar-intel.princeton.edu:47793/model_f\",\n",
    "                          pruner=optuna.pruners.HyperbandPruner())\n",
    "# # regular training\n",
    "# study.optimize(objective, n_trials=40)\n",
    "# bringing back the best one to look at\n",
    "objective(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987c49c-21bc-41e3-8eed-7b22d9fa3688",
   "metadata": {},
   "source": [
    "### Limitations on kernel sizes math ###\n",
    "for pooling, no padding, dilation 1, stride = kernel_size:\n",
    "\n",
    "L_po = floor( 1 + (L_pi - pk_size)/pk_size ) = floor( L_pi/pk_size )\n",
    "=> L_pi ge L_po * pk_size => L_pi = L_po * pk_size minimum allowable L_pi\n",
    "\n",
    "for convolution, no padding, dilation 1, stride 1:\n",
    "\n",
    "L_co = floor( L_ci - ck_size + 1 ) => L_ci = L_co + ck_size - 1\n",
    "\n",
    "composed limit, for one layer (L_pi = L_co, L_po = 1 minimum valid output size):\n",
    "\n",
    "L_ci = L_pi + ck_size + 1 = L_po * pk_size + ck_size + 1 = pk_size + ck_size + 1\n",
    "\n",
    "composed limit, two layers (for added layer L_po = pk_size + ck_size + 1):\n",
    "\n",
    "L_ci = (pk_size + ck_size + 1) * pk_size + ck_size + 1 < (pk_size + ck_size + 1)^2 for positive integers I think\n",
    "\n",
    "so L_i > (pk_size + ck_size + 1)^n is a safe limit => pk_size + ck_size < L_i^(1/n) - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339c119-5a50-46eb-8f78-85f20526dc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
