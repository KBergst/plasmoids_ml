{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b060324f-e4f6-4144-b3a2-5a31d717e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dtype = torch.double\n",
    "   \n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb13a855-8f8e-478b-a09a-79717e599bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"/tigress/kendrab/analysis-notebooks/model_outs/01-08-23/samples/A131937_modelfile.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc658f-0304-4fe6-b3b0-24375de3e3b8",
   "metadata": {},
   "source": [
    "### Paste model parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc584682-4019-4d70-b817-06fa436ecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"A\"\n",
    "\n",
    "# hyperparameters\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "out_channels = 32  # like 'filters' in keras\n",
    "thinning_factor = [0.9, None]\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor}\n",
    "\n",
    "# other parameters\n",
    "batch_size = 256  # idk what this should be for best performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c30a06-6046-418b-a4a9-9a89a37671b6",
   "metadata": {},
   "source": [
    "### Paste model class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55300d36-b2dd-4583-834e-30149d850f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(nn.Module):\n",
    "    \"\"\" 1D CNN Model \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define these all separately because they will get different weights\n",
    "        # consider smooshing these together into one convolution with in_channels=5\n",
    "        self.bx_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.by_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.bz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.jy_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.vz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        # TODO split this into CNN and classifier parts to facilitate domain adaptation\n",
    "        self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                         padding='valid'),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.MaxPool1d(pool_size),\n",
    "                                               nn.Flatten(),\n",
    "                                               nn.LazyLinear(stride*2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Unflatten(1,(2,stride)))\n",
    "                                               \n",
    "\n",
    "    def forward(self, bx, by, bz, jy, vz):\n",
    "        bx_proc = self.bx_layers(bx)\n",
    "        by_proc = self.by_layers(by)\n",
    "        bz_proc = self.bz_layers(bz)\n",
    "        jy_proc = self.jy_layers(jy)\n",
    "        vz_proc = self.vz_layers(vz)\n",
    "        combined = .2*(bx_proc + by_proc + bz_proc + jy_proc + vz_proc)\n",
    "        logits = self.post_merge_layers(combined)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7ec72df-8c7d-4712-a576-b3e793d7b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)  # number of samples\n",
    "    tot_points = size*stride\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss_sum, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, bx, by, bz, jy, vz, _, _, y in dataloader:\n",
    "            pred = model(bx, by, bz, jy, vz)\n",
    "            test_loss_sum += loss_fn(pred, y).item()  # .item() fetches the python scalar\n",
    "            # number of correct per-point predictions\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    ##### FIX VECTOR LOSS STUFF\n",
    "    ##### WHAT SHAPE SHOULD LOSS BE? FIGURE IT OUT AND MAKE IT SO\n",
    "    ##### THEN HAVE THE DIAGNOSTICS CALCULATED CORRECTLY\n",
    "    test_loss_sum /= num_batches\n",
    "    correct /= tot_points\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss_sum:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682e49a-2605-4d1f-af99-3136af871062",
   "metadata": {},
   "source": [
    "### Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb431f6b-ae64-4f07-ae8e-37308d009a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelA(\n",
       "  (bx_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (by_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bz_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (jy_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (vz_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (post_merge_layers): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): LazyLinear(in_features=0, out_features=20, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Unflatten(dim=1, unflattened_size=(2, 10))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelA()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "checkpoint = torch.load(model_file)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "model.eval()  # set to correct mode to get the correct results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ee598-13d3-4968-979a-dbf7d3d16332",
   "metadata": {},
   "source": [
    "### Load and preprocess the data to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b5c521f-8b8b-4a05-9ee1-903e7a1af219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.99419509, 0.10759244]) array([0.99894021, 0.04602667])\n",
      " array([-0.95871101, -0.28438213]) array([ 0.99957724, -0.02907491])\n",
      " array([-0.82161736, -0.5700394 ]) array([-0.98637197, -0.16453063])\n",
      " array([-0.99862113,  0.05249617]) array([0.99342442, 0.11448984])\n",
      " array([-0.93083282, -0.36544528]) array([0.99927685, 0.03802332])\n",
      " array([ 0.99492641, -0.10060532]) array([0.98095034, 0.19425866])\n",
      " array([-0.98765565,  0.15664071]) array([0.9986014 , 0.05286999])\n",
      " array([-0.99996872,  0.00791005]) array([-0.98819535,  0.15319904])\n",
      " array([-0.99442794,  0.10541857]) array([ 0.99904997, -0.04357938])\n",
      " array([-0.99954364,  0.03020777]) array([0.99262004, 0.12126608])\n",
      " array([-0.99634114,  0.08546541]) array([ 0.48020034, -0.87715884])\n",
      " array([-0.99504616,  0.09941394]) array([0.99874165, 0.05015088])\n",
      " array([0.99925404, 0.03861809]) array([-0.23676372,  0.97156726])\n",
      " array([-0.98901444,  0.14781893]) array([-0.89252643,  0.4509951 ])\n",
      " array([-0.98923887, -0.14630947]) array([ 0.99658173, -0.08261271])\n",
      " array([ 0.99876766, -0.04963016]) array([-0.99971818, -0.02373928])\n",
      " array([ 0.99895073, -0.04579791]) array([-0.9971268,  0.0757506])\n",
      " array([ 0.99462402, -0.10355216]) array([-0.99945586,  0.03298445])\n",
      " array([ 0.13524399, -0.99081232]) array([-0.95985487,  0.28049711])\n",
      " array([-0.98132935,  0.1923349 ]) array([-0.97500547, -0.22218084])\n",
      " array([-0.95530124,  0.29563414]) array([-0.99930847,  0.03718305])\n",
      " array([-0.99971167,  0.02401189]) array([-0.98116083,  0.19319269])\n",
      " array([-0.98005255,  0.19873853]) array([ 0.9855105 , -0.16961444])\n",
      " array([0.97017651, 0.24239955]) array([0.99711168, 0.0759493 ])\n",
      " array([-0.9990366 , -0.04388485]) array([ 0.99640107, -0.08476389])\n",
      " array([-0.97853174, -0.20609618]) array([-0.99926906, -0.03822768])\n",
      " array([-0.95718039, -0.28949213]) array([ 0.98975398, -0.14278325])\n",
      " array([ 0.99697591, -0.07771117]) array([0.97615963, 0.21705385])\n",
      " array([-0.99681464,  0.07975321]) array([-0.99882928,  0.04837426])\n",
      " array([-0.89449233, -0.4470833 ]) array([0.99921949, 0.03950191])\n",
      " array([-0.95742476,  0.28868291]) array([-0.99980719,  0.01963618])\n",
      " array([-0.99384495,  0.11078007]) array([0.99304302, 0.11775211])\n",
      " array([-0.99071705,  0.13594015]) array([-0.9842338 , -0.17687237])\n",
      " array([-0.73229098,  0.68099186]) array([0.99971218, 0.02399094])\n",
      " array([0.99547813, 0.09499098]) array([-0.99793586, -0.06421851])\n",
      " array([ 0.99934679, -0.03613848]) array([-0.9743127 ,  0.22519937])\n",
      " array([-0.94162675, -0.33665868]) array([0.99295245, 0.11851343])\n",
      " array([-0.94257085,  0.33400627]) array([-0.91338567,  0.40709534])\n",
      " array([ 0.99635585, -0.08529376]) array([-0.99526691, -0.09717905])\n",
      " array([-0.97365887,  0.22800965]) array([0.98994453, 0.14145613])\n",
      " array([-0.8016226 , -0.59783041]) array([-0.99991275,  0.01320971])\n",
      " array([ 0.97429424, -0.22527925]) array([-0.9942869 ,  0.10674063])\n",
      " array([-0.97354754, -0.22848454]) array([ 0.9945384 , -0.10437127])\n",
      " array([0.99605005, 0.08879362]) array([ 0.99670569, -0.08110344])\n",
      " array([-0.9948011 , -0.10183698]) array([-0.99803663,  0.06263294])\n",
      " array([-0.99963175,  0.02713605]) array([0.98829835, 0.15253319])\n",
      " array([-0.96465709,  0.26350844]) array([0.9764077 , 0.21593517])\n",
      " array([-0.96348133, -0.26777551]) array([0.99995915, 0.00903917])\n",
      " array([0.992999  , 0.11812277]) array([-0.99408352,  0.1086184 ])\n",
      " array([-0.98031785,  0.19742573]) array([-0.96863727, -0.24847907])]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "basedir = '/tigress/kendrab/21032023/'\n",
    "readpaths = []\n",
    "\n",
    "for i in range(10):\n",
    "    totdir = basedir+str(i)+'/'\n",
    "    for j in range(5,60,5):\n",
    "        readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "idx_list = []  # to keep track of which file what sample came from\n",
    "s_list = []\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "jy_list = []\n",
    "vz_list = []\n",
    "x0_list = []\n",
    "x1_list = []\n",
    "topo_list = []\n",
    "\n",
    "train_idx = None\n",
    "\n",
    "for idx, filepath in enumerate(readpaths):\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        print(file.keys())\n",
    "        print(file['unit_vec'][()])\n",
    "        sys.exit()\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_smooth'][:])\n",
    "        by_list += list(file['by'][:])\n",
    "        bz_list += list(file['bz_smooth'][:])\n",
    "        jy_list += list(file['jy'][:])\n",
    "        vz_list += list(file['vz'][:]) \n",
    "        x0_list += list(file['x_mms'][:])\n",
    "        x1_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "        \n",
    "        if idx == int(.7*len(readpaths)):  # roughly 70-30 train-test split for now\n",
    "            train_idx = len(bx_list)\n",
    "\n",
    "print(len(bx_list))\n",
    "# do train test split\n",
    "idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "s_train_list = s_list[:train_idx] \n",
    "bx_train_list = bx_list[:train_idx] \n",
    "by_train_list = by_list[:train_idx] \n",
    "bz_train_list = bz_list[:train_idx] \n",
    "jy_train_list = jy_list[:train_idx] \n",
    "vz_train_list = vz_list[:train_idx] \n",
    "x0_train_list = x0_list[:train_idx] \n",
    "x1_train_list = x1_list[:train_idx] \n",
    "topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "idx_test_list = idx_list[train_idx:] \n",
    "s_test_list = s_list[train_idx:] \n",
    "bx_test_list = bx_list[train_idx:] \n",
    "by_test_list = by_list[train_idx:] \n",
    "bz_test_list = bz_list[train_idx:] \n",
    "jy_test_list = jy_list[train_idx:] \n",
    "vz_test_list = vz_list[train_idx:] \n",
    "x0_test_list = x0_list[train_idx:] \n",
    "x1_test_list = x1_list[train_idx:] \n",
    "topo_test_list = topo_list[train_idx:] \n",
    "\n",
    "# BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 50-50 between train and test\n",
    "# lots of magic numbers here but we don't have time to make the code nice rn\n",
    "noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "noplasmoids_paths = []\n",
    "for j in range(5,55,5):\n",
    "        noplasmoids_paths.append(noplasmoids_dir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "for k in range(5):\n",
    "    # training part\n",
    "    with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "        idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_train_list += list(file['s'][:])\n",
    "        bx_train_list += list(file['bx_smooth'][:])\n",
    "        by_train_list += list(file['by'][:])\n",
    "        bz_train_list += list(file['bz_smooth'][:])\n",
    "        jy_train_list += list(file['jy'][:])\n",
    "        vz_train_list += list(file['vz'][:]) \n",
    "        x0_train_list += list(file['x_mms'][:])\n",
    "        x1_train_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_train_list += topo_list_tmp    \n",
    "        \n",
    "    # testing part\n",
    "    with h5py.File(noplasmoids_paths[k+5], 'r') as file:\n",
    "        idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_test_list += list(file['s'][:])\n",
    "        bx_test_list += list(file['bx_smooth'][:])\n",
    "        by_test_list += list(file['by'][:])\n",
    "        bz_test_list += list(file['bz_smooth'][:])\n",
    "        jy_test_list += list(file['jy'][:])\n",
    "        vz_test_list += list(file['vz'][:]) \n",
    "        x0_test_list += list(file['x_mms'][:])\n",
    "        x1_test_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_test_list += topo_list_tmp        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c83c91-747b-4ace-9d5d-8f3d738c2986",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85afbd-cb81-4c62-940e-c0259e6bf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk into sliding windows\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "s_train = batch_subsects(s_train_list, input_length, stride)  # not going through training so don't need to shape right\n",
    "bx_train = np.expand_dims(batch_subsects(bx_train_list, input_length, stride),1)\n",
    "by_train = np.expand_dims(batch_subsects(by_train_list, input_length, stride),1)\n",
    "bz_train = np.expand_dims(batch_subsects(bz_train_list, input_length, stride),1)\n",
    "jy_train = np.expand_dims(batch_subsects(jy_train_list, input_length, stride),1)\n",
    "vz_train = np.expand_dims(batch_subsects(vz_train_list, input_length, stride),1)\n",
    "x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "topo_train = np.swapaxes(batch_unpadded_subsects(topo_train_list, padding_length, stride), 1, 2)\n",
    "\n",
    "print(bx_train.shape)\n",
    "\n",
    "idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "s_test = np.expand_dims(batch_subsects(s_test_list, input_length, stride),1)\n",
    "bx_test = np.expand_dims(batch_subsects(bx_test_list, input_length, stride),1)\n",
    "by_test = np.expand_dims(batch_subsects(by_test_list, input_length, stride),1)\n",
    "bz_test = np.expand_dims(batch_subsects(bz_test_list, input_length, stride),1)\n",
    "jy_test = np.expand_dims(batch_subsects(jy_test_list, input_length, stride),1)\n",
    "vz_test = np.expand_dims(batch_subsects(vz_test_list, input_length, stride),1)\n",
    "x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "topo_test = np.swapaxes(batch_unpadded_subsects(topo_test_list, padding_length, stride), 1, 2)\n",
    "\n",
    "# shuffle the segments so they aren't adjacent to overlapping/similar segments\n",
    "idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train, topo_train = \\\n",
    "    shuffle(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test, topo_test = \\\n",
    "    shuffle(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train], topo_train = \\\n",
    "    rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train],\n",
    "                         topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n",
    "# numpy arrays to torch tensors (while crying about how many lines of code this is surely there is a better way)\n",
    "idx_train = torch.from_numpy(idx_train).to(device, dtype=dtype)\n",
    "s_train = torch.from_numpy(s_train).to(device, dtype=dtype)\n",
    "bx_train = torch.from_numpy(bx_train).to(device, dtype=dtype)\n",
    "by_train = torch.from_numpy(by_train).to(device, dtype=dtype)\n",
    "bz_train = torch.from_numpy(bz_train).to(device, dtype=dtype)\n",
    "jy_train = torch.from_numpy(jy_train).to(device, dtype=dtype)\n",
    "vz_train = torch.from_numpy(vz_train).to(device, dtype=dtype)\n",
    "x0_train = torch.from_numpy(x0_train).to(device, dtype=dtype)\n",
    "x1_train = torch.from_numpy(x1_train).to(device, dtype=dtype)\n",
    "topo_train = torch.from_numpy(topo_train).to(device, dtype=dtype)\n",
    "\n",
    "idx_test = torch.from_numpy(idx_test).to(device, dtype=dtype)\n",
    "s_test = torch.from_numpy(s_test).to(device, dtype=dtype)\n",
    "bx_test = torch.from_numpy(bx_test).to(device, dtype=dtype)\n",
    "by_test = torch.from_numpy(by_test).to(device, dtype=dtype)\n",
    "bz_test = torch.from_numpy(bz_test).to(device, dtype=dtype)\n",
    "jy_test = torch.from_numpy(jy_test).to(device, dtype=dtype)\n",
    "vz_test = torch.from_numpy(vz_test).to(device, dtype=dtype)\n",
    "x0_test = torch.from_numpy(x0_test).to(device, dtype=dtype)\n",
    "x1_test = torch.from_numpy(x1_test).to(device, dtype=dtype)\n",
    "topo_test = torch.from_numpy(topo_test).to(device, dtype=dtype)\n",
    "\n",
    "# collect data into Datasets\n",
    "train_dset = TensorDataset(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train,\n",
    "                              x0_train, x1_train, topo_train)\n",
    "test_dset =  TensorDataset(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test,\n",
    "                              x0_test, x1_test, topo_test)\n",
    "# Make DataLoaders for the training and test data\n",
    "train_dl = DataLoader(train_dset, batch_size = batch_size)\n",
    "test_dl = DataLoader(test_dset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71dfa4-ff50-437c-9fcc-68da2af5240b",
   "metadata": {},
   "source": [
    "### Do whatever testing you want with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acb8ce-63fa-4fee-88be-0cb9c2cfac5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
