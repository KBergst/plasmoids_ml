{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0eb48d-8440-475a-aa60-49ab3e708475",
   "metadata": {},
   "source": [
    "used with [domain_adaptation_1](./domain_adaptation_1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff7e04c-f3c9-4670-92d9-a1ac5500007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import h5py\n",
    "mms_data_loc = '/tigress/kendrab/analysis-notebooks/mms_data/mms_slices/'\n",
    "from_numpy_types = [np.float64, np.float32, np.float16, np.complex64, np.complex128, np.int64,\n",
    "                    np.int32, np.int16, np.int8, np.uint8, bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7c9bd-951a-4fcc-9c0b-f105c688f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_file_size = 10*1024  # minimum acceptable file size in bytes to avoid trying to open empty files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8006a-45e8-4f76-9e4c-9f5de7a3a742",
   "metadata": {
    "tags": []
   },
   "source": [
    "## get filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e90aee-df2b-4761-a158-a4f55e86bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(folder=mms_data_loc):\n",
    "    ls_call = subprocess.run(\"ls\", capture_output=True, cwd=folder)\n",
    "    filenames = ls_call.stdout.decode().split()\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3896ba9-a3b6-4249-bd4b-4fee925c25ec",
   "metadata": {},
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68443d6c-aa6c-4087-86a6-6140ddaa13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mms_data(filename, folder=mms_data_loc): \n",
    "    top_level_data = {}  # if the mms data has more complexity than label -> data this won't get things well\n",
    "    #make sure file isn't too small\n",
    "    if os.stat(folder+filename).st_size < min_file_size:\n",
    "        print(\"File is empty or too small. Skipping\")\n",
    "        return top_level_data\n",
    "    with h5py.File(folder + filename,'r') as file:\n",
    "        for label, data in file.items():\n",
    "            top_level_data[label] = data[()]  # returns entire h5py dataset of that label\n",
    "    return top_level_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908328d-1f76-4ed2-a121-eaf77457d5d4",
   "metadata": {},
   "source": [
    "### Make a nice dataloader of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b432074-2646-4cd9-af8c-37725f04af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mms_data(mms_data_dict):\n",
    "    components_list = []\n",
    "    for label, data in mms_data_dict.items():\n",
    "        # handle vector data\n",
    "        if len(data.shape) == 3:  # batch, vector components, timeseries\n",
    "            for i in range(data.shape[1]):\n",
    "                tensor_component = torch.from_numpy(data[:,i:i+1,:]).to(device, dtype=dtype)  # neat trick to keep numpy array dimensionality\n",
    "                components_list.append(tensor_component)\n",
    "        # handle scalar data\n",
    "        elif len(data.shape) == 2:  # batch, timeseries\n",
    "            if data.dtype in from_numpy_types:\n",
    "                data = data[:,None,:]\n",
    "                tensor_component = torch.from_numpy(data[:,0:1,:]).to(device, dtype=dtype)\n",
    "                components_list.append(tensor_component)\n",
    "            else:\n",
    "                try:  # handle case of time as a string\n",
    "                    time_data = np.vectorize(lambda x: datetime.strptime(x.decode('ascii'), '%Y-%m-%dT%H:%M:%S.%f').timestamp())(data)\n",
    "                    tensor_component = torch.from_numpy(np.expand_dims(time_data,1)).to(device, dtype=dtype)\n",
    "                    components_list.append(tensor_component)\n",
    "                    del tensor_component\n",
    "                    del time_data\n",
    "                except TypeError:\n",
    "                    raise TypeError(f\"Dataset is of type {data.dtype} which cannot be\" \n",
    "                                    \"converted to tensor via implemented methods. Try reformatting.\")\n",
    "        # can't handle tensor(2D+ not pytorch tensor) data yet (e.g. 2d/3d distribution functions)\n",
    "        else:\n",
    "            raise ValueError(f\"dataset is of shape {data.shape} which cannot be automatically processed.\")\n",
    "    \n",
    "    mms_dset = TensorDataset(*components_list)\n",
    "    mms_dl = DataLoader(mms_dset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    return mms_dl\n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
