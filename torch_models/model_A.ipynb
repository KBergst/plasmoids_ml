{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c21a810",
   "metadata": {},
   "source": [
    "## Simple model using Bx, By, Bz, jy, vx\n",
    "classifications: o_structures, null\n",
    "\n",
    "fixed length timeseries informed by d_per_de ~ 4\n",
    "\n",
    "Direct translation of model_delta from tensorflow work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29f252d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4150777/604797722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dtype = torch.double\n",
    "   \n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "\n",
    "start = dt.datetime.now(dt.timezone.utc)  # for timing\n",
    "time_str = start.strftime(\"%H%M%S\")\n",
    "date_str = start.strftime(\"%d-%m-%y\")\n",
    "start_str = date_str + time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257023f",
   "metadata": {},
   "source": [
    "### Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"A\"\n",
    "\n",
    "# hyperparameters\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "out_channels = 32  # like 'filters' in keras\n",
    "thinning_factor = [0.85, None]\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 256  # idk what this should be for best performance \n",
    "hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor,\n",
    "              'batch_size':batch_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86699f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO feed hyperparameters into __init__\n",
    "class ModelA(nn.Module):\n",
    "    \"\"\" 1D CNN Model \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define these all separately because they will get different weights\n",
    "        # consider smooshing these together into one convolution with in_channels=5\n",
    "        self.bx_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.by_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.bz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.jy_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.vx_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        # TODO split this into CNN and classifier parts to facilitate domain adaptation\n",
    "        self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                         padding='valid'),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.MaxPool1d(pool_size),\n",
    "                                               nn.Flatten(),\n",
    "                                               nn.LazyLinear(stride*2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Unflatten(1,(2,stride)))\n",
    "                                               \n",
    "\n",
    "    def forward(self, bx, by, bz, jy, vx):\n",
    "        bx_proc = self.bx_layers(bx)\n",
    "        by_proc = self.by_layers(by)\n",
    "        bz_proc = self.bz_layers(bz)\n",
    "        jy_proc = self.jy_layers(jy)\n",
    "        vx_proc = self.vx_layers(vx)\n",
    "        combined = .2*(bx_proc + by_proc + bz_proc + jy_proc + vx_proc)\n",
    "        logits = self.post_merge_layers(combined)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f45156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelA().to(device=device, dtype=torch.double)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df1202",
   "metadata": {},
   "source": [
    "### Define the training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d6d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)  # the length of a tensordataset is the batch size (shared first dim)\n",
    "    for batch, (_, _, bx, by, bz, jy, vx, _, _, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(bx, by, bz, jy, vx)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current_sample = (batch+1)*bx.shape[0]\n",
    "            print(f\"mean loss: {loss}, sample {current_sample}/{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3950dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    size = len(dataloader.dataset)  # number of samples\n",
    "    tot_points = size*stride\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss_sum, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, bx, by, bz, jy, vx, _, _, y in dataloader:\n",
    "            pred = model(bx, by, bz, jy, vx)\n",
    "            pred_list.append(pred.cpu().numpy())\n",
    "            test_loss_sum += loss_fn(pred, y).item()  # .item() fetches the python scalar\n",
    "            # number of correct per-point predictions\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    tot_pred = np.concatenate(pred_list, axis=0)\n",
    "    test_loss_sum /= num_batches\n",
    "    correct /= tot_points\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss_sum:>8f} \\n\")    \n",
    "    return tot_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af466e",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d8cd80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4150777/3427110327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0midx_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bx_mms_smooth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# check this structure!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0ms_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "basedir = '/tigress/kendrab/21032023/'\n",
    "readpaths = []\n",
    "\n",
    "for i in range(10):\n",
    "    totdir = basedir+str(i)+'/'\n",
    "    for j in range(5,60,5):\n",
    "        readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "idx_list = []  # to keep track of which file what sample came from\n",
    "s_list = []\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "jy_list = []\n",
    "vx_list = []\n",
    "x0_list = []\n",
    "x1_list = []\n",
    "topo_list = []\n",
    "\n",
    "train_idx = None\n",
    "\n",
    "for idx, filepath in enumerate(readpaths):\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_mms_smooth'][:])\n",
    "        by_list += list(file['by_mms'][:])\n",
    "        bz_list += list(file['bz_mms_smooth'][:])\n",
    "        jy_list += list(file['jy_mms'][:])\n",
    "        vx_list += list(file['vx_mms'][:])  # vx_mms is simulation vx  thus the filename \n",
    "        x0_list += list(file['x_mms'][:])\n",
    "        x1_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "        \n",
    "        if idx == int(.7*len(readpaths)):  # roughly 70-30 train-test split for now\n",
    "            train_idx = len(bx_list)\n",
    "\n",
    "print(len(bx_list))\n",
    "# do train test split\n",
    "idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "s_train_list = s_list[:train_idx] \n",
    "bx_train_list = bx_list[:train_idx] \n",
    "by_train_list = by_list[:train_idx] \n",
    "bz_train_list = bz_list[:train_idx] \n",
    "jy_train_list = jy_list[:train_idx] \n",
    "vx_train_list = vx_list[:train_idx] \n",
    "x0_train_list = x0_list[:train_idx] \n",
    "x1_train_list = x1_list[:train_idx] \n",
    "topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "idx_test_list = idx_list[train_idx:] \n",
    "s_test_list = s_list[train_idx:] \n",
    "bx_test_list = bx_list[train_idx:] \n",
    "by_test_list = by_list[train_idx:] \n",
    "bz_test_list = bz_list[train_idx:] \n",
    "jy_test_list = jy_list[train_idx:] \n",
    "vx_test_list = vx_list[train_idx:] \n",
    "x0_test_list = x0_list[train_idx:] \n",
    "x1_test_list = x1_list[train_idx:] \n",
    "topo_test_list = topo_list[train_idx:] \n",
    "\n",
    "print(len(bx_train_list))\n",
    "print(len(bx_test_list))\n",
    "# BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 50-50 between train and test\n",
    "# lots of magic numbers here but we don't have time to make the code nice rn\n",
    "noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "noplasmoids_paths = []\n",
    "for j in range(5,55,5):\n",
    "        noplasmoids_paths.append(noplasmoids_dir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "for k in range(5):\n",
    "    # training part\n",
    "    with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "        idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_train_list += list(file['s'][:])\n",
    "        bx_train_list += list(file['bx_mms_smooth'][:])\n",
    "        by_train_list += list(file['by_mms'][:])\n",
    "        bz_train_list += list(file['bz_mms_smooth'][:])\n",
    "        jy_train_list += list(file['jy_mms'][:])\n",
    "        vx_train_list += list(file['vx_mms'][:]) \n",
    "        x0_train_list += list(file['x_mms'][:])\n",
    "        x1_train_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_train_list += topo_list_tmp    \n",
    "        \n",
    "    # testing part\n",
    "    with h5py.File(noplasmoids_paths[k+5], 'r') as file:\n",
    "        idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_test_list += list(file['s'][:])\n",
    "        bx_test_list += list(file['bx_mms_smooth'][:])\n",
    "        by_test_list += list(file['by_mms'][:])\n",
    "        bz_test_list += list(file['bz_mms_smooth'][:])\n",
    "        jy_test_list += list(file['jy_mms'][:])\n",
    "        vx_test_list += list(file['vx_mms'][:]) \n",
    "        x0_test_list += list(file['x_mms'][:])\n",
    "        x1_test_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_test_list += topo_list_tmp  \n",
    "        \n",
    "print(len(bx_train_list))\n",
    "print(len(bx_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0b48d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6ea8c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_unpadded_subsects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4150777/49818650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# chunk into sliding windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0midx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_unpadded_subsects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_train_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ms_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_subsects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_train_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# not going through training so don't need to shape right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_subsects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbx_train_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_unpadded_subsects' is not defined"
     ]
    }
   ],
   "source": [
    "# chunk into sliding windows\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "s_train = batch_subsects(s_train_list, input_length, stride)  # not going through training so don't need to shape right\n",
    "bx_train = np.expand_dims(batch_subsects(bx_train_list, input_length, stride),1)\n",
    "by_train = np.expand_dims(batch_subsects(by_train_list, input_length, stride),1)\n",
    "bz_train = np.expand_dims(batch_subsects(bz_train_list, input_length, stride),1)\n",
    "jy_train = np.expand_dims(batch_subsects(jy_train_list, input_length, stride),1)\n",
    "vx_train = np.expand_dims(batch_subsects(vx_train_list, input_length, stride),1)\n",
    "x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "topo_train = np.swapaxes(batch_unpadded_subsects(topo_train_list, padding_length, stride), 1, 2)\n",
    "\n",
    "print(bx_train.shape)\n",
    "\n",
    "idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "s_test = np.expand_dims(batch_subsects(s_test_list, input_length, stride),1)\n",
    "bx_test = np.expand_dims(batch_subsects(bx_test_list, input_length, stride),1)\n",
    "by_test = np.expand_dims(batch_subsects(by_test_list, input_length, stride),1)\n",
    "bz_test = np.expand_dims(batch_subsects(bz_test_list, input_length, stride),1)\n",
    "jy_test = np.expand_dims(batch_subsects(jy_test_list, input_length, stride),1)\n",
    "vx_test = np.expand_dims(batch_subsects(vx_test_list, input_length, stride),1)\n",
    "x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "topo_test = np.swapaxes(batch_unpadded_subsects(topo_test_list, padding_length, stride), 1, 2)\n",
    "\n",
    "# shuffle the segments so they aren't adjacent to overlapping/similar segments\n",
    "idx_train, s_train, bx_train, by_train, bz_train, jy_train, vx_train, x0_train, x1_train, topo_train = \\\n",
    "    shuffle(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vx_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "idx_test, s_test, bx_test, by_test, bz_test, jy_test, vx_test, x0_test, x1_test, topo_test = \\\n",
    "    shuffle(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vx_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[idx_train, s_train, bx_train, by_train, bz_train, jy_train, vx_train, x0_train, x1_train], topo_train = \\\n",
    "    rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, jy_train, vx_train, x0_train, x1_train],\n",
    "                         topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n",
    "# numpy arrays to torch tensors (while crying about how many lines of code this is surely there is a better way)\n",
    "idx_train = torch.from_numpy(idx_train).to(device, dtype=dtype)\n",
    "s_train = torch.from_numpy(s_train).to(device, dtype=dtype)\n",
    "bx_train = torch.from_numpy(bx_train).to(device, dtype=dtype)\n",
    "by_train = torch.from_numpy(by_train).to(device, dtype=dtype)\n",
    "bz_train = torch.from_numpy(bz_train).to(device, dtype=dtype)\n",
    "jy_train = torch.from_numpy(jy_train).to(device, dtype=dtype)\n",
    "vx_train = torch.from_numpy(vx_train).to(device, dtype=dtype)\n",
    "x0_train = torch.from_numpy(x0_train).to(device, dtype=dtype)\n",
    "x1_train = torch.from_numpy(x1_train).to(device, dtype=dtype)\n",
    "topo_train = torch.from_numpy(topo_train).to(device, dtype=dtype)\n",
    "\n",
    "idx_test = torch.from_numpy(idx_test).to(device, dtype=dtype)\n",
    "s_test = torch.from_numpy(s_test).to(device, dtype=dtype)\n",
    "bx_test = torch.from_numpy(bx_test).to(device, dtype=dtype)\n",
    "by_test = torch.from_numpy(by_test).to(device, dtype=dtype)\n",
    "bz_test = torch.from_numpy(bz_test).to(device, dtype=dtype)\n",
    "jy_test = torch.from_numpy(jy_test).to(device, dtype=dtype)\n",
    "vx_test = torch.from_numpy(vx_test).to(device, dtype=dtype)\n",
    "x0_test = torch.from_numpy(x0_test).to(device, dtype=dtype)\n",
    "x1_test = torch.from_numpy(x1_test).to(device, dtype=dtype)\n",
    "topo_test = torch.from_numpy(topo_test).to(device, dtype=dtype)\n",
    "\n",
    "# collect data into Datasets\n",
    "train_dset = TensorDataset(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vx_train,\n",
    "                              x0_train, x1_train, topo_train)\n",
    "test_dset =  TensorDataset(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vx_test,\n",
    "                              x0_test, x1_test, topo_test)\n",
    "# Make DataLoaders for the training and test data\n",
    "train_dl = DataLoader(train_dset, batch_size = batch_size)\n",
    "test_dl = DataLoader(test_dset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ae267",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')  # We are allowing reduction bc backward() \n",
    "                                            # needs a scalar or to specify a different gradient.\n",
    "                                                # Easier this way. Probably.\n",
    "opt = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, opt)\n",
    "    test_loop(test_dl, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f151b",
   "metadata": {},
   "source": [
    "### Make output directories if they do not exist and set up output file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4939d338",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generic_outputs_structure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4150777/708141995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m log_file, cf_file, samplefile_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                 model_name, date_str, time_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generic_outputs_structure' is not defined"
     ]
    }
   ],
   "source": [
    "log_file, cf_file, samplefile_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                model_name, date_str, time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e462",
   "metadata": {},
   "source": [
    "### Observe the results, dump information to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file, 'w') as log:\n",
    "    log.write(f\"Model {model_name} trained on {start_str}\\n\")\n",
    "    log.write(f\"loss function \\t\\t{loss_fn.__repr__()}\\n\")\n",
    "    log.write(\"Hyperparameters:\\n\")\n",
    "    for key in hyperparams.keys():\n",
    "        log.write(f\"{key}\\t\\t{hyperparams[key]}\\n\")\n",
    "        \n",
    "    log.write(\"Training performance\\n\")     \n",
    "    print(\"Training performance\")\n",
    "    train_topo_pred = test_loop(train_dl, model, loss_fn) \n",
    "    train_1d = np.argmax(topo_train.cpu().numpy(), axis=1).flatten() # for confusion matrix\n",
    "    train_1d_pred = np.argmax(train_topo_pred, axis=1).flatten()  \n",
    "    num_per_cat = [np.sum(topo_train.cpu().numpy()[:,i,:] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "    # TODO CALCULATE RECALL PER CATEGORY\n",
    "\n",
    "    log.write(\"Testing performance\\n\")\n",
    "    print(\"Testing performance\")\n",
    "    test_topo_pred = test_loop(test_dl, model, loss_fn)\n",
    "    test_1d = np.argmax(topo_test.cpu().numpy(), axis=1).flatten() # for confusion matrix\n",
    "    test_1d_pred = np.argmax(test_topo_pred, axis=1).flatten()  \n",
    "    num_per_cat = [np.sum(topo_test.cpu().numpy()[:,i,:] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "\n",
    "    end = dt.datetime.now(dt.timezone.utc)    \n",
    "    log.write(f\"runtime_seconds\\t\\t{(end-start).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac634e0c",
   "metadata": {},
   "source": [
    "### Plot confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35216819",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_traintest_cf_matrices(train_1d, train_1d_pred, test_1d, test_1d_pred, cf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198481d",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe410f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss_fn': loss_fn}, samplefile_start+\"_modelfile.tar\")\n",
    "\n",
    "## To load:\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "# model.eval()\n",
    "# # - or -\n",
    "# model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b93d053",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf844a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reps([bx_train.cpu().detach().numpy(), \n",
    "           by_train.cpu().detach().numpy(),\n",
    "           bz_train.cpu().detach().numpy(),\n",
    "           jy_train.cpu().detach().numpy(),\n",
    "           vx_train.cpu().detach().numpy() ], ['bx','by','bz','jy','vx'], s_train.cpu().detach().numpy(),\n",
    "          topo_train.cpu().detach().numpy(),\n",
    "          train_topo_pred, samplefile_start, inputs_padding=padding_length, \n",
    "          true_coords=np.stack([x0_train.cpu().detach().numpy(), x1_train.cpu().detach().numpy()], axis=-1), exs_per_cat=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
