{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auburn-geology",
   "metadata": {},
   "source": [
    "### Resnet-based model for funsies\n",
    "This one is just going to use the magnetic field for now until I decide how to add multiple images instead of one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appreciated-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dtype = torch.double\n",
    "   \n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "\n",
    "start = dt.datetime.now(dt.timezone.utc)  # for timing\n",
    "time_str = start.strftime(\"%H%M%S\")\n",
    "date_str = start.strftime(\"%d-%m-%y\")\n",
    "start_str = date_str + time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-eclipse",
   "metadata": {},
   "source": [
    "### Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "postal-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"B\"\n",
    "\n",
    "# hyperparameters\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "thinning_factor = [0.8, None]\n",
    "learning_rate = 0.01\n",
    "epochs = 4\n",
    "hyperparams = {'learning_rate':learning_rate, 'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor}\n",
    "\n",
    "# other parameters\n",
    "batch_size = 64  # idk what this should be for best performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifteen-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tv.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "# Replace last layer to give the right output shape\n",
    "model.fc = nn.Sequential(nn.Linear(in_features=2048, out_features=2*stride, bias=True), \n",
    "                         nn.Unflatten(1,(2,stride)))\n",
    "# to GPU\n",
    "model = model.to(device=device, dtype=torch.double)\n",
    "# freeze everything except this last layer\n",
    "# TRY DIFFERENT LAYERS FROZEN/UNFROZEN\n",
    "# model.requires_grad_(False)\n",
    "# model.fc.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-blade",
   "metadata": {},
   "source": [
    "### Define the training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liquid-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)  # the length of a tensordataset is the batch size (shared first dim)\n",
    "    for batch, (_, _, B, _, _, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(B)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current_sample = (batch+1)*B.shape[0]\n",
    "            print(f\"mean loss: {loss}, sample {current_sample}/{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    size = len(dataloader.dataset)  # number of samples\n",
    "    tot_points = size*stride\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss_sum, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, B, _, _, y in dataloader:\n",
    "            pred = model(B)\n",
    "            pred_list.append(pred.cpu().numpy())\n",
    "            test_loss_sum += loss_fn(pred, y).item()  # .item() fetches the python scalar\n",
    "            # number of correct per-point predictions\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    ##### MAKE SURE DIAGNOSTICS ARE CALCULATED CORRECTLY\n",
    "    tot_pred = np.concatenate(pred_list, axis=0)\n",
    "    test_loss_sum /= num_batches\n",
    "    correct /= tot_points\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss_sum:>8f} \\n\")\n",
    "    return tot_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-telephone",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "underlying-drive",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    }
   ],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "basedir = '/tigress/kendrab/21032023/'\n",
    "readpaths = []\n",
    "\n",
    "for i in range(10):\n",
    "    totdir = basedir+str(i)+'/'\n",
    "    for j in range(5,60,5):\n",
    "        readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "idx_list = []  # to keep track of which file what sample came from\n",
    "s_list = []\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "x0_list = []\n",
    "x1_list = []\n",
    "topo_list = []\n",
    "\n",
    "train_idx = None\n",
    "\n",
    "for idx, filepath in enumerate(readpaths):\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_mms_smooth'][:])\n",
    "        by_list += list(file['by_mms'][:])\n",
    "        bz_list += list(file['bz_mms_smooth'][:])\n",
    "        x0_list += list(file['x_mms'][:])\n",
    "        x1_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "        \n",
    "        if idx == int(.7*len(readpaths)):  # roughly 70-30 train-test split for now\n",
    "            train_idx = len(bx_list)\n",
    "\n",
    "print(len(bx_list))\n",
    "# do train test split\n",
    "idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "s_train_list = s_list[:train_idx] \n",
    "bx_train_list = bx_list[:train_idx] \n",
    "by_train_list = by_list[:train_idx] \n",
    "bz_train_list = bz_list[:train_idx] \n",
    "x0_train_list = x0_list[:train_idx] \n",
    "x1_train_list = x1_list[:train_idx] \n",
    "topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "idx_test_list = idx_list[train_idx:] \n",
    "s_test_list = s_list[train_idx:] \n",
    "bx_test_list = bx_list[train_idx:] \n",
    "by_test_list = by_list[train_idx:] \n",
    "bz_test_list = bz_list[train_idx:] \n",
    "x0_test_list = x0_list[train_idx:] \n",
    "x1_test_list = x1_list[train_idx:] \n",
    "topo_test_list = topo_list[train_idx:] \n",
    "\n",
    "# BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 50-50 between train and test\n",
    "# lots of magic numbers here but we don't have time to make the code nice rn\n",
    "noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "noplasmoids_paths = []\n",
    "\n",
    "for j in range(5,55,5):\n",
    "        noplasmoids_paths.append(noplasmoids_dir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "for k in range(5):\n",
    "    # training part\n",
    "    with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "        idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_train_list += list(file['s'][:])\n",
    "        bx_train_list += list(file['bx_mms_smooth'][:])\n",
    "        by_train_list += list(file['by_mms'][:])\n",
    "        bz_train_list += list(file['bz_mms_smooth'][:])\n",
    "        x0_train_list += list(file['x_mms'][:])\n",
    "        x1_train_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_train_list += topo_list_tmp    \n",
    "        \n",
    "    # testing part\n",
    "    with h5py.File(noplasmoids_paths[k+5], 'r') as file:\n",
    "        idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_test_list += list(file['s'][:])\n",
    "        bx_test_list += list(file['bx_mms_smooth'][:])\n",
    "        by_test_list += list(file['by_mms'][:])\n",
    "        bz_test_list += list(file['bz_mms_smooth'][:])\n",
    "        x0_test_list += list(file['x_mms'][:])\n",
    "        x1_test_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_test_list += topo_list_tmp        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-vietnamese",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overhead-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch: 369510\n",
      "Number of null samples: 296973\n",
      "Number of non-null samples: 72537\n",
      "With thinning factor 0.8 will remove 237578 null samples\n"
     ]
    }
   ],
   "source": [
    "# chunk into sliding windows\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "s_train = batch_subsects(s_train_list, input_length, stride)  # not going through training so don't need to shape right\n",
    "bx_train = np.expand_dims(batch_subsects(bx_train_list, input_length, stride),1)\n",
    "by_train = np.expand_dims(batch_subsects(by_train_list, input_length, stride),1)\n",
    "bz_train = np.expand_dims(batch_subsects(bz_train_list, input_length, stride),1)\n",
    "x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "topo_train = np.swapaxes(batch_unpadded_subsects(topo_train_list, padding_length, stride), 1, 2)\n",
    "\n",
    "idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "s_test = np.expand_dims(batch_subsects(s_test_list, input_length, stride),1)\n",
    "bx_test = np.expand_dims(batch_subsects(bx_test_list, input_length, stride),1)\n",
    "by_test = np.expand_dims(batch_subsects(by_test_list, input_length, stride),1)\n",
    "bz_test = np.expand_dims(batch_subsects(bz_test_list, input_length, stride),1)\n",
    "x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "topo_test = np.swapaxes(batch_unpadded_subsects(topo_test_list, padding_length, stride), 1, 2)\n",
    "\n",
    "# shuffle the segments so they aren't adjacent to overlapping/similar segments\n",
    "idx_train, s_train, bx_train, by_train, bz_train, x0_train, x1_train, topo_train = \\\n",
    "    shuffle(idx_train, s_train, bx_train, by_train, bz_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "idx_test, s_test, bx_test, by_test, bz_test, x0_test, x1_test, topo_test = \\\n",
    "    shuffle(idx_test, s_test, bx_test, by_test, bz_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[idx_train, s_train, bx_train, by_train, bz_train, x0_train, x1_train], topo_train = \\\n",
    "    rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, x0_train, x1_train],\n",
    "                         topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-graduate",
   "metadata": {},
   "source": [
    "### Transform Bx,y,z to one 2d 3-channel \"image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so slow and I hate it but that is how it must be :(\n",
    "# End up with shape (batch, 3, input_length, input_length)\n",
    "img_shape_train = (bx_train.shape[0], 3, input_length, input_length)\n",
    "b_img_train = np.zeros(img_shape_train)\n",
    "img_shape_test = (bx_test.shape[0], 3, input_length, input_length)\n",
    "b_img_test = np.zeros(img_shape_test)\n",
    "\n",
    "# calculate range over last dim for the transformation\n",
    "bx_train_r = np.ptp(bx_train, axis=-1)\n",
    "by_train_r = np.ptp(by_train, axis=-1)\n",
    "bz_train_r = np.ptp(bz_train, axis=-1)\n",
    "bx_test_r = np.ptp(bx_test, axis=-1)\n",
    "by_test_r = np.ptp(by_test, axis=-1)\n",
    "bz_test_r = np.ptp(bz_test, axis=-1)\n",
    "\n",
    "for i in range(input_length):  # lets only do this garbage for loop once\n",
    "    for j in range(input_length):\n",
    "        # bx -> 0, by -> 1, bz -> 2\n",
    "        # train array\n",
    "        b_img_train[:,0,i,j] = (bx_train[:,:,i]*(1 - np.abs(bx_train[:,:,i] - bx_train[:,:,j])\n",
    "                                                 /bx_train_r)).squeeze()\n",
    "        b_img_train[:,1,i,j] = (by_train[:,:,i]*(1 - np.abs(by_train[:,:,i] - by_train[:,:,j])\n",
    "                                                 /by_train_r)).squeeze()\n",
    "        b_img_train[:,2,i,j] = (bz_train[:,:,i]*(1 - np.abs(bz_train[:,:,i] - bz_train[:,:,j])\n",
    "                                                 /bz_train_r)).squeeze()\n",
    "        # test array\n",
    "        b_img_test[:,0,i,j] = (bx_test[:,:,i]*(1 - np.abs(bx_test[:,:,i] - bx_test[:,:,j])\n",
    "                                                 /bx_test_r)).squeeze()\n",
    "        b_img_test[:,1,i,j] = (by_test[:,:,i]*(1 - np.abs(by_test[:,:,i] - by_test[:,:,j])\n",
    "                                                 /by_test_r)).squeeze()\n",
    "        b_img_test[:,2,i,j] = (bz_test[:,:,i]*(1 - np.abs(bz_test[:,:,i] - bz_test[:,:,j])\n",
    "                                                 /bz_test_r)).squeeze()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-battle",
   "metadata": {},
   "source": [
    "### Move to GPU and make datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy arrays to torch tensors (while crying about how many lines of code this is surely there is a better way)\n",
    "idx_train = torch.from_numpy(idx_train).to(device, dtype=dtype)\n",
    "s_train = torch.from_numpy(s_train).to(device, dtype=dtype)\n",
    "b_img_train = torch.from_numpy(b_img_train).to(device, dtype=dtype)\n",
    "x0_train = torch.from_numpy(x0_train).to(device, dtype=dtype)\n",
    "x1_train = torch.from_numpy(x1_train).to(device, dtype=dtype)\n",
    "topo_train = torch.from_numpy(topo_train).to(device, dtype=dtype)\n",
    "\n",
    "idx_test = torch.from_numpy(idx_test).to(device, dtype=dtype)\n",
    "s_test = torch.from_numpy(s_test).to(device, dtype=dtype)\n",
    "b_img_test = torch.from_numpy(b_img_test).to(device, dtype=dtype)\n",
    "x0_test = torch.from_numpy(x0_test).to(device, dtype=dtype)\n",
    "x1_test = torch.from_numpy(x1_test).to(device, dtype=dtype)\n",
    "topo_test = torch.from_numpy(topo_test).to(device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data into Datasets\n",
    "train_dset = TensorDataset(idx_train, s_train, b_img_train,\n",
    "                              x0_train, x1_train, topo_train)\n",
    "test_dset =  TensorDataset(idx_test, s_test, b_img_test,\n",
    "                              x0_test, x1_test, topo_test)\n",
    "# Make DataLoaders for the training and test data\n",
    "train_dl = DataLoader(train_dset, batch_size = batch_size)\n",
    "test_dl = DataLoader(test_dset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-nevada",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')  # We are allowing reduction bc backward() \n",
    "                                            # needs a scalar or to specify a different gradient.\n",
    "                                                # Easier this way. Probably.\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, opt)\n",
    "    test_loop(test_dl, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-carry",
   "metadata": {},
   "source": [
    "### Make output directories if they do not exist and set up output file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file, cf_file, samplefile_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                model_name, date_str, time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-workstation",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss_fn': loss_fn}, samplefile_start+\"_modelfile.tar\")\n",
    "\n",
    "## To load:\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "# model.eval()\n",
    "# # - or -\n",
    "# model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-machine",
   "metadata": {},
   "source": [
    "### Observe the results, dump information to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open(log_file, 'w') as log:\n",
    "        log.write(f\"Model {model_name} trained on {start_str}\\n\")\n",
    "        log.write(f\"loss function \\t\\t{loss_fn.__repr__()}\\n\")\n",
    "        log.write(\"Hyperparameters:\\n\")\n",
    "        for key in hyperparams.keys():\n",
    "            log.write(f\"{key}\\t\\t{hyperparams[key]}\\n\")\n",
    "\n",
    "        log.write(\"Training performance\\n\")        \n",
    "        print(\"Training performance\")\n",
    "        train_topo_pred = test_loop(train_dl, model, loss_fn)   \n",
    "        train_1d = np.argmax(topo_train.cpu().numpy(), axis=1).flatten() # for confusion matrix\n",
    "        train_1d_pred = np.argmax(train_topo_pred, axis=1).flatten()  \n",
    "        num_per_cat = [np.sum(topo_train.cpu().numpy()[:,i,:] == 1) for i in range(2)]\n",
    "        log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "        print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "        # TODO CALCULATE RECALL PER CATEGORY\n",
    "\n",
    "        log.write(\"Testing performance\\n\")\n",
    "        print(\"Testing performance\")\n",
    "        test_topo_pred = test_loop(test_dl, model, loss_fn)\n",
    "        test_1d = np.argmax(topo_test.cpu().numpy(), axis=1).flatten() # for confusion matrix\n",
    "        test_1d_pred = np.argmax(test_topo_pred, axis=1).flatten()  \n",
    "        num_per_cat = [np.sum(topo_test.cpu().numpy()[:,i,:] == 1) for i in range(2)]\n",
    "        log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "        print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "\n",
    "        end = dt.datetime.now(dt.timezone.utc)    \n",
    "        log.write(f\"runtime_seconds\\t\\t{(end-start).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-mechanics",
   "metadata": {},
   "source": [
    "### Plot confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_traintest_cf_matrices(train_1d, train_1d_pred, test_1d, test_1d_pred, cf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
