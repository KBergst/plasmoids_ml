{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe34827",
   "metadata": {},
   "source": [
    "Relies on calling notebook to populate some parameters... hmmm... not great. But I don't want a long spaghetti jupyter notebook either. I just need to finish my thesis I can worry about writing good code at my future job :/\n",
    "\n",
    "used with [domain_adaptation_1](./domain_adaptation_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8a17d",
   "metadata": {},
   "source": [
    "## Model-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2496372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"/tigress/kendrab/analysis-notebooks/model_outs/14-06-24/F143444_modelfile.tar\"\n",
    "\n",
    "model_name = \"F\"\n",
    "# hyperparameters\n",
    "padding_length = 39  # amount of data on each side of each segment for additional info\n",
    "stride = 11  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "kernel_size = 3\n",
    "pool_size = 5\n",
    "out_channels = 36  # like 'filters' in keras\n",
    "thinning_factor = 0.5445523804834298\n",
    "learning_rate = 0.0025266393119259896\n",
    "batch_size = 11  # idk what this should be for best performance \n",
    "try:\n",
    "    override_batch_size\n",
    "except NameError:\n",
    "    override_batch_size = None\n",
    "if override_batch_size:  # doing this instead of refactoring and \n",
    "    # potentially majorly fucking things up. very dumb and bad\n",
    "    batch_size = n\n",
    "num_conv = 1\n",
    "dropout_fraction = 0.11224778904622225\n",
    "kp_limit = 9\n",
    "\n",
    "hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'thinning_factor':thinning_factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642819a",
   "metadata": {},
   "source": [
    "## Model class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae88c1-9e8a-444d-b1f6-d61259270390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_layers_n_times(layer_list, n):  # this instead of something simpler to be absolutely sure the layers are different objects and not repeating the same one\n",
    "    new_layer_list = []\n",
    "    for i in range(n):\n",
    "        for layer in layer_list:\n",
    "            new_layer_list.append(copy.deepcopy(layer))\n",
    "    return new_layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO feed hyperparameters into __init__\n",
    "class ModelF(nn.Module):\n",
    "    \"\"\" 1D CNN Model \"\"\"\n",
    "    def __init__(self):  # TODO change optuna params from globals to inputs to init\n",
    "        super().__init__()\n",
    "        # define these all separately because they will get different weights\n",
    "        # consider smooshing these together into one convolution with in_channels=6. Idk if a good idea\n",
    "        self.bx_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "        self.by_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "        self.bz_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "        self.ex_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "        self.ey_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "        self.ez_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "        self.jy_layers = nn.Sequential(*repeat_layers_n_times([nn.LazyConv1d(out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(), nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction)], num_conv))\n",
    "\n",
    "        self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                                     padding='valid'), nn.ReLU(),\n",
    "                                                           nn.MaxPool1d(pool_size), nn.Dropout(p=dropout_fraction),\n",
    "                                                nn.Flatten(),\n",
    "                                                nn.LazyLinear(stride))\n",
    "\n",
    "\n",
    "    def forward(self, bx, by, bz, ex, ey, ez, jy):\n",
    "        bx_proc = self.bx_layers(bx)\n",
    "        by_proc = self.by_layers(by)\n",
    "        bz_proc = self.bz_layers(bz)\n",
    "        ex_proc = self.ex_layers(ex)\n",
    "        ey_proc = self.ey_layers(ey)\n",
    "        ez_proc = self.ez_layers(ez)\n",
    "        jy_proc = self.jy_layers(jy)\n",
    "        combined = (bx_proc + by_proc + bz_proc + ex_proc + ey_proc + ez_proc + jy_proc)/6.\n",
    "        logits = self.post_merge_layers(combined)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263883c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load the model into 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210dcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model_state(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "    model.eval()  # set to correct mode to get the correct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a58169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/kendrab/.conda/envs/torch-env/lib/python3.10/site-packages/ipykernel_launcher.py', '-f', '/home/kendrab/.local/share/jupyter/runtime/kernel-6e46495a-f050-476f-96cd-5ec60679bacf.json']\n",
      "/home/kendrab/.conda/envs/torch-env/lib/python3.10/site-packages/ipykernel_launcher.py\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this will be called if running the notebook, even from another notebook\n",
    "    model = ModelF().to(device=device, dtype=torch.double)\n",
    "    import_model_state(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
