{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd201d8-e686-4ee3-9c1a-3afd4cfbdd59",
   "metadata": {},
   "source": [
    "Relies on calling notebook to populate some parameters... hmmm... not great. But I don't want a long spaghetti jupyter notebook either. I just need to finish my thesis I can worry about writing good code at my future job :/\n",
    "\n",
    "used with [domain_adaptation_1](./domain_adaptation_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944d5b8-6eb1-49c7-9873-e370971dd42e",
   "metadata": {},
   "source": [
    "## Model-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505aaaab-3b65-4112-86be-e45d1730c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"/tigress/kendrab/analysis-notebooks/model_outs/11-08-23/samples/D143059_modelfile.tar\"\n",
    "\n",
    "model_name = \"D\"\n",
    "\n",
    "# hyperparameters\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "out_channels = 32  # like 'filters' in keras\n",
    "thinning_factor = [0.85, None]\n",
    "learning_rate = 0.01\n",
    "batch_size = 256  # idk what this should be for best performance \n",
    "\n",
    "hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'thinning_factor':thinning_factor,\n",
    "              'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b32c5-de6f-45dc-8690-f124c726d210",
   "metadata": {},
   "source": [
    "## Model class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1301d3e-86a3-4829-a0d8-2f1548ecda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO feed hyperparameters into __init__\n",
    "class ModelD(nn.Module):\n",
    "    \"\"\" 1D CNN Model \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define these all separately because they will get different weights\n",
    "        # consider smooshing these together into one convolution with in_channels=6. Idk if a good idea\n",
    "        self.bx_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.by_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.bz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.ex_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.ey_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.ez_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.jy_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        \n",
    "        # TODO split this into CNN and classifier parts to facilitate domain adaptation\n",
    "        self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                         padding='valid'),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.MaxPool1d(pool_size),\n",
    "                                               nn.Flatten(),\n",
    "                                               nn.LazyLinear(stride*2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Unflatten(1,(2,stride)))\n",
    "                                               \n",
    "\n",
    "    def forward(self, bx, by, bz, ex, ey, ez, jy):\n",
    "        bx_proc = self.bx_layers(bx)\n",
    "        by_proc = self.by_layers(by)\n",
    "        bz_proc = self.bz_layers(bz)\n",
    "        ex_proc = self.ex_layers(ex)\n",
    "        ey_proc = self.ey_layers(ey)\n",
    "        ez_proc = self.ez_layers(ez)\n",
    "        jy_proc = self.jy_layers(jy)\n",
    "        combined = (bx_proc + by_proc + bz_proc + ex_proc + ey_proc + ez_proc + jy_proc)/6.\n",
    "        logits = self.post_merge_layers(combined)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b348e4-1e96-472c-b079-7d6eee461405",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load the model into 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51235335-69b8-42ae-a24e-33278fbcfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model_state(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "    model.eval()  # set to correct mode to get the correct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ac01e8-debe-428f-a601-dab0d2fdb44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/kendrab/.conda/envs/torch-env/lib/python3.10/site-packages/ipykernel_launcher.py', '-f', '/home/kendrab/.local/share/jupyter/runtime/kernel-6e46495a-f050-476f-96cd-5ec60679bacf.json']\n",
      "/home/kendrab/.conda/envs/torch-env/lib/python3.10/site-packages/ipykernel_launcher.py\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this will be called if running the notebook, even from another notebook\n",
    "    model = ModelD().to(device=device, dtype=torch.double)\n",
    "    import_model_state(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
