{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dbd5b0-b58f-4562-88e5-d54a2aae0d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# import gc  # debug that memory leak\n",
    "# import tracemalloc  # DEBUG THAT MEMORY LEAK\n",
    "import psutil  # :( mem leak\n",
    "import h5py\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "dtype = torch.double\n",
    "\n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_mms_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ee82cc-8e22-4d49-b146-cbbc154996c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "mms_epochs = 1  # number of times to loop through the entire mms dataset (start with 1 lmao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dffbea-838a-431b-a5af-abe668145b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : make notebook to load sim data x\n",
    "# TODO : there is TOO MUCH MMS data to load it all at once. Need to figure out how to do the training in batches\n",
    "# TODO : set up the ADDA architecture x\n",
    "# TODO : see what it spits out / how to interpret the adaptation results (discriminator metric?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15690aa3-6188-4ca4-9c7d-95c0ae87bcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/torch-env/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_model.ipynb\n",
    "\n",
    "# TODO?: enforce that key variables that need to exist later down the pipeline\n",
    "# are populated by import_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64919069-602c-44e4-a6fe-5566f4dd5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the model's features (before classification step)\n",
    "# get_graph_node_names(model)\n",
    "return_nodes = {'post_merge_layers.2' : 'features'}\n",
    "feat_sim = create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5dc84a4-00be-45df-b0c8-009ff25726f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (layer1): Sequential(\n",
      "    (0): LazyLinear(in_features=0, out_features=60, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (domain_label): Sequential(\n",
      "    (0): Linear(in_features=60, out_features=1, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n",
      "MMSFeatExtract(\n",
      "  (bx_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (by_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (bz_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (ex_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (ey_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (ez_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (jy_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (post_merge_layers): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# build the domain adaptation structure\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/adda_constructor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1acad2fe-aa9e-4b61-9bc8-719b6cdee894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537972, 1, 30)\n",
      "(537972, 2, 10)\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Load the sim data\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_sim_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdf29a4-095b-4f40-b294-c059b7685b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mms data locations and shuffle the files to not be chronological\n",
    "mms_filenames = get_filenames()\n",
    "debug_filename = \"2021-08-18T04-48-00_2021-08-20T04-30-30.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604828d-fcbd-4689-9b48-8e338b880f5e",
   "metadata": {},
   "source": [
    "### Time to do some training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f545db8-1d45-4d20-ad95-7d020b6e53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optim_disc = torch.optim.Adam(discrim.parameters(),lr=learning_rate)\n",
    "optim_feat = torch.optim.Adam(feat_mms.parameters(),lr=learning_rate)\n",
    "loss_feat = []\n",
    "loss_disc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b5089d-d95d-4dc4-9223-77b1cc6741d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "getting MMS file 2022-07-11T07-38-30_2022-07-11T23-28-00.h5, number 1 of 230\n",
      "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
      "2407.125\n",
      "2659.25\n",
      "2716.8125\n",
      "getting MMS file 2018-10-03T23-00-30_2018-10-03T23-52-00.h5, number 2 of 230\n",
      "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
      "2633.125\n",
      "2764.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_152560/3198102143.py:13\u001b[0m, in \u001b[0;36mformat_mms_data\u001b[0;34m(mms_data_dict)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     tensor_component \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     14\u001b[0m     components_list\u001b[38;5;241m.\u001b[39mappend(tensor_component)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.bytes_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mms_data_dict) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# data was not loaded, skip this file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m mms_dl \u001b[38;5;241m=\u001b[39m \u001b[43mformat_mms_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmms_data_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(process\u001b[38;5;241m.\u001b[39mmemory_info()\u001b[38;5;241m.\u001b[39mrss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m mms_dl\n",
      "File \u001b[0;32m/tmp/ipykernel_152560/3198102143.py:18\u001b[0m, in \u001b[0;36mformat_mms_data\u001b[0;34m(mms_data_dict)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# handle case of time as a string\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m         time_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS.\u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         tensor_component \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mexpand_dims(time_data,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     20\u001b[0m         components_list\u001b[38;5;241m.\u001b[39mappend(tensor_component)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.10/site-packages/numpy/lib/function_base.py:2328\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2325\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2326\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.10/site-packages/numpy/lib/function_base.py:2411\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2414\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/tmp/ipykernel_152560/3198102143.py:18\u001b[0m, in \u001b[0;36mformat_mms_data.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# handle case of time as a string\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m         time_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS.\u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtimestamp())(data)\n\u001b[1;32m     19\u001b[0m         tensor_component \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mexpand_dims(time_data,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     20\u001b[0m         components_list\u001b[38;5;241m.\u001b[39mappend(tensor_component)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.10/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    570\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.10/_strptime.py:416\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m group_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    415\u001b[0m     minute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(found_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m group_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    417\u001b[0m     second \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(found_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m group_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(mms_epochs):\n",
    "    print(f\"Starting Epoch {epoch+1}\")\n",
    "    mms_filenames = shuffle(mms_filenames)\n",
    "    sim_iter = iter(sim_dl)  # to make sure we loop through the whole dataset before starting over even as we switch between mms files\n",
    "    for i, mms_file in enumerate(mms_filenames):\n",
    "        print(f\"getting MMS file {mms_file}, number {i+1} of {len(mms_filenames)}\")\n",
    "        # get the mms data from the file\n",
    "        process = psutil.Process()  # start debug loop thingy\n",
    "        print(process.memory_info().rss/1024/1024)\n",
    "        mms_data_dict = get_mms_data(mms_file)\n",
    "        print(process.memory_info().rss/1024/1024)\n",
    "        if len(mms_data_dict) == 0:  # data was not loaded, skip this file\n",
    "            continue\n",
    "        mms_dl = format_mms_data(mms_data_dict)\n",
    "        print(process.memory_info().rss/1024/1024)            \n",
    "        training_step = train_adda(sim_dl, mms_dl, feat_sim, feat_mms, discrim, loss_fn, optim_disc,\n",
    "                                   optim_feat, iter_source = sim_iter)\n",
    "        sim_iter = training_step[\"iter_source\"]\n",
    "        loss_feat = training_step[\"loss_feat\"]\n",
    "        loss_disc = training_step[\"loss_disc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb2efe-1a87-4e8d-81ad-895a57eb9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].plot(loss_disc)\n",
    "ax[1].plot(loss_feat)\n",
    "ax[0].set(title=\"Discriminator loss\", xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "ax[1].set(title=\"MMS Feature Extractor loss\", xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "fig.savefig(\"/tigress/kendrab/analysis-notebooks/model_outs/scratchwork/training_losses.svg\")  # TODO: save model and training info to its own folder\n",
    "plt.close(fig='all')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c782a80-444d-479c-91a8-1ddcaca29149",
   "metadata": {},
   "source": [
    "Starting Epoch 1\n",
    "getting MMS file 2020-07-15T20-25-30_2020-07-16T00-04-00.h5, number 1 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2407.25\n",
    "2659.3125\n",
    "2716.8125\n",
    "getting MMS file 2019-08-13T17-50-30_2019-08-13T19-38-30.h5, number 2 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2716.875\n",
    "3013.0\n",
    "2884.5\n",
    "getting MMS file 2021-07-14T15-33-30_2021-07-14T23-55-00.h5, number 3 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2884.5\n",
    "3098.0\n",
    "3075.9375\n",
    "getting MMS file 2018-08-08T08-44-30_2018-08-08T18-17-30.h5, number 4 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "3075.9375\n",
    "3330.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
