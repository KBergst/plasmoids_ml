{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9704d4-67e0-48fc-bea3-c8878a2fc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-objective hyperparameter optimization on NDB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36942150-38b4-4b42-8cb2-bb8dc9bb39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# import gc  # debug that memory leak\n",
    "# import tracemalloc  # DEBUG THAT MEMORY LEAK\n",
    "import psutil  # :( mem leak\n",
    "import h5py\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import torchinfo\n",
    "import optuna\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "dtype = torch.double\n",
    "\n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_mms_data.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/adda_constructor.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/ndb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ee82cc-8e22-4d49-b146-cbbc154996c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "mms_epochs = 1  # number of times to loop through the entire mms dataset (start with 1 lmao)\n",
    "ndb_samples = 10000\n",
    "name='adda_ndb'\n",
    "\n",
    "rank = os.environ.get(\"OMPI_COMM_WORLD_RANK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15690aa3-6188-4ca4-9c7d-95c0ae87bcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/torch-env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_model.ipynb\n",
    "batch_size = 11\n",
    "# TODO?: enforce that key variables that need to exist later down the pipeline\n",
    "# are populated by import_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64919069-602c-44e4-a6fe-5566f4dd5441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Flatten: 1-1                           --\n",
       "├─LazyLinear: 1-2                        2,387\n",
       "=================================================================\n",
       "Total params: 2,387\n",
       "Trainable params: 2,387\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the model's features (before classification step)\n",
    "# get_graph_node_names(model)\n",
    "return_nodes = {'post_merge_layers.2' : 'features'}\n",
    "feat_sim = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "mock_data = torch.ones((batch_size, 1, input_length), dtype=dtype)\n",
    "tsum = torchinfo.summary(feat_sim, input_data = [mock_data for i in range(7)])\n",
    "feat_shape = tsum.summary_list[-1].output_size\n",
    "# extract the classifier part\n",
    "all_classifier = nn.Sequential(*list(model.children())[-1][-2:])\n",
    "torchinfo.summary(all_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acad2fe-aa9e-4b61-9bc8-719b6cdee894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425291, 1, 89)\n",
      "(425291, 2, 11)\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Load the sim data\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_sim_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdf29a4-095b-4f40-b294-c059b7685b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mms data locations and shuffle the files to not be chronological\n",
    "global_mms_filenames = get_filenames()\n",
    "debug_filename = \"2021-08-18T04-48-00_2021-08-20T04-30-30.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604828d-fcbd-4689-9b48-8e338b880f5e",
   "metadata": {},
   "source": [
    "### Time to do some training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f97e80-6159-41b0-905e-2881ffa624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # timing stuff\n",
    "    start = datetime.now(timezone.utc)  # for timing\n",
    "    time_str = start.strftime(\"%H%M%S\")\n",
    "    date_str = start.strftime(\"%d-%m-%y\")\n",
    "    start_str = date_str + time_str\n",
    "\n",
    "    # make suggestions\n",
    "    mms_num_conv = trial.suggest_int('num_conv', 1, 2)  # for separate layers but also for end layer (so really there are num_conv*2 layers)\n",
    "    mms_kp_limit = int(input_length**(1/(mms_num_conv+1)))  # a non-maximal upper bound on sizes to avoid running out of length\n",
    "    print(f\"kernel eqn limit kp={kp_limit}\")\n",
    "    min_pool_size=2\n",
    "    mms_kernel_size = trial.suggest_int('kernel_size', 2, min(mms_kp_limit - min_pool_size, 10))  # max size 10 or lower\n",
    "    mms_pool_size = trial.suggest_int('pool_size', min_pool_size, min(mms_kp_limit - mms_kernel_size, 5))\n",
    "    mms_out_channels = trial.suggest_int('out_channels', 8, 56)  # like 'filters' in keras\n",
    "    mms_learning_rate = trial.suggest_float('learning_rate', 0.001, 0.003, log=True)  # CANT CHANGE LOG NOW >:(\n",
    "    mms_dropout_fraction = trial.suggest_float('dropout', 0, 0.3)    \n",
    "    discrim_width = trial.suggest_int('discrim_width', 30, 200)\n",
    "    discrim_length = trial.suggest_int('discrim_length', 1, 3)  # number of layer for discrim\n",
    "    # build the domain adaptation structure\n",
    "    discrim, feat_mms = create_adda(mms_num_conv, mms_kp_limit, mms_kernel_size, mms_pool_size, mms_out_channels, mms_learning_rate,\n",
    "                                    mms_dropout_fraction, discrim_width, discrim_length, feat_shape)\n",
    "    hyperparams_adda = {'mms_num_conv': mms_num_conv, 'mms_kp_limit': mms_kp_limit, 'mms_kernel_size':mms_kernel_size, 'mms_pool_size':mms_pool_size,\n",
    "                        'mms_out_channels':mms_out_channels, 'mms_learning_rate':mms_learning_rate, 'mms_dropout_fraction':mms_dropout_fraction,\n",
    "                        'discrim_width':discrim_width, 'discrim_length':discrim_length, 'feat_shape':feat_shape}\n",
    "\n",
    "    loss_fn_disc = nn.BCEWithLogitsLoss()\n",
    "    loss_fn_feat = nn.BCEWithLogitsLoss()\n",
    "    optim_disc = torch.optim.Adam(discrim.parameters(),lr=mms_learning_rate)\n",
    "    optim_feat = torch.optim.Adam(feat_mms.parameters(),lr=mms_learning_rate)\n",
    "    sum_loss_feat = 0\n",
    "    sum_loss_disc = 0\n",
    "\n",
    "    mms_filenames = global_mms_filenames\n",
    "    for epoch in range(mms_epochs):\n",
    "        loss_feat = []\n",
    "        loss_disc = []\n",
    "        print(f\"Starting Epoch {epoch+1}\")\n",
    "        mms_filenames = shuffle(mms_filenames)\n",
    "        sim_iter = iter(sim_dl)  # to make sure we loop through the whole dataset before starting over even as we switch between mms files\n",
    "        for i, mms_file in enumerate(mms_filenames):\n",
    "            print(f\"getting MMS file {mms_file}, number {i+1} of {len(mms_filenames)}\")\n",
    "            # get the mms data from the file\n",
    "            process = psutil.Process()  # start debug loop thingy\n",
    "#            print(process.memory_info().rss/1024/1024)\n",
    "            mms_data_dict = get_mms_data(mms_file)\n",
    "#            print(process.memory_info().rss/1024/1024)\n",
    "            if len(mms_data_dict) == 0:  # data was not loaded, skip this file\n",
    "                continue\n",
    "            mms_dl = format_mms_data(mms_data_dict)\n",
    "#            print(process.memory_info().rss/1024/1024)            \n",
    "            training_step = train_adda(sim_dl, mms_dl, feat_sim, feat_mms, discrim, loss_fn_disc, loss_fn_feat, optim_disc,\n",
    "                                       optim_feat, iter_source = sim_iter)\n",
    "            sim_iter = training_step[\"iter_source\"]\n",
    "            loss_feat += training_step[\"loss_feat\"]\n",
    "            loss_disc += training_step[\"loss_disc\"]\n",
    "\n",
    "\n",
    "        sum_loss_feat = sum(loss_feat)\n",
    "        sum_loss_disc = sum(loss_disc)\n",
    "    # calculate ndb score\n",
    "    mms_features, _ = get_mms_features(feature_extractor=feat_mms, n=ndb_samples)\n",
    "    ndb_sim_dl = DataLoader(sim_dset, batch_size = ndb_samples, shuffle=True, drop_last=True)\n",
    "    ndb_sim_samples = next(iter(ndb_sim_dl))\n",
    "    sim_features = get_sim_features(feature_extractor = feat_sim, samples = ndb_sim_samples, n = ndb_samples)[\"features\"]\n",
    "    mms_features_flat = torch.flatten(mms_features, start_dim=1).detach()\n",
    "    sim_features_flat = torch.flatten(sim_features, start_dim=1).detach()\n",
    "    ndb = ndb_score(sim_features_flat, mms_features_flat)\n",
    "    \n",
    "    # save if we are retrieving a specific trial\n",
    "    if study.user_attrs['save']:\n",
    "        print(\"Saving model...\")  # DEBUG\n",
    "\n",
    "        if rank is not None:\n",
    "            time_str += f\"_{rank}\"  # differentiate between mpi ranks that started at same second\n",
    "        log_file, _, _, file_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                name, date_str, time_str)\n",
    "        # Dump information to file\n",
    "        with open(log_file, 'w') as log:\n",
    "            log.write(f\"MMS model {name} domain adapted on {start_str}\\n\")\n",
    "            log.write(f\"using model file {model_file}\\n\")\n",
    "            log.write(f\"trial number: {trial.number}\\n\")\n",
    "            log.write(f\"Feature extractor loss: {sum_loss_feat}\\n\")\n",
    "            log.write(f\"Discriminator loss: {sum_loss_disc}\\n\")\n",
    "            log.write(f\"NDB score: {ndb}\\n\")\n",
    "            log.write(\"Hyperparameters:\\n\")\n",
    "            for key in hyperparams_adda.keys():\n",
    "                log.write(f\"{key}\\t\\t{hyperparams_adda[key]}\\n\")\n",
    "                \n",
    "        # save the mms classifier\n",
    "        class MMSModel(nn.Module):\n",
    "            def __init__(self, feat_extract, classifier):\n",
    "                super().__init__()\n",
    "                self.feat_extract = feat_extract\n",
    "                self.classifier = classifier\n",
    "            \n",
    "            def forward(self, bx, by, bz, ex, ey, ez, jy):\n",
    "                features = self.feat_extract(bx, by, bz, ex, ey, ez, jy)\n",
    "                logits = self.classifier(features)\n",
    "                return logits\n",
    "            \n",
    "        mms_classifier = MMSModel(feat_mms, all_classifier)\n",
    "        print(torchinfo.summary(mms_classifier))\n",
    "        torch.save(mms_classifier.state_dict(), file_start+\"mms_classifier_statedict.tar\")\n",
    "        # save the discriminator\n",
    "        torch.save(discrim.state_dict(), file_start+\"discriminator_statedict.tar\")              \n",
    "    \n",
    "    # trial ended\n",
    "    end = datetime.now(timezone.utc)\n",
    "    print(f\"trial execution time (s): {(end-start).total_seconds()}\")\n",
    "    return ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a63e4ec-e85d-45a9-894b-9ff77f22ed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel eqn limit kp=9\n",
      "Starting Epoch 1\n",
      "getting MMS file 2018-07-14T20-13-00_2018-07-15T00-35-00.h5, number 1 of 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/torch-env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "components loaded in order ['B', 'E', 'j', 'time']\n",
      "training on 425291 source samples and 656 target samples\n",
      "discrim loss: 1.5279083579177888, feat extract loss: 0.7148340462514934, sample 550/649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-20 14:44:14,712] Trial 2 finished with value: 0.92 and parameters: {'num_conv': 2, 'kernel_size': 2, 'pool_size': 2, 'out_channels': 47, 'learning_rate': 0.002472484482157696, 'dropout': 0.1510181627464457, 'discrim_width': 183, 'discrim_length': 2}. Best is trial 2 with value: 0.92.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin number 0 found statistically different with z-score 25.06245100574046 > 1.96\n",
      "Bin number 1 found statistically different with z-score 11.701757632746013 > 1.96\n",
      "Bin number 2 found statistically different with z-score 19.943827285976244 > 1.96\n",
      "Bin number 3 found statistically different with z-score 17.33339579613343 > 1.96\n",
      "Bin number 4 found statistically different with z-score 26.592276007173666 > 1.96\n",
      "Bin number 5 found statistically different with z-score 19.11967141289623 > 1.96\n",
      "Bin number 6 found statistically different with z-score 127.09214578910922 > 1.96\n",
      "Bin number 7 found statistically different with z-score 29.185541984320274 > 1.96\n",
      "Bin number 8 found statistically different with z-score 25.5464847504731 > 1.96\n",
      "Bin number 9 found statistically different with z-score 14.17744509767187 > 1.96\n",
      "Bin number 10 found statistically different with z-score 12.498425196844146 > 1.96\n",
      "Bin number 11 found statistically different with z-score 5.244547367700472 > 1.96\n",
      "Bin number 12 found statistically different with z-score 17.8318908015268 > 1.96\n",
      "Bin number 13 found statistically similar with z-score 1.4142842783549567 < 1.96\n",
      "Bin number 14 found statistically different with z-score 17.422338863585164 > 1.96\n",
      "Bin number 15 found statistically different with z-score 20.58625074372289 > 1.96\n",
      "Bin number 16 found statistically different with z-score 18.736480753199327 > 1.96\n",
      "Bin number 17 found statistically different with z-score 25.899157682502302 > 1.96\n",
      "Bin number 18 found statistically different with z-score 16.071503936926113 > 1.96\n",
      "Bin number 19 found statistically different with z-score 15.263278452777286 > 1.96\n",
      "Bin number 20 found statistically similar with z-score 1.7321807259954505 < 1.96\n",
      "Bin number 21 found statistically different with z-score 21.91488415108522 > 1.96\n",
      "Bin number 22 found statistically different with z-score 22.991558290603713 > 1.96\n",
      "Bin number 23 found statistically different with z-score 22.763917618798956 > 1.96\n",
      "Bin number 24 found statistically different with z-score 25.337030510281895 > 1.96\n",
      "trial execution time (s): 56.732712\n"
     ]
    }
   ],
   "source": [
    "# assume study already made\n",
    "\"\"\" study = optuna.create_study(study_name='adda_optim_ndb',storage=\"mysql+mysqldb://optunauser:Frikkenoptuna@stellar-intel.princeton.edu:47793/adda_ndb\", direction='minimize')\"\"\"\n",
    "study = optuna.load_study(study_name='adda_optim_ndb',storage=\"mysql+mysqldb://optunauser:Frikkenoptuna@stellar-intel.princeton.edu:47793/adda_ndb\")\n",
    "# regular training with saving models\n",
    "study.set_user_attr('save',True)\n",
    "study.optimize(objective, n_trials=2)\n",
    "# # bringing back the best one to train a new model\n",
    "# trial_num = study.user_attrs['knee_trial_num']\n",
    "# best_params = study.trials[trial_num].params\n",
    "# study.enqueue_trial(best_params, skip_if_exists=False)\n",
    "# study.set_user_attr('save',True)\n",
    "# study.optimize(objective, n_trials=4)\n",
    "# objective(study.trials[trial_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cb2efe-1a87-4e8d-81ad-895a57eb9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the loss\n",
    "# fig, ax = plt.subplots(2)\n",
    "# ax[0].plot(loss_disc)\n",
    "# ax[1].plot(loss_feat)\n",
    "# ax[0].set(title=\"Discriminator loss\", xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "# ax[1].set(title=\"MMS Feature Extractor loss\", xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "# fig.savefig(\"/tigress/kendrab/analysis-notebooks/model_outs/scratchwork/training_losses.svg\")  # TODO: save model and training info to its own folder\n",
    "# plt.close(fig='all')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c782a80-444d-479c-91a8-1ddcaca29149",
   "metadata": {},
   "source": [
    "Starting Epoch 1\n",
    "getting MMS file 2020-07-15T20-25-30_2020-07-16T00-04-00.h5, number 1 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2407.25\n",
    "2659.3125\n",
    "2716.8125\n",
    "getting MMS file 2019-08-13T17-50-30_2019-08-13T19-38-30.h5, number 2 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2716.875\n",
    "3013.0\n",
    "2884.5\n",
    "getting MMS file 2021-07-14T15-33-30_2021-07-14T23-55-00.h5, number 3 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2884.5\n",
    "3098.0\n",
    "3075.9375\n",
    "getting MMS file 2018-08-08T08-44-30_2018-08-08T18-17-30.h5, number 4 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "3075.9375\n",
    "3330.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
