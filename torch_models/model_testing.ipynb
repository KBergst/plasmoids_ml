{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b060324f-e4f6-4144-b3a2-5a31d717e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dtype = torch.double\n",
    "   \n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb13a855-8f8e-478b-a09a-79717e599bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"/tigress/kendrab/analysis-notebooks/model_outs/01-08-23/samples/A131937_modelfile.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc658f-0304-4fe6-b3b0-24375de3e3b8",
   "metadata": {},
   "source": [
    "### Paste model parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc584682-4019-4d70-b817-06fa436ecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"A\"\n",
    "\n",
    "# hyperparameters\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "out_channels = 32  # like 'filters' in keras\n",
    "thinning_factor = [0.9, None]\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor}\n",
    "\n",
    "# other parameters\n",
    "batch_size = 256  # idk what this should be for best performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c30a06-6046-418b-a4a9-9a89a37671b6",
   "metadata": {},
   "source": [
    "### Paste model class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55300d36-b2dd-4583-834e-30149d850f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(nn.Module):\n",
    "    \"\"\" 1D CNN Model \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define these all separately because they will get different weights\n",
    "        # consider smooshing these together into one convolution with in_channels=5\n",
    "        self.bx_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.by_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.bz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.jy_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.vz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        # TODO split this into CNN and classifier parts to facilitate domain adaptation\n",
    "        self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                         padding='valid'),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.MaxPool1d(pool_size),\n",
    "                                               nn.Flatten(),\n",
    "                                               nn.LazyLinear(stride*2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Unflatten(1,(2,stride)))\n",
    "                                               \n",
    "\n",
    "    def forward(self, bx, by, bz, jy, vz):\n",
    "        bx_proc = self.bx_layers(bx)\n",
    "        by_proc = self.by_layers(by)\n",
    "        bz_proc = self.bz_layers(bz)\n",
    "        jy_proc = self.jy_layers(jy)\n",
    "        vz_proc = self.vz_layers(vz)\n",
    "        combined = .2*(bx_proc + by_proc + bz_proc + jy_proc + vz_proc)\n",
    "        logits = self.post_merge_layers(combined)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7ec72df-8c7d-4712-a576-b3e793d7b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)  # number of samples\n",
    "    tot_points = size*stride\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss_sum, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, bx, by, bz, jy, vz, _, _, y in dataloader:\n",
    "            pred = model(bx, by, bz, jy, vz)\n",
    "            test_loss_sum += loss_fn(pred, y).item()  # .item() fetches the python scalar\n",
    "            # number of correct per-point predictions\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    ##### FIX VECTOR LOSS STUFF\n",
    "    ##### WHAT SHAPE SHOULD LOSS BE? FIGURE IT OUT AND MAKE IT SO\n",
    "    ##### THEN HAVE THE DIAGNOSTICS CALCULATED CORRECTLY\n",
    "    test_loss_sum /= num_batches\n",
    "    correct /= tot_points\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss_sum:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682e49a-2605-4d1f-af99-3136af871062",
   "metadata": {},
   "source": [
    "### Restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb431f6b-ae64-4f07-ae8e-37308d009a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelA(\n",
       "  (bx_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (by_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bz_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (jy_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (vz_layers): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (post_merge_layers): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=valid)\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): LazyLinear(in_features=0, out_features=20, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Unflatten(dim=1, unflattened_size=(2, 10))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelA()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "checkpoint = torch.load(model_file)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "model.eval()  # set to correct mode to get the correct results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ee598-13d3-4968-979a-dbf7d3d16332",
   "metadata": {},
   "source": [
    "### Load and preprocess the data to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b5c521f-8b8b-4a05-9ee1-903e7a1af219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['bx', 'bx_mms', 'bx_smooth', 'by', 'by_mms', 'bz', 'bz_mms', 'bz_smooth', 'current_sheets', 'd_per_de', 'ex', 'ex_mms', 'ey', 'ey_mms', 'ez', 'ez_mms', 'flux_fn', 'fluxfn_hessian_det', 'jy', 'jy_mms', 'o_coords', 'o_structures', 's', 'separatrices', 'topo', 'unit_vec', 'vx_mms', 'vz', 'x_coords', 'x_mms', 'z_mms', 'zero_pt']>\n",
      "[-208.41887901 -208.1949515  -207.97102398 -207.74709647 -207.52316895\n",
      " -207.29924143 -207.07531392 -206.8513864  -206.62745889 -206.40353137\n",
      " -206.17960386 -205.95567634 -205.73174883 -205.50782131 -205.2838938\n",
      " -205.05996628 -204.83603876 -204.61211125 -204.38818373 -204.16425622\n",
      " -203.9403287  -203.71640119 -203.49247367 -203.26854616 -203.04461864\n",
      " -202.82069113 -202.59676361 -202.37283609 -202.14890858 -201.92498106\n",
      " -201.70105355 -201.47712603 -201.25319852 -201.029271   -200.80534349\n",
      " -200.58141597 -200.35748846 -200.13356094 -199.90963342 -199.68570591\n",
      " -199.46177839 -199.23785088 -199.01392336 -198.78999585 -198.56606833\n",
      " -198.34214082 -198.1182133  -197.89428579 -197.67035827 -197.44643075\n",
      " -197.22250324 -196.99857572 -196.77464821 -196.55072069 -196.32679318\n",
      " -196.10286566 -195.87893815 -195.65501063 -195.43108312 -195.2071556\n",
      " -194.98322808 -194.75930057 -194.53537305 -194.31144554 -194.08751802\n",
      " -193.86359051 -193.63966299 -193.41573548 -193.19180796 -192.96788045\n",
      " -192.74395293 -192.52002541 -192.2960979  -192.07217038 -191.84824287\n",
      " -191.62431535 -191.40038784 -191.17646032 -190.95253281 -190.72860529\n",
      " -190.50467778 -190.28075026 -190.05682274 -189.83289523 -189.60896771\n",
      " -189.3850402  -189.16111268 -188.93718517 -188.71325765 -188.48933014\n",
      " -188.26540262 -188.04147511 -187.81754759 -187.59362008 -187.36969256\n",
      " -187.14576504 -186.92183753 -186.69791001 -186.4739825  -186.25005498\n",
      " -186.02612747 -185.80219995 -185.57827244 -185.35434492 -185.13041741\n",
      " -184.90648989 -184.68256237 -184.45863486 -184.23470734 -184.01077983\n",
      " -183.78685231 -183.5629248  -183.33899728 -183.11506977 -182.89114225\n",
      " -182.66721474 -182.44328722 -182.2193597  -181.99543219 -181.77150467\n",
      " -181.54757716 -181.32364964 -181.09972213 -180.87579461 -180.6518671\n",
      " -180.42793958 -180.20401207 -179.98008455 -179.75615703 -179.53222952\n",
      " -179.308302   -179.08437449 -178.86044697 -178.63651946 -178.41259194\n",
      " -178.18866443 -177.96473691 -177.7408094  -177.51688188 -177.29295436\n",
      " -177.06902685 -176.84509933 -176.62117182 -176.3972443  -176.17331679\n",
      " -175.94938927 -175.72546176 -175.50153424 -175.27760673 -175.05367921\n",
      " -174.82975169 -174.60582418 -174.38189666 -174.15796915 -173.93404163\n",
      " -173.71011412 -173.4861866  -173.26225909 -173.03833157 -172.81440406\n",
      " -172.59047654 -172.36654902 -172.14262151 -171.91869399 -171.69476648\n",
      " -171.47083896 -171.24691145 -171.02298393 -170.79905642 -170.5751289\n",
      " -170.35120139 -170.12727387 -169.90334635 -169.67941884 -169.45549132\n",
      " -169.23156381 -169.00763629 -168.78370878 -168.55978126 -168.33585375\n",
      " -168.11192623 -167.88799872 -167.6640712  -167.44014368 -167.21621617\n",
      " -166.99228865 -166.76836114 -166.54443362 -166.32050611 -166.09657859\n",
      " -165.87265108 -165.64872356 -165.42479605 -165.20086853 -164.97694101\n",
      " -164.7530135  -164.52908598 -164.30515847 -164.08123095 -163.85730344\n",
      " -163.63337592 -163.40944841 -163.18552089 -162.96159338 -162.73766586\n",
      " -162.51373834 -162.28981083 -162.06588331 -161.8419558  -161.61802828\n",
      " -161.39410077 -161.17017325 -160.94624574 -160.72231822 -160.49839071\n",
      " -160.27446319 -160.05053567 -159.82660816 -159.60268064 -159.37875313\n",
      " -159.15482561 -158.9308981  -158.70697058 -158.48304307 -158.25911555\n",
      " -158.03518804 -157.81126052 -157.587333   -157.36340549 -157.13947797\n",
      " -156.91555046 -156.69162294 -156.46769543 -156.24376791 -156.0198404\n",
      " -155.79591288 -155.57198537 -155.34805785 -155.12413033 -154.90020282\n",
      " -154.6762753  -154.45234779 -154.22842027 -154.00449276 -153.78056524\n",
      " -153.55663773 -153.33271021 -153.1087827  -152.88485518 -152.66092766\n",
      " -152.43700015 -152.21307263 -151.98914512 -151.7652176  -151.54129009\n",
      " -151.31736257 -151.09343506 -150.86950754 -150.64558003 -150.42165251\n",
      " -150.19772499 -149.97379748 -149.74986996 -149.52594245 -149.30201493\n",
      " -149.07808742 -148.8541599  -148.63023239 -148.40630487 -148.18237736\n",
      " -147.95844984 -147.73452232 -147.51059481 -147.28666729 -147.06273978\n",
      " -146.83881226 -146.61488475 -146.39095723 -146.16702972 -145.9431022\n",
      " -145.71917469 -145.49524717 -145.27131965 -145.04739214 -144.82346462\n",
      " -144.59953711 -144.37560959 -144.15168208 -143.92775456 -143.70382705\n",
      " -143.47989953 -143.25597202 -143.0320445  -142.80811698 -142.58418947\n",
      " -142.36026195 -142.13633444 -141.91240692 -141.68847941 -141.46455189\n",
      " -141.24062438 -141.01669686 -140.79276935 -140.56884183 -140.34491431\n",
      " -140.1209868  -139.89705928 -139.67313177 -139.44920425 -139.22527674\n",
      " -139.00134922 -138.77742171 -138.55349419 -138.32956668 -138.10563916\n",
      " -137.88171164 -137.65778413 -137.43385661 -137.2099291  -136.98600158\n",
      " -136.76207407 -136.53814655 -136.31421904 -136.09029152 -135.86636401\n",
      " -135.64243649 -135.41850897 -135.19458146 -134.97065394 -134.74672643\n",
      " -134.52279891 -134.2988714  -134.07494388 -133.85101637 -133.62708885\n",
      " -133.40316134 -133.17923382 -132.95530631 -132.73137879 -132.50745127\n",
      " -132.28352376 -132.05959624 -131.83566873 -131.61174121 -131.3878137\n",
      " -131.16388618 -130.93995867 -130.71603115 -130.49210364 -130.26817612\n",
      " -130.0442486  -129.82032109 -129.59639357 -129.37246606 -129.14853854\n",
      " -128.92461103 -128.70068351 -128.476756   -128.25282848 -128.02890097\n",
      " -127.80497345 -127.58104593 -127.35711842 -127.1331909  -126.90926339\n",
      " -126.68533587 -126.46140836 -126.23748084 -126.01355333 -125.78962581\n",
      " -125.5656983  -125.34177078 -125.11784326 -124.89391575 -124.66998823\n",
      " -124.44606072 -124.2221332  -123.99820569 -123.77427817 -123.55035066\n",
      " -123.32642314 -123.10249563 -122.87856811 -122.65464059 -122.43071308\n",
      " -122.20678556 -121.98285805 -121.75893053 -121.53500302 -121.3110755\n",
      " -121.08714799 -120.86322047 -120.63929296 -120.41536544 -120.19143792\n",
      " -119.96751041 -119.74358289 -119.51965538 -119.29572786 -119.07180035\n",
      " -118.84787283 -118.62394532 -118.4000178  -118.17609029 -117.95216277\n",
      " -117.72823525 -117.50430774 -117.28038022 -117.05645271 -116.83252519\n",
      " -116.60859768 -116.38467016 -116.16074265 -115.93681513 -115.71288762\n",
      " -115.4889601  -115.26503258 -115.04110507 -114.81717755 -114.59325004\n",
      " -114.36932252 -114.14539501 -113.92146749 -113.69753998 -113.47361246\n",
      " -113.24968495 -113.02575743 -112.80182991 -112.5779024  -112.35397488\n",
      " -112.13004737 -111.90611985 -111.68219234 -111.45826482 -111.23433731\n",
      " -111.01040979 -110.78648228 -110.56255476 -110.33862724 -110.11469973\n",
      " -109.89077221 -109.6668447  -109.44291718 -109.21898967 -108.99506215\n",
      " -108.77113464 -108.54720712 -108.32327961 -108.09935209 -107.87542457\n",
      " -107.65149706 -107.42756954 -107.20364203 -106.97971451 -106.755787\n",
      " -106.53185948 -106.30793197 -106.08400445 -105.86007694 -105.63614942\n",
      " -105.4122219  -105.18829439 -104.96436687 -104.74043936 -104.51651184\n",
      " -104.29258433 -104.06865681 -103.8447293  -103.62080178 -103.39687427\n",
      " -103.17294675 -102.94901923 -102.72509172 -102.5011642  -102.27723669\n",
      " -102.05330917 -101.82938166 -101.60545414 -101.38152663 -101.15759911\n",
      " -100.9336716  -100.70974408 -100.48581656 -100.26188905 -100.03796153\n",
      "  -99.81403402  -99.5901065   -99.36617899  -99.14225147  -98.91832396\n",
      "  -98.69439644  -98.47046893  -98.24654141  -98.02261389  -97.79868638\n",
      "  -97.57475886  -97.35083135  -97.12690383  -96.90297632  -96.6790488\n",
      "  -96.45512129  -96.23119377  -96.00726626  -95.78333874  -95.55941122\n",
      "  -95.33548371  -95.11155619  -94.88762868  -94.66370116  -94.43977365\n",
      "  -94.21584613  -93.99191862  -93.7679911   -93.54406359  -93.32013607\n",
      "  -93.09620855  -92.87228104  -92.64835352  -92.42442601  -92.20049849\n",
      "  -91.97657098  -91.75264346  -91.52871595  -91.30478843  -91.08086092\n",
      "  -90.8569334   -90.63300588  -90.40907837  -90.18515085  -89.96122334\n",
      "  -89.73729582  -89.51336831  -89.28944079  -89.06551328  -88.84158576\n",
      "  -88.61765825  -88.39373073  -88.16980321  -87.9458757   -87.72194818\n",
      "  -87.49802067  -87.27409315  -87.05016564  -86.82623812  -86.60231061\n",
      "  -86.37838309  -86.15445558  -85.93052806  -85.70660054  -85.48267303\n",
      "  -85.25874551  -85.034818    -84.81089048  -84.58696297  -84.36303545\n",
      "  -84.13910794  -83.91518042  -83.69125291  -83.46732539  -83.24339787\n",
      "  -83.01947036  -82.79554284  -82.57161533  -82.34768781  -82.1237603\n",
      "  -81.89983278  -81.67590527  -81.45197775  -81.22805024  -81.00412272\n",
      "  -80.7801952   -80.55626769  -80.33234017  -80.10841266  -79.88448514\n",
      "  -79.66055763  -79.43663011  -79.2127026   -78.98877508  -78.76484757\n",
      "  -78.54092005  -78.31699254  -78.09306502  -77.8691375   -77.64520999\n",
      "  -77.42128247  -77.19735496  -76.97342744  -76.74949993  -76.52557241\n",
      "  -76.3016449   -76.07771738  -75.85378987  -75.62986235  -75.40593483\n",
      "  -75.18200732  -74.9580798   -74.73415229  -74.51022477  -74.28629726\n",
      "  -74.06236974  -73.83844223  -73.61451471  -73.3905872   -73.16665968\n",
      "  -72.94273216  -72.71880465  -72.49487713  -72.27094962  -72.0470221\n",
      "  -71.82309459  -71.59916707  -71.37523956  -71.15131204  -70.92738453\n",
      "  -70.70345701  -70.47952949  -70.25560198  -70.03167446  -69.80774695\n",
      "  -69.58381943  -69.35989192  -69.1359644   -68.91203689  -68.68810937\n",
      "  -68.46418186  -68.24025434  -68.01632682  -67.79239931  -67.56847179\n",
      "  -67.34454428  -67.12061676  -66.89668925  -66.67276173  -66.44883422\n",
      "  -66.2249067   -66.00097919  -65.77705167  -65.55312415  -65.32919664\n",
      "  -65.10526912  -64.88134161  -64.65741409  -64.43348658  -64.20955906\n",
      "  -63.98563155  -63.76170403  -63.53777652  -63.313849    -63.08992148\n",
      "  -62.86599397  -62.64206645  -62.41813894  -62.19421142  -61.97028391\n",
      "  -61.74635639  -61.52242888  -61.29850136  -61.07457385  -60.85064633\n",
      "  -60.62671881  -60.4027913   -60.17886378  -59.95493627  -59.73100875\n",
      "  -59.50708124  -59.28315372  -59.05922621  -58.83529869  -58.61137118]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "basedir = '/tigress/kendrab/21032023/'\n",
    "readpaths = []\n",
    "\n",
    "for i in range(10):\n",
    "    totdir = basedir+str(i)+'/'\n",
    "    for j in range(5,60,5):\n",
    "        readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "idx_list = []  # to keep track of which file what sample came from\n",
    "s_list = []\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "jy_list = []\n",
    "vz_list = []\n",
    "x0_list = []\n",
    "x1_list = []\n",
    "topo_list = []\n",
    "\n",
    "train_idx = None\n",
    "\n",
    "for idx, filepath in enumerate(readpaths):\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        print(file.keys())\n",
    "        print(file['x_mms'][0])\n",
    "        sys.exit()\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_smooth'][:])\n",
    "        by_list += list(file['by'][:])\n",
    "        bz_list += list(file['bz_smooth'][:])\n",
    "        jy_list += list(file['jy'][:])\n",
    "        vz_list += list(file['vz'][:]) \n",
    "        x0_list += list(file['x_mms'][:])\n",
    "        x1_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "        \n",
    "        if idx == int(.7*len(readpaths)):  # roughly 70-30 train-test split for now\n",
    "            train_idx = len(bx_list)\n",
    "\n",
    "print(len(bx_list))\n",
    "# do train test split\n",
    "idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "s_train_list = s_list[:train_idx] \n",
    "bx_train_list = bx_list[:train_idx] \n",
    "by_train_list = by_list[:train_idx] \n",
    "bz_train_list = bz_list[:train_idx] \n",
    "jy_train_list = jy_list[:train_idx] \n",
    "vz_train_list = vz_list[:train_idx] \n",
    "x0_train_list = x0_list[:train_idx] \n",
    "x1_train_list = x1_list[:train_idx] \n",
    "topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "idx_test_list = idx_list[train_idx:] \n",
    "s_test_list = s_list[train_idx:] \n",
    "bx_test_list = bx_list[train_idx:] \n",
    "by_test_list = by_list[train_idx:] \n",
    "bz_test_list = bz_list[train_idx:] \n",
    "jy_test_list = jy_list[train_idx:] \n",
    "vz_test_list = vz_list[train_idx:] \n",
    "x0_test_list = x0_list[train_idx:] \n",
    "x1_test_list = x1_list[train_idx:] \n",
    "topo_test_list = topo_list[train_idx:] \n",
    "\n",
    "# BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 50-50 between train and test\n",
    "# lots of magic numbers here but we don't have time to make the code nice rn\n",
    "noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "noplasmoids_paths = []\n",
    "for j in range(5,55,5):\n",
    "        noplasmoids_paths.append(noplasmoids_dir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "for k in range(5):\n",
    "    # training part\n",
    "    with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "        idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_train_list += list(file['s'][:])\n",
    "        bx_train_list += list(file['bx_smooth'][:])\n",
    "        by_train_list += list(file['by'][:])\n",
    "        bz_train_list += list(file['bz_smooth'][:])\n",
    "        jy_train_list += list(file['jy'][:])\n",
    "        vz_train_list += list(file['vz'][:]) \n",
    "        x0_train_list += list(file['x_mms'][:])\n",
    "        x1_train_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_train_list += topo_list_tmp    \n",
    "        \n",
    "    # testing part\n",
    "    with h5py.File(noplasmoids_paths[k+5], 'r') as file:\n",
    "        idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_smooth'][:]]  # check this structure!!!\n",
    "        s_test_list += list(file['s'][:])\n",
    "        bx_test_list += list(file['bx_smooth'][:])\n",
    "        by_test_list += list(file['by'][:])\n",
    "        bz_test_list += list(file['bz_smooth'][:])\n",
    "        jy_test_list += list(file['jy'][:])\n",
    "        vz_test_list += list(file['vz'][:]) \n",
    "        x0_test_list += list(file['x_mms'][:])\n",
    "        x1_test_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_test_list += topo_list_tmp        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c83c91-747b-4ace-9d5d-8f3d738c2986",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85afbd-cb81-4c62-940e-c0259e6bf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk into sliding windows\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "s_train = batch_subsects(s_train_list, input_length, stride)  # not going through training so don't need to shape right\n",
    "bx_train = np.expand_dims(batch_subsects(bx_train_list, input_length, stride),1)\n",
    "by_train = np.expand_dims(batch_subsects(by_train_list, input_length, stride),1)\n",
    "bz_train = np.expand_dims(batch_subsects(bz_train_list, input_length, stride),1)\n",
    "jy_train = np.expand_dims(batch_subsects(jy_train_list, input_length, stride),1)\n",
    "vz_train = np.expand_dims(batch_subsects(vz_train_list, input_length, stride),1)\n",
    "x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "topo_train = np.swapaxes(batch_unpadded_subsects(topo_train_list, padding_length, stride), 1, 2)\n",
    "\n",
    "print(bx_train.shape)\n",
    "\n",
    "idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "s_test = np.expand_dims(batch_subsects(s_test_list, input_length, stride),1)\n",
    "bx_test = np.expand_dims(batch_subsects(bx_test_list, input_length, stride),1)\n",
    "by_test = np.expand_dims(batch_subsects(by_test_list, input_length, stride),1)\n",
    "bz_test = np.expand_dims(batch_subsects(bz_test_list, input_length, stride),1)\n",
    "jy_test = np.expand_dims(batch_subsects(jy_test_list, input_length, stride),1)\n",
    "vz_test = np.expand_dims(batch_subsects(vz_test_list, input_length, stride),1)\n",
    "x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "topo_test = np.swapaxes(batch_unpadded_subsects(topo_test_list, padding_length, stride), 1, 2)\n",
    "\n",
    "# shuffle the segments so they aren't adjacent to overlapping/similar segments\n",
    "idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train, topo_train = \\\n",
    "    shuffle(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test, topo_test = \\\n",
    "    shuffle(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train], topo_train = \\\n",
    "    rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train, x0_train, x1_train],\n",
    "                         topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n",
    "# numpy arrays to torch tensors (while crying about how many lines of code this is surely there is a better way)\n",
    "idx_train = torch.from_numpy(idx_train).to(device, dtype=dtype)\n",
    "s_train = torch.from_numpy(s_train).to(device, dtype=dtype)\n",
    "bx_train = torch.from_numpy(bx_train).to(device, dtype=dtype)\n",
    "by_train = torch.from_numpy(by_train).to(device, dtype=dtype)\n",
    "bz_train = torch.from_numpy(bz_train).to(device, dtype=dtype)\n",
    "jy_train = torch.from_numpy(jy_train).to(device, dtype=dtype)\n",
    "vz_train = torch.from_numpy(vz_train).to(device, dtype=dtype)\n",
    "x0_train = torch.from_numpy(x0_train).to(device, dtype=dtype)\n",
    "x1_train = torch.from_numpy(x1_train).to(device, dtype=dtype)\n",
    "topo_train = torch.from_numpy(topo_train).to(device, dtype=dtype)\n",
    "\n",
    "idx_test = torch.from_numpy(idx_test).to(device, dtype=dtype)\n",
    "s_test = torch.from_numpy(s_test).to(device, dtype=dtype)\n",
    "bx_test = torch.from_numpy(bx_test).to(device, dtype=dtype)\n",
    "by_test = torch.from_numpy(by_test).to(device, dtype=dtype)\n",
    "bz_test = torch.from_numpy(bz_test).to(device, dtype=dtype)\n",
    "jy_test = torch.from_numpy(jy_test).to(device, dtype=dtype)\n",
    "vz_test = torch.from_numpy(vz_test).to(device, dtype=dtype)\n",
    "x0_test = torch.from_numpy(x0_test).to(device, dtype=dtype)\n",
    "x1_test = torch.from_numpy(x1_test).to(device, dtype=dtype)\n",
    "topo_test = torch.from_numpy(topo_test).to(device, dtype=dtype)\n",
    "\n",
    "# collect data into Datasets\n",
    "train_dset = TensorDataset(idx_train, s_train, bx_train, by_train, bz_train, jy_train, vz_train,\n",
    "                              x0_train, x1_train, topo_train)\n",
    "test_dset =  TensorDataset(idx_test, s_test, bx_test, by_test, bz_test, jy_test, vz_test,\n",
    "                              x0_test, x1_test, topo_test)\n",
    "# Make DataLoaders for the training and test data\n",
    "train_dl = DataLoader(train_dset, batch_size = batch_size)\n",
    "test_dl = DataLoader(test_dset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71dfa4-ff50-437c-9fcc-68da2af5240b",
   "metadata": {},
   "source": [
    "### Do whatever testing you want with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acb8ce-63fa-4fee-88be-0cb9c2cfac5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
