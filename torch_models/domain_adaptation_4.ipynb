{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9704d4-67e0-48fc-bea3-c8878a2fc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareto with multi-objective hyperparameter optimization on feature extractor and discriminator losses\n",
    "# BUT IMPLEMENT DIFFERENTLYYYYYYYYYYYYYYYFSKDFJLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59dbd5b0-b58f-4562-88e5-d54a2aae0d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# import gc  # debug that memory leak\n",
    "# import tracemalloc  # DEBUG THAT MEMORY LEAK\n",
    "import psutil  # :( mem leak\n",
    "import h5py\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot, binary_cross_entropy_with_logits\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "import torchinfo\n",
    "import optuna\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "dtype = torch.double\n",
    "\n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_mms_data.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/adda_constructor.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/ndb.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ee82cc-8e22-4d49-b146-cbbc154996c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "mms_epochs = 1  # number of times to loop through the entire mms dataset (start with 1 lmao)\n",
    "name='adda_2'\n",
    "metric_samples = 10000\n",
    "rank = os.environ.get(\"OMPI_COMM_WORLD_RANK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15690aa3-6188-4ca4-9c7d-95c0ae87bcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/torch-env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# load a model\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_model.ipynb\n",
    "batch_size = 11\n",
    "# TODO?: enforce that key variables that need to exist later down the pipeline\n",
    "# are populated by import_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64919069-602c-44e4-a6fe-5566f4dd5441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Sequential                               --\n",
       "├─Flatten: 1-1                           --\n",
       "├─LazyLinear: 1-2                        2,387\n",
       "=================================================================\n",
       "Total params: 2,387\n",
       "Trainable params: 2,387\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the model's features (before classification step)\n",
    "# get_graph_node_names(model)\n",
    "return_nodes = {'post_merge_layers.2' : 'features'}\n",
    "feat_sim = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "mock_data = torch.ones((batch_size, 1, input_length), dtype=dtype)\n",
    "tsum = torchinfo.summary(feat_sim, input_data = [mock_data for i in range(7)])\n",
    "feat_shape = tsum.summary_list[-1].output_size\n",
    "# extract the classifier part\n",
    "all_classifier = nn.Sequential(*list(model.children())[-1][-2:])\n",
    "torchinfo.summary(all_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1acad2fe-aa9e-4b61-9bc8-719b6cdee894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425291, 1, 89)\n",
      "(425291, 2, 11)\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Load the sim data\n",
    "%run /tigress/kendrab/analysis-notebooks/torch_models/import_sim_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdf29a4-095b-4f40-b294-c059b7685b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mms data locations and shuffle the files to not be chronological\n",
    "global_mms_filenames = get_filenames()\n",
    "debug_filename = \"2021-08-18T04-48-00_2021-08-20T04-30-30.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604828d-fcbd-4689-9b48-8e338b880f5e",
   "metadata": {},
   "source": [
    "### Time to do some training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f97e80-6159-41b0-905e-2881ffa624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # timing stuff\n",
    "    start = datetime.now(timezone.utc)  # for timing\n",
    "    time_str = start.strftime(\"%H%M%S\")\n",
    "    date_str = start.strftime(\"%d-%m-%y\")\n",
    "    start_str = date_str + time_str\n",
    "\n",
    "    # make suggestions\n",
    "    mms_num_conv = trial.suggest_int('num_conv', 1, 2)  # for separate layers but also for end layer (so really there are num_conv*2 layers)\n",
    "    mms_kp_limit = int(input_length**(1/(mms_num_conv+1)))  # a non-maximal upper bound on sizes to avoid running out of length\n",
    "    print(f\"kernel eqn limit kp={kp_limit}\")\n",
    "    min_pool_size=2\n",
    "    mms_kernel_size = trial.suggest_int('kernel_size', 2, min(mms_kp_limit - min_pool_size, 10))  # max size 10 or lower\n",
    "    mms_pool_size = trial.suggest_int('pool_size', min_pool_size, min(mms_kp_limit - mms_kernel_size, 5))\n",
    "    mms_out_channels = trial.suggest_int('out_channels', 8, 56)  # like 'filters' in keras\n",
    "    mms_learning_rate = trial.suggest_float('learning_rate', 0.001, 0.003, log=True)  # CANT CHANGE LOG NOW >:(\n",
    "    mms_dropout_fraction = trial.suggest_float('dropout', 0, 0.3)    \n",
    "    discrim_width = trial.suggest_int('discrim_width', 30, 200)\n",
    "    discrim_length = trial.suggest_int('discrim_length', 1, 3)  # number of layer for discrim\n",
    "    # build the domain adaptation structure\n",
    "    discrim, feat_mms = create_adda(mms_num_conv, mms_kp_limit, mms_kernel_size, mms_pool_size, mms_out_channels, mms_learning_rate,\n",
    "                                    mms_dropout_fraction, discrim_width, discrim_length, feat_shape)\n",
    "    hyperparams_adda = {'mms_num_conv': mms_num_conv, 'mms_kp_limit': mms_kp_limit, 'mms_kernel_size':mms_kernel_size, 'mms_pool_size':mms_pool_size,\n",
    "                        'mms_out_channels':mms_out_channels, 'mms_learning_rate':mms_learning_rate, 'mms_dropout_fraction':mms_dropout_fraction,\n",
    "                        'discrim_width':discrim_width, 'discrim_length':discrim_length, 'feat_shape':feat_shape}\n",
    "\n",
    "    loss_fn_disc = nn.BCEWithLogitsLoss()\n",
    "    loss_fn_feat = nn.BCEWithLogitsLoss()\n",
    "    optim_disc = torch.optim.Adam(discrim.parameters(),lr=mms_learning_rate)\n",
    "    optim_feat = torch.optim.Adam(feat_mms.parameters(),lr=mms_learning_rate)\n",
    "    loss_feat = []  # for last mms_epoch only\n",
    "    loss_disc = []\n",
    "\n",
    "    mms_filenames = global_mms_filenames\n",
    "    for epoch in range(mms_epochs):\n",
    "        print(f\"Starting Epoch {epoch+1}\")\n",
    "        mms_filenames = shuffle(mms_filenames)\n",
    "        sim_iter = iter(sim_dl)  # to make sure we loop through the whole dataset before starting over even as we switch between mms files\n",
    "        for i, mms_file in enumerate(mms_filenames):\n",
    "            print(f\"getting MMS file {mms_file}, number {i+1} of {len(mms_filenames)}\")\n",
    "            # get the mms data from the file\n",
    "            process = psutil.Process()  # start debug loop thingy\n",
    "#            print(process.memory_info().rss/1024/1024)\n",
    "            mms_data_dict = get_mms_data(mms_file)\n",
    "#            print(process.memory_info().rss/1024/1024)\n",
    "            if len(mms_data_dict) == 0:  # data was not loaded, skip this file\n",
    "                continue\n",
    "            mms_dl = format_mms_data(mms_data_dict)\n",
    "#            print(process.memory_info().rss/1024/1024)            \n",
    "            training_step = train_adda(sim_dl, mms_dl, feat_sim, feat_mms, discrim, loss_fn_disc, loss_fn_feat, optim_disc,\n",
    "                                       optim_feat, iter_source = sim_iter)\n",
    "            sim_iter = training_step[\"iter_source\"]\n",
    "            loss_feat = training_step[\"loss_feat\"]\n",
    "            loss_disc = training_step[\"loss_disc\"]\n",
    "\n",
    "    # calculate sample losses\n",
    "    discrim.eval()\n",
    "    mms_features, _ = get_mms_features(feature_extractor=feat_mms, n=metric_samples)\n",
    "    metric_sim_dl = DataLoader(sim_dset, batch_size = metric_samples, shuffle=True, drop_last=True)\n",
    "    metric_sim_samples = next(iter(metric_sim_dl))\n",
    "    sim_features = get_sim_features(feature_extractor = feat_sim, samples = metric_sim_samples, n = metric_samples)[\"features\"]\n",
    "    loss_disc_sim = binary_cross_entropy_with_logits(discrim(sim_features), torch.zeros_like(discrim(sim_features)), reduction='mean')\n",
    "    loss_disc_mms = binary_cross_entropy_with_logits(discrim(mms_features), torch.ones_like(discrim(mms_features)), reduction='mean')\n",
    "    loss_disc_sample = (loss_disc_sim + loss_disc_mms)/2\n",
    "    loss_feat_sample = binary_cross_entropy_with_logits(discrim(mms_features), torch.zeros_like(discrim(mms_features)), reduction='mean')\n",
    "    print(\"sample losses:\",loss_disc_sample, loss_feat_sample)   \n",
    "    # save if we are retrieving a specific trial\n",
    "    if study.user_attrs['save']:\n",
    "        print(\"Saving model...\")  # DEBUG\n",
    "\n",
    "        if rank is not None:\n",
    "            time_str += f\"_{rank}\"  # differentiate between mpi ranks that started at same second\n",
    "        log_file, _, _, file_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                name, date_str, time_str)\n",
    "        # Dump information to file\n",
    "        with open(log_file, 'w') as log:\n",
    "            log.write(f\"MMS model {name} domain adapted on {start_str}\\n\")\n",
    "            log.write(f\"using model file {model_file}\\n\")\n",
    "            log.write(f\"trial number: {trial.number}\\n\")\n",
    "            log.write(f\"avg sample Feature extractor loss: {loss_feat_sample}\\n\")\n",
    "            log.write(f\"avg sample Discriminator loss: {loss_disc_sample}\\n\")\n",
    "            log.write(\"Hyperparameters:\\n\")\n",
    "            for key in hyperparams_adda.keys():\n",
    "                log.write(f\"{key}\\t\\t{hyperparams_adda[key]}\\n\")\n",
    "                \n",
    "        # save the mms classifier\n",
    "        class MMSModel(nn.Module):\n",
    "            def __init__(self, feat_extract, classifier):\n",
    "                super().__init__()\n",
    "                self.feat_extract = feat_extract\n",
    "                self.classifier = classifier\n",
    "            \n",
    "            def forward(self, bx, by, bz, ex, ey, ez, jy):\n",
    "                features = self.feat_extract(bx, by, bz, ex, ey, ez, jy)\n",
    "                logits = self.classifier(features)\n",
    "                return logits\n",
    "            \n",
    "        mms_classifier = MMSModel(feat_mms, all_classifier)\n",
    "        print(torchinfo.summary(mms_classifier))\n",
    "        torch.save(mms_classifier.state_dict(), file_start+\"mms_classifier_statedict.tar\")\n",
    "        # save the discriminator\n",
    "        torch.save(discrim.state_dict(), file_start+\"discriminator_statedict.tar\")              \n",
    "    \n",
    "    # trial ended\n",
    "    end = datetime.now(timezone.utc)\n",
    "    print(f\"trial execution time (s): {(end-start).total_seconds()}\")\n",
    "    return loss_feat_sample, loss_disc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a63e4ec-e85d-45a9-894b-9ff77f22ed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel eqn limit kp=9\n",
      "Starting Epoch 1\n",
      "getting MMS file 2020-08-01T07-07-00_2020-08-01T12-41-30.h5, number 1 of 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kendrab/.conda/envs/torch-env/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "components loaded in order ['B', 'E', 'j', 'time']\n",
      "training on 425291 source samples and 21057 target samples\n",
      "discrim loss: 1.2685235245461597, feat extract loss: 1.9116658167146268, sample 550/21054\n",
      "discrim loss: 1.3377656462063583, feat extract loss: 0.753527251969409, sample 1100/21054\n",
      "discrim loss: 1.4355612306003973, feat extract loss: 0.6947617783165823, sample 1650/21054\n",
      "discrim loss: 1.40227460504394, feat extract loss: 0.8351833744734924, sample 2200/21054\n",
      "discrim loss: 1.3894053591699858, feat extract loss: 0.6283817506697008, sample 2750/21054\n",
      "discrim loss: 1.3426788922048771, feat extract loss: 0.7515917998866075, sample 3300/21054\n",
      "discrim loss: 1.453092320582722, feat extract loss: 0.6550173472657863, sample 3850/21054\n",
      "discrim loss: 1.3879178522585893, feat extract loss: 0.7268684230580592, sample 4400/21054\n",
      "discrim loss: 1.2773907541830782, feat extract loss: 0.8389861504676621, sample 4950/21054\n",
      "discrim loss: 1.1567717523400454, feat extract loss: 1.2768851597189999, sample 5500/21054\n",
      "discrim loss: 0.7408521528488661, feat extract loss: 1.8212043666265314, sample 6050/21054\n",
      "discrim loss: 2.0774930339918942, feat extract loss: 0.2935250686818697, sample 6600/21054\n",
      "discrim loss: 1.0957763293509089, feat extract loss: 2.2960206463921136, sample 7150/21054\n",
      "discrim loss: 0.7646253482955319, feat extract loss: 3.0341116125504457, sample 7700/21054\n",
      "discrim loss: 0.3386731389152986, feat extract loss: 2.9913643270683923, sample 8250/21054\n",
      "discrim loss: 0.4715234220161729, feat extract loss: 5.932386055708398, sample 8800/21054\n",
      "discrim loss: 0.5477536851404026, feat extract loss: 1.8245592727940547, sample 9350/21054\n",
      "discrim loss: 1.0840472704890247, feat extract loss: 1.5788600481361614, sample 9900/21054\n",
      "discrim loss: 0.4714924478610475, feat extract loss: 1.8817767755970214, sample 10450/21054\n",
      "discrim loss: 0.5055128607279509, feat extract loss: 1.843761232948845, sample 11000/21054\n",
      "discrim loss: 0.2179599423087539, feat extract loss: 3.600150423809384, sample 11550/21054\n",
      "discrim loss: 0.4487848357592885, feat extract loss: 2.294773358465395, sample 12100/21054\n",
      "discrim loss: 0.19432868578511642, feat extract loss: 2.9016743071654982, sample 12650/21054\n",
      "discrim loss: 1.1143716335390303, feat extract loss: 5.546271307507559, sample 13200/21054\n",
      "discrim loss: 0.7587580738656419, feat extract loss: 1.8771724002084236, sample 13750/21054\n",
      "discrim loss: 0.1885592091892067, feat extract loss: 2.5038537825134615, sample 14300/21054\n",
      "discrim loss: 0.2884961829246795, feat extract loss: 3.537808229098647, sample 14850/21054\n",
      "discrim loss: 0.20987973895833398, feat extract loss: 4.394301935142136, sample 15400/21054\n",
      "discrim loss: 0.1318230969913056, feat extract loss: 4.060256555909913, sample 15950/21054\n",
      "discrim loss: 0.5293962945963951, feat extract loss: 7.617459045458545, sample 16500/21054\n",
      "discrim loss: 0.4955082216783291, feat extract loss: 5.111582336409523, sample 17050/21054\n",
      "discrim loss: 0.08385563689192294, feat extract loss: 6.063599152546024, sample 17600/21054\n",
      "discrim loss: 0.16714156958484638, feat extract loss: 3.0757463670989265, sample 18150/21054\n",
      "discrim loss: 0.06036021079046035, feat extract loss: 3.7812845521545864, sample 18700/21054\n",
      "discrim loss: 0.18484424469491553, feat extract loss: 4.472051697681433, sample 19250/21054\n",
      "discrim loss: 0.2845412340254684, feat extract loss: 5.045869394763766, sample 19800/21054\n",
      "discrim loss: 0.05249075105045449, feat extract loss: 7.487674188198761, sample 20350/21054\n",
      "discrim loss: 0.09035642862862132, feat extract loss: 6.550335937052074, sample 20900/21054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 11:04:45,707] Trial 3 finished with values: [6.804661426932067, 0.08747125308259905] and parameters: {'num_conv': 1, 'kernel_size': 2, 'pool_size': 5, 'out_channels': 40, 'learning_rate': 0.0015329166026905683, 'dropout': 0.2962925044030714, 'discrim_width': 70, 'discrim_length': 2}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0875, dtype=torch.float64, grad_fn=<DivBackward0>) tensor(6.8047, dtype=torch.float64,\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Saving model...\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "MMSModel                                 --\n",
      "├─MMSFeatExtract: 1-1                    --\n",
      "│    └─Sequential: 2-1                   --\n",
      "│    │    └─Conv1d: 3-1                  120\n",
      "│    │    └─LeakyReLU: 3-2               --\n",
      "│    │    └─AvgPool1d: 3-3               --\n",
      "│    │    └─Dropout: 3-4                 --\n",
      "│    └─Sequential: 2-2                   --\n",
      "│    │    └─Conv1d: 3-5                  120\n",
      "│    │    └─LeakyReLU: 3-6               --\n",
      "│    │    └─AvgPool1d: 3-7               --\n",
      "│    │    └─Dropout: 3-8                 --\n",
      "│    └─Sequential: 2-3                   --\n",
      "│    │    └─Conv1d: 3-9                  120\n",
      "│    │    └─LeakyReLU: 3-10              --\n",
      "│    │    └─AvgPool1d: 3-11              --\n",
      "│    │    └─Dropout: 3-12                --\n",
      "│    └─Sequential: 2-4                   --\n",
      "│    │    └─Conv1d: 3-13                 120\n",
      "│    │    └─LeakyReLU: 3-14              --\n",
      "│    │    └─AvgPool1d: 3-15              --\n",
      "│    │    └─Dropout: 3-16                --\n",
      "│    └─Sequential: 2-5                   --\n",
      "│    │    └─Conv1d: 3-17                 120\n",
      "│    │    └─LeakyReLU: 3-18              --\n",
      "│    │    └─AvgPool1d: 3-19              --\n",
      "│    │    └─Dropout: 3-20                --\n",
      "│    └─Sequential: 2-6                   --\n",
      "│    │    └─Conv1d: 3-21                 120\n",
      "│    │    └─LeakyReLU: 3-22              --\n",
      "│    │    └─AvgPool1d: 3-23              --\n",
      "│    │    └─Dropout: 3-24                --\n",
      "│    └─Sequential: 2-7                   --\n",
      "│    │    └─Conv1d: 3-25                 120\n",
      "│    │    └─LeakyReLU: 3-26              --\n",
      "│    │    └─AvgPool1d: 3-27              --\n",
      "│    │    └─Dropout: 3-28                --\n",
      "│    └─Sequential: 2-8                   --\n",
      "│    │    └─Conv1d: 3-29                 6,480\n",
      "│    │    └─LeakyReLU: 3-30              --\n",
      "│    │    └─AvgPool1d: 3-31              --\n",
      "│    └─Sequential: 2-9                   --\n",
      "│    │    └─Flatten: 3-32                --\n",
      "│    │    └─Linear: 3-33                 52,056\n",
      "│    │    └─Unflatten: 3-34              --\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─Flatten: 2-10                     --\n",
      "│    └─LazyLinear: 2-11                  2,387\n",
      "=================================================================\n",
      "Total params: 61,763\n",
      "Trainable params: 61,763\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "trial execution time (s): 183.139948\n"
     ]
    }
   ],
   "source": [
    "# assume study already made\n",
    "\"\"\" study = optuna.create_study(study_name='adda_optim2',storage=\"mysql+mysqldb://optunauser:Frikkenoptuna@stellar-intel.princeton.edu:47793/adda_2\", directions=['minimize','minimize'])\"\"\"\n",
    "# use hyperband pruner \n",
    "study = optuna.load_study(study_name='adda_optim2',storage=\"mysql+mysqldb://optunauser:Frikkenoptuna@stellar-intel.princeton.edu:47793/adda_2\")\n",
    "# regular training\n",
    "study.set_user_attr('save',True)\n",
    "study.optimize(objective, n_trials=1)\n",
    "# # bringing back the best one to train a new model\n",
    "# trial_num = study.user_attrs['knee_trial_num']\n",
    "# best_params = study.trials[trial_num].params\n",
    "# study.enqueue_trial(best_params, skip_if_exists=False)\n",
    "# study.set_user_attr('save',True)\n",
    "# study.optimize(objective, n_trials=4)\n",
    "# objective(study.trials[trial_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cb2efe-1a87-4e8d-81ad-895a57eb9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the loss\n",
    "# fig, ax = plt.subplots(2)\n",
    "# ax[0].plot(loss_disc)\n",
    "# ax[1].plot(loss_feat)\n",
    "# ax[0].set(title=\"Discriminator loss\", xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "# ax[1].set(title=\"MMS Feature Extractor loss\", xlabel=\"training iteration\", ylabel=\"loss\")\n",
    "# fig.savefig(\"/tigress/kendrab/analysis-notebooks/model_outs/scratchwork/training_losses.svg\")  # TODO: save model and training info to its own folder\n",
    "# plt.close(fig='all')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c782a80-444d-479c-91a8-1ddcaca29149",
   "metadata": {},
   "source": [
    "Starting Epoch 1\n",
    "getting MMS file 2020-07-15T20-25-30_2020-07-16T00-04-00.h5, number 1 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2407.25\n",
    "2659.3125\n",
    "2716.8125\n",
    "getting MMS file 2019-08-13T17-50-30_2019-08-13T19-38-30.h5, number 2 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2716.875\n",
    "3013.0\n",
    "2884.5\n",
    "getting MMS file 2021-07-14T15-33-30_2021-07-14T23-55-00.h5, number 3 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "2884.5\n",
    "3098.0\n",
    "3075.9375\n",
    "getting MMS file 2018-08-08T08-44-30_2018-08-08T18-17-30.h5, number 4 of 230\n",
    "overridden with debug file 2021-08-18T04-48-00_2021-08-20T04-30-30.h5\n",
    "3075.9375\n",
    "3330.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
