{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e030ff-19ef-4da5-a052-98905278507f",
   "metadata": {},
   "source": [
    "## Simple model using Bx, By, Bz, Ex, Ey, Ez, jy\n",
    "classifications: o_structures, null\n",
    "\n",
    "fixed length timeseries informed by d_per_de ~ 4\n",
    "\n",
    "Direct translation of Model C but with jy tacked on to see if that was what was improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a10f508-9e73-4a91-a634-8fc144de904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from  torch.nn.functional import one_hot\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dtype = torch.double\n",
    "   \n",
    "# Get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/eval_utils.ipynb\n",
    "\n",
    "start = dt.datetime.now(dt.timezone.utc)  # for timing\n",
    "time_str = start.strftime(\"%H%M%S\")\n",
    "date_str = start.strftime(\"%d-%m-%y\")\n",
    "start_str = date_str + time_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fbc4a-4f4f-4e92-9c78-378b3dfac7f9",
   "metadata": {},
   "source": [
    "### Make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b825e7-e361-4b20-8d59-b1aa86fe026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"D\"\n",
    "\n",
    "# hyperparameters\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "out_channels = 32  # like 'filters' in keras\n",
    "thinning_factor = [0.85, None]\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "batch_size = 256  # idk what this should be for best performance \n",
    "hyperparams = {'learning_rate':learning_rate, 'out_channels':out_channels, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'input_length':input_length, 'stride':stride, 'epochs':epochs, 'thinning_factor':thinning_factor,\n",
    "              'batch_size':batch_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5d8bfbb-78fe-4327-94e3-f64e3a42e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO feed hyperparameters into __init__\n",
    "class ModelD(nn.Module):\n",
    "    \"\"\" 1D CNN Model \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define these all separately because they will get different weights\n",
    "        # consider smooshing these together into one convolution with in_channels=6. Idk if a good idea\n",
    "        self.bx_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.by_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.bz_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.ex_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.ey_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.ez_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        self.jy_layers = nn.Sequential(nn.Conv1d(1, out_channels, kernel_size, padding='valid'),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool1d(pool_size))\n",
    "        \n",
    "        self.post_merge_layers = nn.Sequential(nn.Conv1d(out_channels, out_channels*2, kernel_size,\n",
    "                                                         padding='valid'),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.MaxPool1d(pool_size),\n",
    "                                               nn.Flatten(),\n",
    "                                               nn.LazyLinear(stride*2),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Unflatten(1,(2,stride)))\n",
    "                                               \n",
    "\n",
    "    def forward(self, bx, by, bz, ex, ey, ez, jy):\n",
    "        bx_proc = self.bx_layers(bx)\n",
    "        by_proc = self.by_layers(by)\n",
    "        bz_proc = self.bz_layers(bz)\n",
    "        ex_proc = self.ex_layers(ex)\n",
    "        ey_proc = self.ey_layers(ey)\n",
    "        ez_proc = self.ez_layers(ez)\n",
    "        jy_proc = self.jy_layers(jy)\n",
    "        combined = (bx_proc + by_proc + bz_proc + ex_proc + ey_proc + ez_proc + jy_proc)/6.\n",
    "        logits = self.post_merge_layers(combined)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d82d88a3-d0b4-4e9b-9770-5c7bb5fadbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelC(\n",
      "  (bx_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (by_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bz_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ex_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ey_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ez_layers): Sequential(\n",
      "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (post_merge_layers): Sequential(\n",
      "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=valid)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Flatten(start_dim=1, end_dim=-1)\n",
      "    (4): LazyLinear(in_features=0, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Unflatten(dim=1, unflattened_size=(2, 10))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ModelD().to(device=device, dtype=torch.double)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045a9c8-4242-4aa1-ad0a-e08d2d256305",
   "metadata": {},
   "source": [
    "### Define the training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ad40ec-e97e-47c2-9e6c-39ef5f6049a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)  # the length of a tensordataset is the shared first dim\n",
    "    for batch, (_, _, bx, by, bz, ex, ey, ez, jy, _, _, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(bx, by, bz, ex, ey, ez, jy)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current_sample = (batch+1)*bx.shape[0]\n",
    "            print(f\"mean loss: {loss}, sample {current_sample}/{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b56ed98-c90f-4df8-8df9-2889bea3529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    size = len(dataloader.dataset)  # number of samples\n",
    "    tot_points = size*stride\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss_sum, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, bx, by, bz, ex, ey, ez, jy, _, _, y in dataloader:\n",
    "            pred = model(bx, by, bz, ex, ey, ez, jy)\n",
    "            pred_list.append(pred.cpu().numpy())\n",
    "            test_loss_sum += loss_fn(pred, y).item()  # .item() fetches the python scalar\n",
    "            # number of correct per-point predictions\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    tot_pred = np.concatenate(pred_list, axis=0)\n",
    "    test_loss_sum /= num_batches\n",
    "    correct /= tot_points\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss_sum:>8f} \\n\")    \n",
    "    return tot_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2875d-29d5-43b1-a26c-639ad1797f47",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fb751f5-d5a9-4cb2-b442-11c720d54412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "7800\n",
      "3200\n",
      "8300\n",
      "3700\n"
     ]
    }
   ],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "basedir = '/tigress/kendrab/21032023/'\n",
    "readpaths = []\n",
    "\n",
    "for i in range(10):\n",
    "    totdir = basedir+str(i)+'/'\n",
    "    for j in range(5,60,5):\n",
    "        readpaths.append(totdir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "idx_list = []  # to keep track of which file what sample came from\n",
    "s_list = []\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "ex_list = []\n",
    "ey_list = []\n",
    "ez_list = []\n",
    "jy_list = []\n",
    "x0_list = []\n",
    "x1_list = []\n",
    "topo_list = []\n",
    "\n",
    "train_idx = None\n",
    "\n",
    "for idx, filepath in enumerate(readpaths):\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        idx_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_list += list(file['s'][:])\n",
    "        bx_list += list(file['bx_mms_smooth'][:])\n",
    "        by_list += list(file['by_mms'][:])\n",
    "        bz_list += list(file['bz_mms_smooth'][:])\n",
    "        ex_list += list(file['ex_mms'][:]) \n",
    "        ey_list += list(file['ey_mms'][:])\n",
    "        ez_list += list(file['ez_mms'][:])  # vx_mms is simulation vz  thus the filename \n",
    "        jy_list += list(file['jy_mms'][:])\n",
    "        x0_list += list(file['x_mms'][:])\n",
    "        x1_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_list += topo_list_tmp\n",
    "        \n",
    "        if idx == int(.7*len(readpaths)):  # roughly 70-30 train-test split for now\n",
    "            train_idx = len(bx_list)\n",
    "\n",
    "print(len(bx_list))\n",
    "# do train test split\n",
    "idx_train_list = idx_list[:train_idx]  # to keep track of which file what sample came from\n",
    "s_train_list = s_list[:train_idx] \n",
    "bx_train_list = bx_list[:train_idx] \n",
    "by_train_list = by_list[:train_idx] \n",
    "bz_train_list = bz_list[:train_idx] \n",
    "ex_train_list = ex_list[:train_idx] \n",
    "ey_train_list = ey_list[:train_idx] \n",
    "ez_train_list = ez_list[:train_idx] \n",
    "jy_train_list = jy_list[:train_idx] \n",
    "x0_train_list = x0_list[:train_idx] \n",
    "x1_train_list = x1_list[:train_idx] \n",
    "topo_train_list = topo_list[:train_idx] \n",
    "\n",
    "idx_test_list = idx_list[train_idx:] \n",
    "s_test_list = s_list[train_idx:] \n",
    "bx_test_list = bx_list[train_idx:] \n",
    "by_test_list = by_list[train_idx:] \n",
    "bz_test_list = bz_list[train_idx:] \n",
    "ex_test_list = ex_list[train_idx:] \n",
    "ey_test_list = ey_list[train_idx:] \n",
    "ez_test_list = ez_list[train_idx:]\n",
    "jy_test_list = jy_list[train_idx:]\n",
    "x0_test_list = x0_list[train_idx:] \n",
    "x1_test_list = x1_list[train_idx:] \n",
    "topo_test_list = topo_list[train_idx:] \n",
    "\n",
    "print(len(bx_train_list))\n",
    "print(len(bx_test_list))\n",
    "# BUT WAIT THERE'S MORE! Include the slices from plain ol current sheets. Split 50-50 between train and test\n",
    "# lots of magic numbers here but we don't have time to make the code nice rn\n",
    "noplasmoids_dir = '/tigress/kendrab/06022023/'\n",
    "noplasmoids_paths = []\n",
    "for j in range(5,55,5):\n",
    "        noplasmoids_paths.append(noplasmoids_dir+f\"100samples_idx{j}_bxbybzjyvzexeyez.hdf5\")\n",
    "        \n",
    "for k in range(5):\n",
    "    # training part\n",
    "    with h5py.File(noplasmoids_paths[k], 'r') as file:\n",
    "        idx_train_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_train_list += list(file['s'][:])\n",
    "        bx_train_list += list(file['bx_mms_smooth'][:])\n",
    "        by_train_list += list(file['by_mms'][:])\n",
    "        bz_train_list += list(file['bz_mms_smooth'][:])\n",
    "        ex_train_list += list(file['ex_mms'][:]) \n",
    "        ey_train_list += list(file['ey_mms'][:])\n",
    "        ez_train_list += list(file['ez_mms'][:]) \n",
    "        jy_train_list += list(file['jy_mms'][:])\n",
    "        x0_train_list += list(file['x_mms'][:])\n",
    "        x1_train_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_train_list += topo_list_tmp    \n",
    "        \n",
    "    # testing part\n",
    "    with h5py.File(noplasmoids_paths[k+5], 'r') as file:\n",
    "        idx_test_list += [np.array([idx for i in bx]) for bx in file['bx_mms_smooth'][:]]  # check this structure!!!\n",
    "        s_test_list += list(file['s'][:])\n",
    "        bx_test_list += list(file['bx_mms_smooth'][:])\n",
    "        by_test_list += list(file['by_mms'][:])\n",
    "        bz_test_list += list(file['bz_mms_smooth'][:])\n",
    "        ex_test_list += list(file['ex_mms'][:]) \n",
    "        ey_test_list += list(file['ey_mms'][:])\n",
    "        ez_test_list += list(file['ez_mms'][:])\n",
    "        jy_test_list += list(file['jy_mms'][:])\n",
    "        x0_test_list += list(file['x_mms'][:])\n",
    "        x1_test_list += list(file['z_mms'][:])\n",
    "        topo_list_tmp = list(file['topo'][:])\n",
    "        for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "            topo_list_tmp[i] = torch.from_numpy(topo_list_tmp[i].astype(int) % 2)  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "            topo_list_tmp[i] = one_hot(topo_list_tmp[i], num_classes=2)\n",
    "        topo_test_list += topo_list_tmp  \n",
    "        \n",
    "print(len(bx_train_list))\n",
    "print(len(bx_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5202f-08e6-4807-a81c-cbb750bb30a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce662e3e-9b0f-45f0-bf06-9591e8b92f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(369510, 1, 30)\n",
      "Total batch: 369510\n",
      "Number of null samples: 296973\n",
      "Number of non-null samples: 72537\n",
      "With thinning factor 0.85 will remove 252427 null samples\n"
     ]
    }
   ],
   "source": [
    "# chunk into sliding windows\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "idx_train = batch_unpadded_subsects(idx_train_list, padding_length, stride)\n",
    "s_train = batch_subsects(s_train_list, input_length, stride)  # not going through training so don't need to shape right\n",
    "bx_train = np.expand_dims(batch_subsects(bx_train_list, input_length, stride),1)\n",
    "by_train = np.expand_dims(batch_subsects(by_train_list, input_length, stride),1)\n",
    "bz_train = np.expand_dims(batch_subsects(bz_train_list, input_length, stride),1)\n",
    "ex_train = np.expand_dims(batch_subsects(ex_train_list, input_length, stride),1)\n",
    "ey_train = np.expand_dims(batch_subsects(ey_train_list, input_length, stride),1)\n",
    "ez_train = np.expand_dims(batch_subsects(ez_train_list, input_length, stride),1)\n",
    "jy_train = np.expand_dims(batch_subsects(jy_train_list, input_length, stride),1)\n",
    "x0_train = batch_unpadded_subsects(x0_train_list, padding_length, stride)\n",
    "x1_train = batch_unpadded_subsects(x1_train_list, padding_length, stride)\n",
    "topo_train = np.swapaxes(batch_unpadded_subsects(topo_train_list, padding_length, stride), 1, 2)\n",
    "\n",
    "print(bx_train.shape)\n",
    "\n",
    "idx_test = batch_unpadded_subsects(idx_test_list, padding_length, stride)\n",
    "s_test = np.expand_dims(batch_subsects(s_test_list, input_length, stride),1)\n",
    "bx_test = np.expand_dims(batch_subsects(bx_test_list, input_length, stride),1)\n",
    "by_test = np.expand_dims(batch_subsects(by_test_list, input_length, stride),1)\n",
    "bz_test = np.expand_dims(batch_subsects(bz_test_list, input_length, stride),1)\n",
    "ex_test = np.expand_dims(batch_subsects(ex_test_list, input_length, stride),1)\n",
    "ey_test = np.expand_dims(batch_subsects(ey_test_list, input_length, stride),1)\n",
    "ez_test = np.expand_dims(batch_subsects(ez_test_list, input_length, stride),1)\n",
    "jy_test = np.expand_dims(batch_subsects(jy_test_list, input_length, stride),1)\n",
    "x0_test = batch_unpadded_subsects(x0_test_list, padding_length, stride)\n",
    "x1_test = batch_unpadded_subsects(x1_test_list, padding_length, stride)\n",
    "topo_test = np.swapaxes(batch_unpadded_subsects(topo_test_list, padding_length, stride), 1, 2)\n",
    "\n",
    "# shuffle the segments so they aren't adjacent to overlapping/similar segments\n",
    "idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train, topo_train = \\\n",
    "    shuffle(idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train, topo_train)\n",
    "\n",
    "idx_test, s_test, bx_test, by_test, bz_test, ex_test, ey_test, ez_test, jy_test, x0_test, x1_test, topo_test = \\\n",
    "    shuffle(idx_test, s_test, bx_test, by_test, bz_test, ex_test, ey_test, ez_test, jy_test, x0_test, x1_test, topo_test)\n",
    "\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train], topo_train = \\\n",
    "    rebalance_ctrl_group([idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train, jy_train, x0_train, x1_train],\n",
    "                         topo_train, null_label=[1,0], thinning_factor = thinning_factor[0])\n",
    "# numpy arrays to torch tensors (while crying about how many lines of code this is surely there is a better way)\n",
    "idx_train = torch.from_numpy(idx_train).to(device, dtype=dtype)\n",
    "s_train = torch.from_numpy(s_train).to(device, dtype=dtype)\n",
    "bx_train = torch.from_numpy(bx_train).to(device, dtype=dtype)\n",
    "by_train = torch.from_numpy(by_train).to(device, dtype=dtype)\n",
    "bz_train = torch.from_numpy(bz_train).to(device, dtype=dtype)\n",
    "ex_train = torch.from_numpy(ex_train).to(device, dtype=dtype)\n",
    "ey_train = torch.from_numpy(ey_train).to(device, dtype=dtype)\n",
    "ez_train = torch.from_numpy(ez_train).to(device, dtype=dtype)\n",
    "jy_train = torch.from_numpy(jy_train).to(device, dtype=dtype)\n",
    "x0_train = torch.from_numpy(x0_train).to(device, dtype=dtype)\n",
    "x1_train = torch.from_numpy(x1_train).to(device, dtype=dtype)\n",
    "topo_train = torch.from_numpy(topo_train).to(device, dtype=dtype)\n",
    "\n",
    "idx_test = torch.from_numpy(idx_test).to(device, dtype=dtype)\n",
    "s_test = torch.from_numpy(s_test).to(device, dtype=dtype)\n",
    "bx_test = torch.from_numpy(bx_test).to(device, dtype=dtype)\n",
    "by_test = torch.from_numpy(by_test).to(device, dtype=dtype)\n",
    "bz_test = torch.from_numpy(bz_test).to(device, dtype=dtype)\n",
    "ex_test = torch.from_numpy(ex_test).to(device, dtype=dtype)\n",
    "ey_test = torch.from_numpy(ey_test).to(device, dtype=dtype)\n",
    "ez_test = torch.from_numpy(ez_test).to(device, dtype=dtype)\n",
    "jy_test = torch.from_numpy(jy_test).to(device, dtype=dtype)\n",
    "x0_test = torch.from_numpy(x0_test).to(device, dtype=dtype)\n",
    "x1_test = torch.from_numpy(x1_test).to(device, dtype=dtype)\n",
    "topo_test = torch.from_numpy(topo_test).to(device, dtype=dtype)\n",
    "\n",
    "# collect data into Datasets\n",
    "train_dset = TensorDataset(idx_train, s_train, bx_train, by_train, bz_train, ex_train, ey_train, ez_train,\n",
    "                              jy_train, x0_train, x1_train, topo_train)\n",
    "test_dset =  TensorDataset(idx_test, s_test, bx_test, by_test, bz_test, ex_test, ey_test, ez_test,\n",
    "                              jy_test, x0_test, x1_test, topo_test)\n",
    "# Make DataLoaders for the training and test data\n",
    "train_dl = DataLoader(train_dset, batch_size = batch_size)\n",
    "test_dl = DataLoader(test_dset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67e988-3bff-4396-a0a3-1d78e88231ee",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f665c-81b1-4b50-a1cf-a17b7130b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')  # We are allowing reduction bc backward() \n",
    "                                            # needs a scalar or to specify a different gradient.\n",
    "                                                # Easier this way. Probably.\n",
    "opt = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21773e63-8692-41a1-839b-ef55c3a8b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dl, model, loss_fn, opt)\n",
    "    test_loop(test_dl, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a19c6-5f51-4cf9-9035-73395e9db292",
   "metadata": {},
   "source": [
    "### Make output directories if they do not exist and set up output file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be596ef8-5b96-4d2a-817d-93d5077ef2d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generic_outputs_structure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4150777/708141995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m log_file, cf_file, samplefile_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                 model_name, date_str, time_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generic_outputs_structure' is not defined"
     ]
    }
   ],
   "source": [
    "log_file, cf_file, samplefile_start = generic_outputs_structure(\"/tigress/kendrab/analysis-notebooks/model_outs/\",\n",
    "                                                                model_name, date_str, time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4b1a2-5563-4eae-a671-aac205c79c8e",
   "metadata": {},
   "source": [
    "### Observe the results, dump information to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645da698-e4c9-4afb-87e4-52f4292f95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file, 'w') as log:\n",
    "    log.write(f\"Model {model_name} trained on {start_str}\\n\")\n",
    "    log.write(f\"loss function \\t\\t{loss_fn.__repr__()}\\n\")\n",
    "    log.write(\"Hyperparameters:\\n\")\n",
    "    for key in hyperparams.keys():\n",
    "        log.write(f\"{key}\\t\\t{hyperparams[key]}\\n\")\n",
    "        \n",
    "    log.write(\"Training performance\\n\")     \n",
    "    print(\"Training performance\")\n",
    "    train_topo_pred = test_loop(train_dl, model, loss_fn) \n",
    "    train_1d = np.argmax(topo_train.cpu().numpy(), axis=1).flatten() # for confusion matrix\n",
    "    train_1d_pred = np.argmax(train_topo_pred, axis=1).flatten()  \n",
    "    num_per_cat = [np.sum(topo_train.cpu().numpy()[:,i,:] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "    # TODO CALCULATE RECALL PER CATEGORY\n",
    "\n",
    "    log.write(\"Testing performance\\n\")\n",
    "    print(\"Testing performance\")\n",
    "    test_topo_pred = test_loop(test_dl, model, loss_fn)\n",
    "    test_1d = np.argmax(topo_test.cpu().numpy(), axis=1).flatten() # for confusion matrix\n",
    "    test_1d_pred = np.argmax(test_topo_pred, axis=1).flatten()  \n",
    "    num_per_cat = [np.sum(topo_test.cpu().numpy()[:,i,:] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "\n",
    "    end = dt.datetime.now(dt.timezone.utc)    \n",
    "    log.write(f\"runtime_seconds\\t\\t{(end-start).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887db36-d4dc-4ed6-b97e-3fb0bbbe5a63",
   "metadata": {},
   "source": [
    "### Plot confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f936f-ed70-40df-8e20-d2e8c05a4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_traintest_cf_matrices(train_1d, train_1d_pred, test_1d, test_1d_pred, cf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d7b0f-d81d-4298-a0ac-064bfa569833",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9a957-c088-42dc-a581-5b82bc8b83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'loss_fn': loss_fn}, samplefile_start+\"_modelfile.tar\")\n",
    "\n",
    "## To load:\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss_fn = checkpoint['loss_fn']\n",
    "\n",
    "# model.eval()\n",
    "# # - or -\n",
    "# model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359457fe-6641-478e-af1a-c892d545a6a9",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e393a75-06ef-405b-a3da-3148d0f8d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reps([bx_train.cpu().detach().numpy(), \n",
    "           by_train.cpu().detach().numpy(),\n",
    "           bz_train.cpu().detach().numpy(),\n",
    "           ex_train.cpu().detach().numpy(),\n",
    "           ey_train.cpu().detach().numpy(),\n",
    "           ez_train.cpu().detach().numpy(),\n",
    "           jy_train.cpu().detach().numpy()], ['bx','by','bz', 'ex', 'ey','ez', 'jy'], s_train.cpu().detach().numpy(),\n",
    "          topo_train.cpu().detach().numpy(),\n",
    "          train_topo_pred, samplefile_start, inputs_padding=padding_length, \n",
    "          true_coords=np.stack([x0_train.cpu().detach().numpy(), x1_train.cpu().detach().numpy()], axis=-1), exs_per_cat=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
