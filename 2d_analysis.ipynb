{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "whole-banks",
   "metadata": {},
   "source": [
    "### Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-cursor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rolled-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import pyvpic\n",
    "from dataframework.src.datasets.vpicdataset import VPICDataset\n",
    "from skimage import measure  # for finding contours\n",
    "from skimage.segmentation import flood # for defining structures\n",
    "import scipy.ndimage as nd   # for smoothing/filtering\n",
    "from scipy.spatial import Delaunay, ConvexHull\n",
    "import scipy.signal as signal\n",
    "import scipy.interpolate as interp\n",
    "from numpy.polynomial import Polynomial\n",
    "import sys  # for debugging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PARAMS ADDED, FUNCTIONALIITY NOT ADDED YET!!!! SORRY\n",
      "Added bx Variable\n",
      "Added by Variable\n",
      "Added bz Variable\n",
      "Added jy Variable\n"
     ]
    }
   ],
   "source": [
    "files = ['/tigress/kendrab/03082021/data.h5','tigress/kendrab/03082021/info']\n",
    "kwargs = {'get_vars' : ['bx', 'by', 'bz', 'jy']}\n",
    "data_03082021 = VPICDataset(vpicfiles=files, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guilty-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PARAMS ADDED, FUNCTIONALIITY NOT ADDED YET!!!! SORRY\n",
      "Added bx Variable\n",
      "Added by Variable\n",
      "Added bz Variable\n",
      "Added jy Variable\n"
     ]
    }
   ],
   "source": [
    "files = ['/tigress/kendrab/02232021/data.h5','tigress/kendrab/02232021/info']\n",
    "data_02232021 = VPICDataset(vpicfiles=files, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-resort",
   "metadata": {},
   "source": [
    "### Function definitons ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collaborative-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PULL MORE STUFF OVER HERE TO STOP REPEATING SO MUCH GD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gross-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(data):\n",
    "    \"\"\" Flatten the data as structured in datasets to be used with an ND Interpolator from scipy\"\"\"\n",
    "    if len(data.shape) == 1:  # already flat\n",
    "        return data\n",
    "    elif len(data.shape) == 2:  # need to flatten everything\n",
    "        return data.flatten()\n",
    "    elif len(data.shape) == 3:  # need second dimension to be time, first dimension has length (number of points)\n",
    "        return np.column_stack(tuple(data[i].flatten() for i in range(data.shape[0])))\n",
    "    else:\n",
    "        raise ValueError(f\"Data has unacceptable dimension {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "critical-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccw(A,B,C):\n",
    "    \"\"\" Test whether the three points are listed in a counterclockwise order, but ~vectorized~\n",
    "    Can't handle colinear points because I'm not handling edge cases rn\n",
    "    A- array, shape (n_pts,2)\n",
    "    B- array, shape (n_pts,2)\n",
    "    C- array, shape (n_pts,2)\n",
    "    \"\"\"\n",
    "    return (C[:,1]-A[:,1])*(B[:,0] - A[:,0]) > (B[:,1] - A[:,1])*(C[:,0] - A[:,0])\n",
    "\n",
    "def intersect_true(A, B, C, D):\n",
    "    \"\"\" Determine whether two line segments AB and CD intersect\n",
    "    A- array, shape (n_pts,2)\n",
    "    B- array, shape (n_pts,2)\n",
    "    C- array, shape (n_pts,2) (optional n_pts = 1)\n",
    "    D- array, shape (n_pts,2) (optional n_pts = 1)\n",
    "    \"\"\"\n",
    "    cond1 = np.logical_not(ccw(A, C, D) == ccw(B, C, D))\n",
    "    cond2 = np.logical_not(ccw(A, B, C) == ccw(A, B, D))\n",
    "    return np.logical_and(cond1, cond2)\n",
    "\n",
    "def line_intersect(A,B,C,D):\n",
    "    \"\"\" Finds the intersection of the lines AB and CD, if it exists\n",
    "    Using https://en.wikipedia.org/wiki/Line%E2%80%93line_intersection#Given_two_points_on_each_line_segment\n",
    "    A- array, shape (n_pts,2)\n",
    "    B- array, shape (n_pts,2)\n",
    "    C- array, shape (n_pts,2)\n",
    "    D- array, shape (n_pts,2)\n",
    "    Typically N_PTS = 1\n",
    "    \"\"\"\n",
    "    denominator = (A[:, 0] - B[:, 0])*(C[:, 1] - D[:, 1]) - (A[:, 1] - B[:, 1])*(C[:, 0] - D[:, 0])\n",
    "    px = ((A[:, 0]*B[:, 1] - A[:, 1]*B[:, 0])*(C[:, 0] - D[:, 0]) \n",
    "          - (A[:, 0] - B[:, 0])*(C[:, 0]*D[:, 1] - C[:, 1]*D[:, 0]))/denominator\n",
    "    py = ((A[:, 0]*B[:, 1] - A[:, 1]*B[:, 0])*(C[:, 1] - D[:, 1]) \n",
    "          - (A[:, 1] - B[:, 1])*(C[:, 0]*D[:, 1] - C[:, 1]*D[:, 0]))/denominator\n",
    "    p = np.stack([px,py], axis=1)\n",
    "    return p\n",
    "\n",
    "def in_idxs(pts, var):\n",
    "    \"\"\" Checks if the indices in pts are with in the Variable var's mesh, for interpolation\n",
    "    E.g. if x mesh has seven values, don't have pts at -0.5 or 6.2\n",
    "    pts- array, shape (n_pts,2)\n",
    "    var- variable whose mesh to use\n",
    "    \n",
    "    Returns:\n",
    "    mask- bool array, shape (n_pts)\n",
    "    \"\"\"\n",
    "    max0 = len(var.mesh[0])\n",
    "    max1 = len(var.mesh[1])\n",
    "    mask = (0 < pts[:,0] < max0) and (0 < pts[:,1] < max1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "republican-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gap_fill(array):\n",
    "    \"\"\" Finds points in the 2d array that could be categorized as 'gaps' \n",
    "    And returns a mask for those points\n",
    "    I'm sure there's a better way to do this but oh well\"\"\"\n",
    "    \n",
    "    # check if there is a pixel to the center left and a pixel in one of the the right spaces\n",
    "    opp_left = np.logical_and(np.roll(array,1,axis=1), \n",
    "                            (np.abs(np.roll(array,(-1,1), axis=(1,0)))\n",
    "                            + np.abs(np.roll(array,(-1,-1), axis=(1,0)))\n",
    "                            + np.abs(np.roll(array,-1, axis=1))))\n",
    "    # check if there is a pixel to the center right and a pixel in one of the the left spaces\n",
    "    opp_right = np.logical_and(np.roll(array,-1,axis=1), \n",
    "                            (np.abs(np.roll(array,(1,-1), axis=(1,0)))\n",
    "                            + np.abs(np.roll(array,(1,1), axis=(1,0)))\n",
    "                            + np.abs(np.roll(array,1, axis=1))))\n",
    "    # check if there is a pixel to the center top and a pixel in one of the the bottom spaces\n",
    "    opp_up = np.logical_and(np.roll(array,1,axis=0), \n",
    "                            (np.abs(np.roll(array,(-1,1), axis=(0,1)))\n",
    "                            + np.abs(np.roll(array,(-1,-1), axis=(0,1)))\n",
    "                            + np.abs(np.roll(array,-1, axis=0))))\n",
    "    # check if there is a pixel to the center bottom and a pixel in one of the the top spaces    \n",
    "    opp_down = np.logical_and(np.roll(array,-1,axis=0), \n",
    "                            (np.abs(np.roll(array,(1,1), axis=(0,1)))\n",
    "                            + np.abs(np.roll(array,(1,-1), axis=(0,1)))\n",
    "                            + np.abs(np.roll(array,1, axis=0))))\n",
    "    # check if either of the diagonals has both pixels\n",
    "    opp_diag = np.logical_or(np.logical_and(np.roll(array,(1,1), axis=(0,1)), np.roll(array,(-1,-1), axis=(0,1))),\n",
    "                            np.logical_and(np.roll(array,(1,-1), axis=(0,1)), np.roll(array,(-1,1), axis=(0,1))))\n",
    "    \n",
    "    gap_filled_array = np.logical_or.reduce((array, opp_left, opp_right, opp_up, opp_down, opp_diag))\n",
    "    return gap_filled_array.astype(np.int64) #returns ones and zeros for output readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statewide-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      " \n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# gap_fill testing\n",
    "a = np.zeros((20,20), dtype=np.int64)\n",
    "a[1,1] = 1\n",
    "a[2,3] = 1\n",
    "a[3,9] = 1\n",
    "a[5,10] = 1\n",
    "a[1,9] = 1\n",
    "a[15,10] = 1\n",
    "a[13,8] = 1\n",
    "a[13,12] = 1\n",
    "print(a)\n",
    "print(' ')\n",
    "print(gap_fill(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-coaching",
   "metadata": {},
   "source": [
    "## X and O point finding ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprised-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "zooms = [[-np.inf,np.inf], [-40, 40]]\n",
    "time_idx = 31 #the time index we are processing rn (testing, will be reformatted nicer someday)\n",
    "smoothing = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opening-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy raw data\n",
    "desired_time = data_03082021.timeseries[time_idx]\n",
    "dt = data_03082021.timeseries[1] - data_03082021.timeseries[0]\n",
    "test_dset = data_03082021.ndslice(timelims=[desired_time - dt/2, desired_time + dt/2], zooms=zooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-groove",
   "metadata": {},
   "source": [
    "### Smoothing Bx and Bz ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "union-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    " # try gaussian filtering in space\n",
    "data_03082021.variables['bx'].data = nd.gaussian_filter(data_03082021.variables['bx'].data,[0,smoothing, smoothing]) \n",
    "data_03082021.variables['bz'].data = nd.gaussian_filter(data_03082021.variables['bz'].data,[0,smoothing, smoothing])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-trinity",
   "metadata": {},
   "source": [
    "### 0 contours of Bx and Bz, zoomed in ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "entertaining-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate number of grid spacings per 1 de: 4\n"
     ]
    }
   ],
   "source": [
    "bx = data_03082021.variables['bx'].ndslice(zooms=zooms)\n",
    "bz = data_03082021.variables['bz'].ndslice(zooms=zooms)\n",
    "jy = data_03082021.variables['jy'].ndslice(zooms=zooms)\n",
    "smooth_bx_data = bx.data[time_idx] \n",
    "smooth_bz_data = bz.data[time_idx]\n",
    "\n",
    "dbx_dz, dbx_dx = np.gradient(smooth_bx_data, *bx.mesh)\n",
    "dbz_dz, dbz_dx = np.gradient(smooth_bz_data, *bz.mesh)\n",
    "fluxfn_hessian_det = dbz_dx*(-dbx_dz) - (-dbx_dx)*dbz_dz\n",
    "topology = np.sign(fluxfn_hessian_det)  # 1 for max or min, -1 for saddle\n",
    "\n",
    "dz_per_de = 1/(bz.mesh[0][1]-bz.mesh[0][0])\n",
    "dx_per_de = 1/(bz.mesh[1][1]-bz.mesh[1][0])\n",
    "d_per_de = int((dz_per_de + dx_per_de)/2)\n",
    "print(\"Approximate number of grid spacings per 1 de:\", d_per_de)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-bowling",
   "metadata": {},
   "source": [
    "### Calculating flux function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "working-sarah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added flux_fn Variable\n"
     ]
    }
   ],
   "source": [
    "data_03082021.calc_fluxfn(b1_name='bz', b2_name='bx')\n",
    "# TODO: make sure you don't have a sign error in the flux function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-tractor",
   "metadata": {},
   "source": [
    "### show those flux contours ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacterial-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750c6ad5fd4646ddb3fb85a3c78b6969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2b1ebf8e42d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flux_fn_zoomed = nd.gaussian_filter(data_03082021.variables['flux_fn'].ndslice(zooms=zooms).data[time_idx],[smoothing, smoothing]) \n",
    "X,Y = np.meshgrid(*bx.mesh, indexing='ij')\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "ctr =ax.contour(X,Y,flux_fn_zoomed, levels=400)\n",
    "plt.colorbar(mappable=ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liked-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906fa90952584146be33ac164217b3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contour finding using skimage.measure.find_contours\n",
    "zeros_bx = measure.find_contours(smooth_bx_data,0)  # this alg uses LINEAR interpolation, so we will too\n",
    "zeros_bz = measure.find_contours(smooth_bz_data,0)\n",
    "# Plot to visualize\n",
    "print(len(zeros_bx), len(zeros_bz))\n",
    "fig2, ax2 = plt.subplots(3)\n",
    "for contour in zeros_bx:\n",
    "    ax2[0].plot(contour[:,0],contour[:,1])\n",
    "    ax2[2].plot(contour[:,0],contour[:,1], color='r')\n",
    "for contour in zeros_bz:\n",
    "    ax2[1].plot(contour[:,0],contour[:,1])\n",
    "    ax2[2].plot(contour[:,0],contour[:,1], color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-stockholm",
   "metadata": {},
   "source": [
    "### Setting up dictionary to hold interpolators of key things ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acute-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_meshgrid = np.meshgrid(*bx.mesh, indexing='ij')  # needed for LinearNDInterpolator\n",
    "#idx_meshgrid = np.mgrid[0:bx.shape[0], 0:bx.shape[1]]\n",
    "#all_idxs = np.column_stack(tuple(idx_meshgrid[i].flatten() for i in range(len(idx_meshgrid))))\n",
    "#all_pts = np.column_stack(tuple(default_meshgrid[i].flatten() for i in range(len(default_meshgrid))))\n",
    "all_pts = np.stack(default_meshgrid, axis=2) # smush together for interpolating as the data\n",
    "interps = {}\n",
    "idx_mesh = (np.array(range(len(bx.mesh[0]))), np.array(range(len(bx.mesh[1]))))\n",
    "interps['all_pts'] = interp.RegularGridInterpolator(idx_mesh, all_pts)\n",
    "interps['fluxfn_hessian_det'] = interp.RegularGridInterpolator(idx_mesh, fluxfn_hessian_det)\n",
    "interps['flux_fn'] = interp.RegularGridInterpolator(idx_mesh, flux_fn_zoomed)\n",
    "interps['jy'] = interp.RegularGridInterpolator(idx_mesh, jy.data[time_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-chuck",
   "metadata": {},
   "source": [
    "### Finding intersections of 0 contours ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "intensive-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "[[  46.87904234  192.20138185]\n",
      " [ 114.97886056  178.42360713]\n",
      " [ 318.49994352  186.67419156]\n",
      " [ 210.30322402  167.5862849 ]\n",
      " [ 442.87898093  182.16726903]\n",
      " [ 639.73057951  153.27858263]\n",
      " [ 588.28429281  165.17121359]\n",
      " [ 833.6854744   153.18075265]\n",
      " [ 883.82992453  173.84579568]\n",
      " [1071.11759467  145.50547115]\n",
      " [1021.79858523  145.09320903]\n",
      " [1273.45605448  164.75710861]\n",
      " [1280.51253178  165.40093509]\n",
      " [1542.02943115  190.36864923]\n",
      " [1469.65852019  153.4039745 ]\n",
      " [1464.09212635  150.45679052]\n",
      " [1616.09703992  173.80010873]\n",
      " [1512.6354686   228.5921268 ]\n",
      " [1550.82189342  216.56050568]\n",
      " [1379.07706725  205.0737245 ]\n",
      " [1430.31336341  150.78887147]\n",
      " [4391.58249611  173.56437832]\n",
      " [5379.34847954  177.025706  ]\n",
      " [5487.93240191  176.58871828]\n",
      " [7200.93944316  175.89243169]\n",
      " [7191.34066627  175.71995566]\n",
      " [7189.80073756  175.65643862]\n",
      " [7206.04018472  175.90010716]\n",
      " [7212.80146102  175.86381673]\n",
      " [7239.92367879  171.97792374]\n",
      " [7278.36943798  171.89045593]\n",
      " [7294.40415335  170.48507449]\n",
      " [7412.31079268  204.37724601]\n",
      " [7438.98319144  187.63852332]\n",
      " [7513.60099443  164.10370621]\n",
      " [7551.93398553  174.02999643]\n",
      " [7730.79265274  191.32104545]\n",
      " [7680.90346511  175.46977207]\n",
      " [8055.25046227  196.7431513 ]\n",
      " [7984.99199787  186.96790442]\n",
      " [8273.85461172  215.79643651]\n",
      " [8260.20594843  211.42358049]\n",
      " [8586.85965862  167.91299759]\n",
      " [8518.20169824  142.66567482]\n",
      " [8668.10188393  167.34534265]\n",
      " [8804.45675899  185.04524977]]\n"
     ]
    }
   ],
   "source": [
    "# each contour is an m x 2 array for m some other number depending on the number of points in the contour\n",
    "# break up each contour into m-1 line segments and check if they intersect each other\n",
    "nulls_list = []\n",
    "for contour_x in zeros_bx:  # sigh there probably isn't a better way to do this\n",
    "    endpt_x_1 = contour_x[:-1]\n",
    "    endpt_x_2 = contour_x[1:]\n",
    "    for contour_z in zeros_bz:\n",
    "        endpt_z_1 = contour_z[:-1]\n",
    "        endpt_z_2 = contour_z[1:]\n",
    "        for i in range(endpt_x_1.shape[0]):  # check if the x contour line segments intersect any of the z contour ones\n",
    "            endpt_x_1i = endpt_x_1[i].reshape(-1,2)\n",
    "            endpt_x_2i = endpt_x_2[i].reshape(-1,2)\n",
    "            intersects = np.nonzero(intersect_true(endpt_z_1, endpt_z_2, endpt_x_1i, endpt_x_2i))[0]  # get indices of contour_x which intercept\n",
    "            if len(intersects) != 0:  # only add in points that exist\n",
    "                intersect_pt = line_intersect(endpt_z_1[intersects], endpt_z_2[intersects], endpt_x_1i, endpt_x_2i)\n",
    "                nulls_list.append(intersect_pt)  # DO NOT round intersections to nearest integer\n",
    "        \n",
    "nulls = np.concatenate(nulls_list, axis=0)\n",
    "m_nulls = tuple(np.sign(interps['fluxfn_hessian_det'](nulls[i]))[0] == -1 \n",
    "                for i in range(nulls.shape[0]))  # need to put this here to account for changing topo later\n",
    "p_nulls = tuple(np.sign(interps['fluxfn_hessian_det'](nulls[i]))[0] == 1\n",
    "                for i in range(nulls.shape[0]))\n",
    "\n",
    "ax2[2].scatter(nulls[m_nulls,0], nulls[m_nulls,1], color='black', marker='$m$')\n",
    "ax2[2].scatter(nulls[p_nulls,0], nulls[p_nulls,1], color='black', marker='$p$')\n",
    "print(len(nulls))\n",
    "print(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-sacrifice",
   "metadata": {},
   "source": [
    "### Reducing the null points to their clusters and reducing each of those to their median value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "friendly-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls after numerical combination: 46\n",
      "Number of nulls after physical combination: 46\n"
     ]
    }
   ],
   "source": [
    "blobs = []  # will hold numpy arrays, each array the coordinates of one 'blob' of nulls\n",
    "adj_idxs = []\n",
    "\"\"\" DON'T USE, OUTDATED!!! NOT GOOD WITH LINEAR INTERPOLATION !!!!\"\"\"\n",
    "\"\"\" Combine the numerically adjacent nulls\"\"\" # NOT NEEDED if sufficiently smooth \n",
    "for new_coord in nulls:  # iterates along the first coordinate, yay\n",
    "    adj_idxs = []  # holds indices of null coordinates directly adjacent to new_coord\n",
    "    for i, blob in enumerate(blobs): \n",
    "        for coord in blob:  # check if new_coord is adjacent to coord in blob\n",
    "            diff = coord - new_coord\n",
    "            if (np.linalg.norm(diff) < 0.5  # make note of adjacent blob (adj. points probably numerical issue)\n",
    "                and np.sign(interps['fluxfn_hessian_det'](coord)) == \n",
    "                np.sign(interps['fluxfn_hessian_det'](new_coord))):  # see if both O or X-points, to combine\n",
    "                adj_idxs.append(i)                                                    \n",
    "                break  # don't want to append i multiple times if it touches multiple coords in a blob  \n",
    "    if len(adj_idxs) > 1:  # adjacent to more than one blob- need to combine blobs!\n",
    "        big_blob = [new_coord.reshape(-1,2)]\n",
    "        for idx in adj_idxs:\n",
    "            big_blob.append(blobs[idx])  # making up the new big blob\n",
    "        np.delete(np.array(blobs, dtype=object), adj_idxs).tolist()\n",
    "        big_blob_arr = np.concatenate(big_blob)\n",
    "        blobs.append(big_blob_arr)\n",
    "    elif len(adj_idxs) == 0:  # not adjacent to any existing blob, makes new blob\n",
    "        blobs.append(new_coord.reshape(-1,2))\n",
    "    elif len(adj_idxs) == 1:  # adjacent to exactly one blob\n",
    "        blobs[adj_idxs[0]] = np.concatenate((blobs[adj_idxs[0]], new_coord.reshape(-1,2)), axis=0)\n",
    "    else:\n",
    "        print(f\"bad size of adj_coords {len(adj_idxs)}\")\n",
    "print(f\"Number of nulls after numerical combination: {len(blobs)}\")\n",
    "\n",
    "\"\"\" Combine physically adjacent blobs, with topology cancellation\"\"\"\n",
    "for i in range(len(blobs)): \n",
    "    for j in range(i+1, len(blobs)):  # no repeats thank you very much\n",
    "        adjacent = False\n",
    "        for coord_i in blobs[i]:\n",
    "            for coord_j in blobs[j]:  # ewwwwwwwww please make this less awful\n",
    "                diff = coord_j - coord_i\n",
    "                if (np.linalg.norm(diff) < 0):  # blobs are touching\n",
    "                    adjacent = True\n",
    "                    break\n",
    "            if adjacent:  # already know blobs i and j are touching\n",
    "                break\n",
    "        if adjacent:  # combine the blobs and update the topology\n",
    "            new_topology = (np.sign(interps['fluxfn_hessian_det'](blobs[i][0]))[0] \n",
    "                            + np.sign(interps['fluxfn_hessian_det'](blobs[j][0]))[0])\n",
    "            blobs[i] = np.concatenate([blobs[i], blobs[j]])  # pool all the touching blob points into the first blob\n",
    "            blobs[j] = []  # second blob is redundant now but don't want to mess up for loop\n",
    "trimmed_blobs = list(blob for blob in blobs if not len(blob) == 0)  # get rid of redundant empty blobs   \n",
    "print(\"Number of nulls after physical combination:\", len(trimmed_blobs))\n",
    "for i, blob in enumerate(trimmed_blobs): #take integer version of the average value and have that be the center for now\n",
    "    avg_place = np.average(blob, axis=0)\n",
    "    closest_idx = np.argmin(np.sum((blob - avg_place)**2, axis=1))\n",
    "    trimmed_blobs[i] = blob[closest_idx].reshape(-1,2)  # update list\n",
    "\n",
    "blobs_arr = np.concatenate(trimmed_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-citizenship",
   "metadata": {},
   "source": [
    "### Separating X and O points ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "convertible-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 2)\n",
      "(23, 2)\n",
      "(0, 2)\n",
      "(0, 2)\n",
      "0.0\n",
      "8879 354\n"
     ]
    }
   ],
   "source": [
    "o_idxs = [np.sign(interps['fluxfn_hessian_det'](blobs_arr[i])[0]) == 1 for i in range(blobs_arr.shape[0])]\n",
    "x_idxs = [np.sign(interps['fluxfn_hessian_det'](blobs_arr[i])[0]) == -1 for i in range(blobs_arr.shape[0])]\n",
    "inconcl_idxs = [np.sign(interps['fluxfn_hessian_det'](blobs_arr[i])[0]) == 0 for i in range(blobs_arr.shape[0])]\n",
    "o_coords = blobs_arr[o_idxs]\n",
    "x_coords = blobs_arr[x_idxs]\n",
    "inconcl_coords = blobs_arr[inconcl_idxs]\n",
    "other_coords = blobs_arr[np.logical_not(np.logical_or.reduce((o_idxs, x_idxs, inconcl_idxs)))]\n",
    "\n",
    "print(o_coords.shape)\n",
    "print(x_coords.shape)\n",
    "print(inconcl_coords.shape)\n",
    "print(other_coords.shape)\n",
    "print(np.max(np.abs(o_coords - nulls[p_nulls,:])))\n",
    "print(max(idx_mesh[0]), max(idx_mesh[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-pendant",
   "metadata": {},
   "source": [
    "### Plot X and O points on flux contour ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "improved-gates",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b1f90ecfe50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLEASE throw these into a function or something, this is nasty :(\n",
    "ax.scatter(*interps['all_pts'](nulls[m_nulls,:]).T, marker='$m$', color='black')\n",
    "ax.scatter(*interps['all_pts'](nulls[p_nulls,:]).T, marker='$p$', color='black')\n",
    "X_opts = interps['all_pts'](o_coords)[:,0]\n",
    "Y_opts = interps['all_pts'](o_coords)[:, 1]\n",
    "X_xpts = interps['all_pts'](x_coords)[:, 0]\n",
    "Y_xpts = interps['all_pts'](x_coords)[:, 1]\n",
    "X_inconclpts = interps['all_pts'](inconcl_coords)[:, 0]\n",
    "Y_inconclpts = interps['all_pts'](inconcl_coords)[:, 1]\n",
    "X_otherpts = interps['all_pts'](other_coords)[:, 0]\n",
    "Y_otherpts = interps['all_pts'](other_coords)[:, 1]\n",
    "ax.scatter(X_opts, Y_opts, marker='o', facecolor='none', edgecolors='r')\n",
    "ax.scatter(X_xpts, Y_xpts, color='r', marker='x')\n",
    "ax.scatter(X_inconclpts, Y_inconclpts, color='r', marker='_')\n",
    "ax.scatter(X_otherpts, Y_otherpts, color='r', marker='$?$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-relative",
   "metadata": {},
   "source": [
    "## Designating regions around X and O points, testing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-exhibition",
   "metadata": {},
   "source": [
    "### Look at plots with topology / flux function hessian ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "obvious-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_range = [np.quantile(fluxfn_hessian_det, .01, interpolation='midpoint'),\n",
    "#               np.quantile(fluxfn_hessian_det, .99, interpolation='midpoint')]\n",
    "# fluxfn_hessian_det_nulls = [fluxfn_hessian_det[tuple(blobs_arr[i])] for i in range(blobs_arr.shape[0])]\n",
    "\n",
    "# figtest, axtest = plt.subplots(2, figsize=(8, 6))\n",
    "# axtest[0].imshow(topology.T, aspect='auto', interpolation='none')\n",
    "# hess_img = axtest[1].imshow(fluxfn_hessian_det.T, aspect='auto', interpolation='none', vmin=value_range[0],\n",
    "#                 vmax = value_range[1])\n",
    "# figtest.colorbar(hess_img)\n",
    "# for axes in axtest:\n",
    "#     axes.scatter(o_coords[:,0], o_coords[:,1], color='r', marker='o')\n",
    "#     axes.scatter(x_coords[:,0], x_coords[:,1], color='r', marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-crest",
   "metadata": {},
   "source": [
    "### Try constant flux contours from x points and overplot w/ B, current density ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "prime-residence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f4c34c7f424f30b43ddc108fb882cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tol = 5 #number of de to be close to\n",
    "boundaries = []\n",
    "smooth_jy = nd.gaussian_filter(data_03082021.variables['jy'].data[time_idx],[smoothing, smoothing])\n",
    "\n",
    "for i in range(x_coords.shape[0]):  # I love nested for loops...\n",
    "    xline_contours = measure.find_contours(flux_fn_zoomed, \n",
    "                                           level=interps['flux_fn'](x_coords[i]))\n",
    "    local_contours = []\n",
    "    for contour in xline_contours:\n",
    "        if min(np.linalg.norm(contour - x_coords[i], axis=1)) <= d_per_de*tol:\n",
    "            local_contours.append(contour)\n",
    "    # find_contours returns list, let's make a list of lists\n",
    "    boundaries.append(local_contours)\n",
    "\n",
    "\n",
    "# fig_j, ax_j = plt.subplots(2)    \n",
    "# img = ax_j[0].imshow(jy.data[time_idx].T, aspect='auto', interpolation='none')\n",
    "# img2 = ax_j[1].imshow(jy.data[time_idx].T, aspect='auto', interpolation='none', vmin = -0.1, vmax=0.00)\n",
    "# fig_j.colorbar(img2)\n",
    "\n",
    "bound_mask = np.zeros_like(flux_fn_zoomed)\n",
    "\n",
    "for i, level in enumerate(boundaries):\n",
    "    for contour in level:\n",
    "        bdy_points = interps['all_pts'](contour)\n",
    "        ax.plot(bdy_points[:,0], bdy_points[:,1], linestyle='dashed')\n",
    "#        ax_j[0].plot(contour[:,0], contour[:,1])\n",
    "#        ax_j[1].plot(contour[:,0], contour[:,1])\n",
    "        \n",
    "        #make a mask that should in theory show where the boundaries are (pixel by pixel)\n",
    "        for point in contour:\n",
    "            bound_mask[tuple(point.astype(np.int64))] = 1\n",
    "# fill in potential gaps in the contours\n",
    "bound_mask_filled = gap_fill(bound_mask)\n",
    "            \n",
    "fig_bdy, ax_bdy = plt.subplots()\n",
    "img_bdy = ax_bdy.imshow(bound_mask_filled.T, aspect='auto', interpolation='none', cmap=plt.get_cmap('binary'))\n",
    "\n",
    "all_structures = np.zeros_like(bound_mask)\n",
    "for i,coord in enumerate(o_coords):\n",
    "    color = rand(3)\n",
    "    if bound_mask_filled[tuple(coord.astype(np.int64))] == 0:\n",
    "        structure = flood(bound_mask_filled, tuple(coord.astype(np.int64)), connectivity=1)\n",
    "        all_structures = np.logical_or(all_structures, structure).astype(np.int64)\n",
    "        \n",
    "        struct_pic = np.stack([structure.T*color[0], structure.T*color[1], structure.T*color[2], structure.T], axis=2)\n",
    "        ax_bdy.imshow(struct_pic, aspect='auto', interpolation='none')\n",
    "    ax_bdy.plot(coord[0],coord[1], 'o', color=color*0.9)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-compatibility",
   "metadata": {},
   "source": [
    "### Try Current sheet IDing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "external-programmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e614a2928b584e57a8d277f244790c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_maxes = nd.maximum_filter(smooth_jy, size=(10, 10))\n",
    "cs_mins = nd.minimum_filter(smooth_jy, size=(10, 10))\n",
    "max_thresh = cs_maxes.mean() + cs_maxes.std() * 3\n",
    "min_thresh = cs_mins.mean() - cs_mins.std() * 3\n",
    "\n",
    "# find areas greater/less than threshold value\n",
    "max_labels, max_num = nd.label(cs_maxes > max_thresh)\n",
    "min_labels, min_num = nd.label(cs_mins < min_thresh)\n",
    "# Get the positions of the extrema\n",
    "max_coords = nd.maximum_position(smooth_jy, labels=max_labels, index=np.arange(1, max_num + 1))\n",
    "min_coords = nd.minimum_position(smooth_jy, labels=min_labels, index=np.arange(1, min_num + 1))\n",
    "# Get the extrema values\n",
    "max_values = nd.maximum(smooth_jy, labels=max_labels, index=np.arange(1, max_num + 1))\n",
    "min_values = nd.minimum(smooth_jy, labels=min_labels, index=np.arange(1, min_num + 1))\n",
    "\n",
    "cs_loc_pos = np.zeros_like(smooth_jy)\n",
    "cs_loc_neg = np.zeros_like(smooth_jy)\n",
    "\n",
    "cs_limit = 0.5\n",
    "for i in range(max_num):\n",
    "    # mask cs values which are \"high enough\"\n",
    "    cs_pos = smooth_jy > max_values[i]*cs_limit\n",
    "    # keep only cs which are local to the sheet\n",
    "    cs_pos = flood(cs_pos, max_coords[i], connectivity=1)\n",
    "    # update mask of all positive current sheets\n",
    "    cs_loc_pos = np.logical_or(cs_loc_pos, cs_pos)\n",
    "    \n",
    "for i in range(min_num):\n",
    "    # mask cs values which are \"low enough\"\n",
    "    cs_neg = smooth_jy < min_values[i]*.5\n",
    "    # keep only cs which are local to the sheet\n",
    "    cs_neg = flood(cs_neg, min_coords[i], connectivity=1)\n",
    "    # update mask of all positive current sheets\n",
    "    cs_loc_neg = np.logical_or(cs_loc_neg, cs_neg)\n",
    "\n",
    "# plot the full current sheet map\n",
    "fig_cs, ax_cs = plt.subplots(2)\n",
    "img_cs = ax_cs[0].imshow(smooth_jy.T, aspect='auto', interpolation='none')\n",
    "img2_cs = ax_cs[1].imshow(smooth_jy.T, aspect='auto', interpolation='none')\n",
    "fig_cs.colorbar(img_cs)\n",
    "# format the masks to show in colors\n",
    "neg_color = [1,1,1,0.5]\n",
    "pos_color = [0,0,0,0.5]\n",
    "cs_loc_neg_img = np.stack([cs_loc_neg.T*neg_color[i] for i in range(len(neg_color))], axis=2)\n",
    "cs_loc_pos_img = np.stack([cs_loc_pos.T*pos_color[i] for i in range(len(pos_color))], axis=2)\n",
    "ax_cs[0].imshow(cs_loc_neg_img, aspect='auto', interpolation='none')\n",
    "ax_cs[0].imshow(cs_loc_pos_img, aspect='auto', interpolation='none')\n",
    "\n",
    "for coord in max_coords:\n",
    "    ax_cs[0].plot(*coord, 'o', color='black')\n",
    "    \n",
    "for coord in min_coords:\n",
    "    ax_cs[0].plot(*coord, 'o', color='white')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-worker",
   "metadata": {},
   "source": [
    "### Try flux fn. Hessian structure def ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rental-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_types = [o_coords, x_coords]\n",
    "# colors = ['black','blue']\n",
    "# paths = [[1,0], [1,1], [0,1], [1,-1]]\n",
    "# paths = [np.array(paths[i]) for i in range(len(paths))]  # make np arrays\n",
    "# meshgrids = np.mgrid[0:topology.shape[0], 0:topology.shape[1]]\n",
    "\n",
    "# for k, null_type in enumerate(null_types):\n",
    "#     structure_masks = []\n",
    "#     point_list = []\n",
    "#     for null in null_type:\n",
    "#         print('begin', null)\n",
    "#         pts = np.empty((2*len(paths), 2))\n",
    "#         no_steps = True\n",
    "#         for i, path in enumerate(paths): \n",
    "#             idx = null.copy()\n",
    "#             max_idx0 = topology.shape[0]\n",
    "#             max_idx1 = topology.shape[1]\n",
    "#             l=0\n",
    "#             while ((topology[tuple(idx)] == topology[tuple(null)])\n",
    "#                    and max(-path[0], 0) <= idx[0] < min(max_idx0 - path[0], max_idx0)\n",
    "#                    and max(-path[1], 0) <= idx[1] < min(max_idx1 - path[1], max_idx1)):  # wont run off the grid\n",
    "#                 no_steps = False\n",
    "#                 idx += path\n",
    "#                 if l < 10000:\n",
    "#                    l += 1\n",
    "#                 else:\n",
    "#                    raise RuntimeError(\"Loop goes too long\")\n",
    "#             pts[i] = idx.reshape(-1,2).copy()\n",
    "#             idx = null.copy()\n",
    "#             l=0\n",
    "#             while ((topology[tuple(idx)] == topology[tuple(null)])\n",
    "#                    and max(path[0],0) <= idx[0] < min(max_idx0 + path[0], max_idx0)\n",
    "#                    and max(path[1],0) <= idx[1] < min(max_idx1 + path[1], max_idx1)):\n",
    "#                 no_steps = False\n",
    "#                 idx -= path\n",
    "#                 if l < 10000:\n",
    "#                    l += 1\n",
    "#                 else:\n",
    "#                    raise RuntimeError(\"Loop goes too long\")\n",
    "#             pts[-1-i] = idx.reshape(-1,2).copy()\n",
    "#         _, degeneracies = np.unique(pts, return_counts = True)\n",
    "#         degenerate = max(degeneracies) >= len(paths)*2\n",
    "#         if not (no_steps or degenerate):  # we went somewhere yay\n",
    "#             hull = Delaunay(pts)\n",
    "#             point_list.append(pts)\n",
    "#             all_pts = np.stack(meshgrids, axis=2)\n",
    "#             structure_mask = np.logical_and(hull.find_simplex(all_pts) > 0, topology == topology[tuple(null)])\n",
    "#             structure_masks.append(structure_mask) \n",
    "#         print('end ', null)  # need to visualize that this thing is actually doing something\n",
    "    \n",
    "#     structure_masks_arr = np.stack(structure_masks, axis=2)\n",
    "#     all_structure_map = np.sign(np.sum(structure_masks_arr, axis=2))\n",
    "#     print(all_structure_map.shape)\n",
    "#     for pt in point_list:\n",
    "#         pt_approx = pt.astype(int)\n",
    "#         hull = ConvexHull(pt_approx)\n",
    "#         vertices = pt_approx[hull.vertices]\n",
    "#         x_border = X[tuple(vertices.T)]\n",
    "#         y_border = Y[tuple(vertices.T)]\n",
    "#         ax.fill(x_border, y_border, color = colors[k], alpha=0.3)\n",
    "#     print(\"plotted\")\n",
    "# plt.show() \n",
    "# fig.savefig(\"outs/2d_test.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-litigation",
   "metadata": {},
   "source": [
    "### Testing are for structure finding methods etc. implemented into the VPICDataset object ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "matched-graham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding structures at simulation time 309.89697265625\n",
      "parameter d_per_de = 4\n",
      "Added bz_smooth Variable\n",
      "Added bx_smooth Variable\n",
      "Added flux_fn Variable\n",
      "Number of nulls:  46\n",
      "parameter x_coords = [[ 114.97886056  178.42360713]\n",
      " [ 318.49994352  186.67419156]\n",
      " [ 588.28429281  165.17121359]\n",
      " [ 833.6854744   153.18075265]\n",
      " [1021.79858523  145.09320903]\n",
      " [1273.45605448  164.75710861]\n",
      " [1542.02943115  190.36864923]\n",
      " [1464.09212635  150.45679052]\n",
      " [1550.82189342  216.56050568]\n",
      " [1379.07706725  205.0737245 ]\n",
      " [4391.58249611  173.56437832]\n",
      " [5487.93240191  176.58871828]\n",
      " [7191.34066627  175.71995566]\n",
      " [7206.04018472  175.90010716]\n",
      " [7239.92367879  171.97792374]\n",
      " [7294.40415335  170.48507449]\n",
      " [7438.98319144  187.63852332]\n",
      " [7551.93398553  174.02999643]\n",
      " [7730.79265274  191.32104545]\n",
      " [8055.25046227  196.7431513 ]\n",
      " [8273.85461172  215.79643651]\n",
      " [8586.85965862  167.91299759]\n",
      " [8804.45675899  185.04524977]]\n",
      "parameter o_coords = [[  46.87904234  192.20138185]\n",
      " [ 210.30322402  167.5862849 ]\n",
      " [ 442.87898093  182.16726903]\n",
      " [ 639.73057951  153.27858263]\n",
      " [ 883.82992453  173.84579568]\n",
      " [1071.11759467  145.50547115]\n",
      " [1280.51253178  165.40093509]\n",
      " [1469.65852019  153.4039745 ]\n",
      " [1616.09703992  173.80010873]\n",
      " [1512.6354686   228.5921268 ]\n",
      " [1430.31336341  150.78887147]\n",
      " [5379.34847954  177.025706  ]\n",
      " [7200.93944316  175.89243169]\n",
      " [7189.80073756  175.65643862]\n",
      " [7212.80146102  175.86381673]\n",
      " [7278.36943798  171.89045593]\n",
      " [7412.31079268  204.37724601]\n",
      " [7513.60099443  164.10370621]\n",
      " [7680.90346511  175.46977207]\n",
      " [7984.99199787  186.96790442]\n",
      " [8260.20594843  211.42358049]\n",
      " [8518.20169824  142.66567482]\n",
      " [8668.10188393  167.34534265]]\n",
      "Added fluxfn_hessian_det Variable\n",
      "Added separatrices Variable\n",
      "Added o_structures Variable\n",
      "Added current_sheets Variable\n"
     ]
    }
   ],
   "source": [
    "test_dset.find_structures(b1_name='bz', b2_name='bx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "uniform-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2add8d4e7c10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xo = interps['all_pts'](test_dset.params['o_coords'])[:,0]\n",
    "yo = interps['all_pts'](test_dset.params['o_coords'])[:, 1]\n",
    "xx = interps['all_pts'](test_dset.params['x_coords'])[:, 0]\n",
    "yx = interps['all_pts'](test_dset.params['x_coords'])[:, 1]\n",
    "ax.scatter(xo, yo, marker='o', facecolor='none', edgecolors='b')\n",
    "ax.scatter(xx, yx, color='b', marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fresh-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(1, 8880, 355)\n",
      "(8880, 1110)\n",
      "False\n",
      "(1, 8880, 355)\n",
      "(8880, 355)\n",
      "94984\n",
      "False\n",
      "(1, 8880, 355)\n",
      "(8880, 355)\n",
      "33387\n",
      "(8827, 0)\n",
      "0.004163027\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_param', '_add_var', '_get_params', '_init_datapkg', '_init_file', '_init_vpicfile', 'bounds', 'calc_fluxfn', 'default_mesh', 'export', 'find_structures', 'ndslice', 'params', 'timeseries', 'variables']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ffb93a54414741a54a142036b291ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.array_equal(cs_loc_pos.astype(np.int64) - cs_loc_neg.astype(np.int64),\n",
    "                     test_dset.variables['current_sheets'].data[0]))\n",
    "print(test_dset.variables['current_sheets'].data.shape)\n",
    "print(cs_loc_pos.shape)\n",
    "print(np.array_equal(all_structures,\n",
    "                     test_dset.variables['o_structures'].data[0]))\n",
    "print(test_dset.variables['o_structures'].data.shape)\n",
    "print(all_structures.shape)\n",
    "print(np.sum(np.abs(test_dset.variables['o_structures'].data[0] - all_structures)))\n",
    "print(np.array_equal(bound_mask_filled,\n",
    "                     test_dset.variables['separatrices'].data[0]))\n",
    "print(test_dset.variables['separatrices'].data.shape)\n",
    "print(bound_mask_filled.shape)\n",
    "print(np.sum(np.abs(test_dset.variables['separatrices'].data[0] - bound_mask_filled)))\n",
    "\n",
    "print(np.unravel_index(np.argmax(bx.data[time_idx] - test_dset.variables['bx_smooth'].data[0]), \n",
    "                       bx.data[time_idx].shape))\n",
    "print(np.max(bx.data[time_idx] - test_dset.variables['bx_smooth'].data[0]))\n",
    "print(dir(test_dset))\n",
    "figcheck, axcheck = plt.subplots(4)\n",
    "axcheck[0].imshow((test_dset.variables['bx'].data[0]).T, aspect='auto')\n",
    "axcheck[1].imshow(bx.data[time_idx].T, aspect='auto')\n",
    "axcheck[2].imshow(test_dset.variables['separatrices'].data[0].T, aspect='auto')\n",
    "axcheck[3].imshow(bound_mask_filled.T, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pending-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.232972\n"
     ]
    }
   ],
   "source": [
    "print(data_03082021.variables['flux_fn'].data[time_idx,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-student",
   "metadata": {},
   "source": [
    "## Try slapdash ML thing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-agriculture",
   "metadata": {},
   "source": [
    "### Handpicking slices through structures (bad practice, just for this test) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eligible-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before making each slice randomly shuffle the endpoints\n",
    "used_time = 31  # in case I change it up above, make sure I remember what I used here\n",
    "slices_dict = {'cs':[[[-40,-5],[20,4]],\n",
    "                     [[-10,10],[-12,-7]],\n",
    "                     [[225,1,],[245,1]],\n",
    "                     [[230,9],[235,-8]],\n",
    "                     [[227,-6],[234,-5]],\n",
    "                     [[-985,-2],[-965,5]],\n",
    "                     [[-982,3],[-965,-1]],\n",
    "                     [[-975,-9],[-971,9]],\n",
    "                     [[-935,-5],[-933,9]],\n",
    "                     [[-940,5],[-920,4]],\n",
    "                     [[-925,6],[-930,-5]],\n",
    "                     [[-860,-5],[-870,-4]],\n",
    "                     [[-867,-7],[-865,4]],\n",
    "                     [[-816,0],[-808,1]],\n",
    "                     [[-813,8],[-814,-5]],\n",
    "                     [[-778,-5],[-774,7]],\n",
    "                     [[-764,-6],[-773,-5]],\n",
    "                     [[-697,-3],[-682,2]],\n",
    "                     [[-690,8],[-689,0]],\n",
    "                     [[-687,-2],[-685,7]],\n",
    "                     [[-675,-6],[-670,-6]],\n",
    "                     [[630,-5],[632,3]],\n",
    "                     [[627,0],[633,-1]],\n",
    "                     [[640,7],[644,-8]],\n",
    "                     [[642,5],[647,6]],\n",
    "                     [[670,3],[681,-3]],\n",
    "                     [[674,-4],[675,8]],\n",
    "                     [[706,-3],[702,1]],\n",
    "                     [[735,-5],[750,0]],\n",
    "                     [[735,0],[747,5]],\n",
    "                     [[738,8],[744,-7]],\n",
    "                     [[863,5],[869,9]],\n",
    "                     [[927,-7],[940,4]],\n",
    "                     [[940,-5],[932,7]],\n",
    "                     [[975,3],[990,8]],\n",
    "                     [[985,7],[989,-8]]],\n",
    "              'fr':[[[208,-6],[217,-2]],\n",
    "                    [[196,-7],[221,7]],\n",
    "                    [[211,-8],[213,-7]],\n",
    "                    [[207,9],[210,-6]],\n",
    "                    [[-994,7],[-978,-3]],\n",
    "                    [[-993,-6],[-986,9]],\n",
    "                    [[-965,-5],[-940,4]],\n",
    "                    [[-964,0],[-945,8]],\n",
    "                    [[-955,7],[-950,-8]],\n",
    "                    [[-900,-5],[-896,7]],\n",
    "                    [[-902,7],[-897,-6]],\n",
    "                    [[-862,-7],[-848,-6]],\n",
    "                    [[-858,5],[-852,-8]],\n",
    "                    [[-808,0],[-786,1]],\n",
    "                    [[-790,8],[-794,-7]],\n",
    "                    [[-685,-1],[-675,-9]],\n",
    "                    [[-670,-9],[-662,1]],\n",
    "                    [[-645,0],[-625,-1]],\n",
    "                    [[610,-6],[623,6]],\n",
    "                    [[609,0],[623,-3]],\n",
    "                    [[640,-7],[636,6]],\n",
    "                    [[635,-1],[641,1]],\n",
    "                    [[688,2],[700,-7]],\n",
    "                    [[690,-5],[695,3]],\n",
    "                    [[715,1],[755,-7]],\n",
    "                    [[725,-8],[721,6]],\n",
    "                    [[787,0],[800,3]],\n",
    "                    [[865,2],[855,9]],\n",
    "                    [[850,3],[864,4]],\n",
    "                    [[948,-3],[965,-4]],\n",
    "                    [[950,-9],[954,7]]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-blink",
   "metadata": {},
   "source": [
    "### Taking slices, trying various features, conglomerating them into datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eligible-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-40, -5], [20, 4]]\n",
      "[[-10, 10], [-12, -7]]\n",
      "[[225, 1], [245, 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This interpolation method cannot handledegenerate dimensions, please reprocesssto avoid dimensions with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bd69ddb77c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mset_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndslice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndslice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mzoom_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwindow_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzooms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzoom_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_pts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0msample_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mesh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mesh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmesh_normed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mesh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mesh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tigress/kendrab/python_pkgs/dataframework/src/datasets/dataset.py\u001b[0m in \u001b[0;36mndslice\u001b[0;34m(self, timelims, zooms, set_pts, interp, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                                                         \u001b[0mset_pts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_pts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                                                         \u001b[0minterp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                                                         **kwargs)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mnew_timeseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefaults_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tigress/kendrab/python_pkgs/dataframework/src/variables/statmeshvar.py\u001b[0m in \u001b[0;36mndslice\u001b[0;34m(self, timelims, zooms, set_pts, interp, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         sliced_var = super().ndslice(timelims=timelims, zooms=zooms,\n\u001b[0;32m---> 91\u001b[0;31m                                      set_pts=set_pts, interp=interp, **kwargs)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msliced_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tigress/kendrab/python_pkgs/dataframework/src/variables/variable.py\u001b[0m in \u001b[0;36mndslice\u001b[0;34m(self, timelims, zooms, set_pts, interp, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mslicedvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zoom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzooms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset_pts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mslicedvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spaceslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mslicedvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tigress/kendrab/python_pkgs/dataframework/src/variables/statmeshvar.py\u001b[0m in \u001b[0;36m_spaceslice\u001b[0;34m(self, set_pts, interp, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     tuple(self.mesh), data_rearr, method=var_interp)\n\u001b[1;32m    137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 raise ValueError('This interpolation method cannot handle'\n\u001b[0m\u001b[1;32m    139\u001b[0m                                  \u001b[0;34m'degenerate dimensions, please reprocesss'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                  'to avoid dimensions with size 1')\n",
      "\u001b[0;31mValueError\u001b[0m: This interpolation method cannot handledegenerate dimensions, please reprocesssto avoid dimensions with size 1"
     ]
    }
   ],
   "source": [
    "data_2d = data_03082021.ndslice(zooms=zooms)\n",
    "dx = data_2d.default_mesh[0][1] - data_2d.default_mesh[0][0]\n",
    "eps = dx/2  # to make sure we have some data that exists\n",
    "full_dset=[]\n",
    "values=[]\n",
    "for key in slices_dict.keys():\n",
    "    for ndslice in slices_dict[key]:\n",
    "        print(ndslice)\n",
    "        values.append(key)\n",
    "        window_0 = np.array([min(ndslice[0][0], ndslice[1][0])-eps, max(ndslice[0][0], ndslice[1][0])+eps])\n",
    "        window_1 = np.array([min(ndslice[0][1], ndslice[1][1])-eps, max(ndslice[0][1], ndslice[1][1])+eps])\n",
    "        set_pts = [np.array(ndslice[0]), np.array(ndslice[1])]\n",
    "        zoom_sample = [window_0, window_1]\n",
    "        sample = data_2d.ndslice(zooms=zoom_sample, set_pts=set_pts)\n",
    "        sample_len = max(sample.default_mesh[0]) - min(sample.default_mesh[0])\n",
    "        mesh_normed = (sample.default_mesh[0] - sample.default_mesh[0][0])/(sample_len/2) - 1\n",
    "        \"\"\"calculate some features to use in the classifier\"\"\"\n",
    "        bx = sample.variables['bx'].data[used_time]\n",
    "        by = sample.variables['by'].data[used_time]\n",
    "        bz = sample.variables['bz'].data[used_time]\n",
    "        jy_avg = np.average(sample.variables['jy'].data[used_time])\n",
    "        b_mag_avg = np.average(np.sqrt(bx*bx+by*by+bz*bz))\n",
    "        \n",
    "        bx_norm_fit = Polynomial.fit(mesh_normed, bx/b_mag_avg, deg=5)\n",
    "        by_norm_fit = Polynomial.fit(mesh_normed, by/b_mag_avg, deg=5)\n",
    "        bz_norm_fit = Polynomial.fit(mesh_normed, bz/b_mag_avg, deg=5)\n",
    "        \n",
    "        features = np.array([jy_avg] + [b_mag_avg] + list(bx_norm_fit) + list(by_norm_fit) + list(bz_norm_fit))\n",
    "        full_dset.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dset_arr = np.vstack(full_dset)\n",
    "print(full_dset_arr.shape)\n",
    "print(len(values))\n",
    "np.savetxt(\"testdata.txt\", np.array([list(full_dset_arr[i])+[values[i]] for i in range(len(values))]), fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-gothic",
   "metadata": {},
   "source": [
    "### Splitting the data up and trying out the RFC ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_dset_arr, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10000, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-respect",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
