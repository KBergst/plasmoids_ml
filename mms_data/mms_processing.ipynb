{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f5c62b-80b2-42bd-a56d-66954ebe8add",
   "metadata": {},
   "source": [
    "### To get the data for domain adaptation and put it into a desired form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd4df82-724d-4d54-832e-dee4ab7fdacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspedas\n",
    "import pytplot\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import h5py\n",
    "import datetime as dt\n",
    "from IPython.display import clear_output\n",
    "import sys  # for debugging\n",
    "import os  # TODO use subprocess instead for ~safety~\n",
    "from scipy.constants import physical_constants\n",
    "os.environ[\"MMS_DATA_DIR\"] = \"/tigress/kendrab/analysis-notebooks/mms_data/pydata/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ed18a6-f8bb-4852-9b25-69e6b2355426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (will change depending on which model we are using)\n",
    "window = 89\n",
    "stride = 11 \n",
    "n_avg_width = 10  # for smoothing the plasma freq\n",
    "\n",
    "# Physical constants (for unit conversion and plasma freq calc.)\n",
    "c = physical_constants[\"speed of light in vacuum\"][0]  # m/s\n",
    "e = physical_constants[\"elementary charge\"][0]  # coulombs\n",
    "m_e = physical_constants[\"electron mass\"][0]  # kg\n",
    "m_i = physical_constants[\"proton mass\"][0]  # kg\n",
    "e_0 = physical_constants[\"vacuum electric permittivity\"][0]  # Farads/m\n",
    "k = physical_constants[\"Boltzmann constant\"][0]  # J/K\n",
    "\n",
    "# PIC characteristic values (MAKE SURE THESE ARE CORRECT FOR THE SIMULATION YOU ARE PROCESSING)\n",
    "mime_pic = 25\n",
    "v_the_sim_cs = 0.25\n",
    "v_the_sim_out = 0.15\n",
    "n_sim = 1\n",
    "n_sim_out = 0.01/2.  # beta/2\n",
    "\n",
    "# MMS characteristic values\n",
    "# assumed tail current sheet density, cm^-3\n",
    "n_cs = 0.625\n",
    "# assumed tail electron temperature, keV\n",
    "e_the_tail = 3.795\n",
    "# assumed lobe density, cm^-3 (Frank 1985)\n",
    "n_lobe = 0.1\n",
    "# assumed lobe temperature, K then J\n",
    "e_the_lobe_K = 1e6\n",
    "e_the_lobe_j = e_the_lobe_K*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b046de5-7732-47c1-b7a6-5fccdafc454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.08026032603e-16 1.3806490000000001e-17\n",
      "0.12187394915783151 0.018365021386649957\n",
      "2.0512997381929443 8.16770080698295\n",
      "44599.65914947709\n",
      "1.90e+17\n"
     ]
    }
   ],
   "source": [
    "# Unit conversion and normalization supportive calculations\n",
    "e_the_j = e_the_tail*1e3*e  # joules\n",
    "print(e_the_j, e_the_lobe_j)\n",
    "v_the_tail = np.sqrt(2*e_the_j/m_e)/c  # velocity in PIC units\n",
    "v_the_lobe = np.sqrt(2*e_the_lobe_j/m_e)/c\n",
    "print(v_the_tail, v_the_lobe)\n",
    "print(v_the_sim_cs/v_the_tail, v_the_sim_out/v_the_lobe)\n",
    "n_cs_m3 = n_cs*(100)**3  # cm^-3 to m^-3\n",
    "plasma_freq = np.sqrt(n_cs_m3*e**2/e_0/m_e) #s^-1\n",
    "print(plasma_freq)\n",
    "\n",
    "# convert mms density to pic units\n",
    "d_e = c/plasma_freq  # in m\n",
    "n_cs_sim = n_cs_m3*d_e**3  # in d_e^(-3)\n",
    "print('{:.2e}'.format(n_cs_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432bca53-cf6e-405b-ab1f-8b7fa9400f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07899960767600023 -0.0019472551195471386 40.56972652580453\n"
     ]
    }
   ],
   "source": [
    "# electric field correction...\n",
    "# the units need to agree outside the log but inside the logs they just need to be locally consistent, because we are doing a ratio and the units are just a scale factor different\n",
    "E_val_sim = np.sqrt(1/mime_pic)*v_the_sim_cs**2*np.log(n_sim_out*v_the_sim_out**2/(n_sim*v_the_sim_cs**2))\n",
    "E_val_tail = np.sqrt(m_e/m_i)*(v_the_tail)**2*np.log(n_lobe*e_the_lobe_j/(n_cs*e_the_j))\n",
    "E_corr_factor = E_val_sim/E_val_tail\n",
    "print(E_val_sim, E_val_tail, E_corr_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f84204-85f6-4d1a-9784-0b52910c4dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8997442222394003e-16\n"
     ]
    }
   ],
   "source": [
    "# current density correction\n",
    "j_val_sim = mime_pic**(-0.5)*n_sim*v_the_sim_cs**2\n",
    "j_val_tail = np.sqrt(m_e/m_i)*n_cs_sim*v_the_tail**2\n",
    "j_corr_factor = j_val_sim/j_val_tail\n",
    "print(j_corr_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6165d-8609-44a9-9602-77f099d14fb2",
   "metadata": {},
   "source": [
    "### Read in the tail region times created by mms_region_to_time.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3f12f9-b239-469a-a7d3-c049ce585ec7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File('/tigress/kendrab/analysis-notebooks/mms_data/interval_times.h5', 'r') as file:\n",
    "    # this is split into two separate lines for readability\n",
    "    times = file['times'][()]  # get the list of times\n",
    "    times = np.vectorize(lambda x: x.decode())(times)  # decode the times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b83b28-294f-46d4-8286-8fff8ce78a6f",
   "metadata": {},
   "source": [
    "### Do the processing one timesegment at a time for memory reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e74264e-cdec-4eff-8eb4-4943064f4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_end(i):\n",
    "    \"\"\" What we want to happen at the end of every loopy boye whether continue or otherwise\"\"\"\n",
    "    # delete the original sc data to free up memory\n",
    "    pytplot.del_data()\n",
    "    # clear the output every n entries to free up memory as well\n",
    "    if (i % 15 == 14):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Cleared output at step {i}\")\n",
    "    return i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d196fb4-779d-4646-b084-ff0aae930f84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared output at step 509\n",
      "Searching for local files...\n",
      "No matching CDF versions found.\n",
      "Missing FPI data for interval ['2023-10-16/18:16:00' '2023-10-16/19:08:30']\n",
      "Searching for local files...\n",
      "No matching CDF versions found.\n",
      "Missing FPI data for interval ['2023-10-20/06:42:00' '2023-10-20/06:51:30']\n",
      "/tigress/kendrab/analysis-notebooks/mms_data/mms_slices/2023-10-23T18-30-00_2023-10-23T18-36-00.h5 already exists. Skipping\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i < times.shape[0]:  # while loop instead of for loop bc time.shape can change in the loop\n",
    "    # cut up intervals that are too long and will crash the process TRY WITHOUT DOING THIS NOW\n",
    "    # num_files = pyspedas.mms.fpi(trange=times[i], probe='1', data_rate='brst', datatype='des-moms',\n",
    "    #                             time_clip=True, available=True, varnames=[\"mms1_des_numberdensity_brst\"])\n",
    "    # while len(num_files) > 10:  # should be a generous limit\n",
    "    #     # datetimes to halve the time interval\n",
    "    #     start_time = dt.datetime.strptime(times[i][0], '%Y-%m-%d/%H:%M:%S')\n",
    "    #     end_time = dt.datetime.strptime(times[i][1], '%Y-%m-%d/%H:%M:%S')\n",
    "    #     mid_time = start_time + (end_time - start_time)/2\n",
    "    #     # back to strings\n",
    "    #     start_time = start_time.strftime('%Y-%m-%d/%H:%M:%S')\n",
    "    #     end_time = end_time.strftime('%Y-%m-%d/%H:%M:%S')\n",
    "    #     mid_time = mid_time.strftime('%Y-%m-%d/%H:%M:%S')\n",
    "    #     times = np.insert(times, i, [start_time, mid_time], axis=0)  # half step becomes new ith interval, original now i+1\n",
    "    #     times[i+1] = [mid_time, end_time]  # updating original interval to be second half step  \n",
    "    #     print(f\"Split long interval {[start_time, end_time]} into {times[i]}, {times[i+1]}\")\n",
    "    #     num_files = pyspedas.mms.fpi(trange=times[i], probe='1', data_rate='brst', datatype='des-moms',\n",
    "    #                             time_clip=True, available=True, varnames=[\"mms1_des_numberdensity_brst\"])\n",
    "    # okay we have our dataset now    \n",
    "    start_time_filestr = times[i,0].replace('/','T').replace(':','-')\n",
    "    end_time_filestr = times[i,1].replace('/','T').replace(':','-')\n",
    "    outfile = f'/tigress/kendrab/analysis-notebooks/mms_data/mms_slices/{start_time_filestr}_{end_time_filestr}.h5'\n",
    "    # SKIP if file exists already\n",
    "    if os.path.exists(outfile):\n",
    "        i = loop_end(i)\n",
    "        print(f\"{outfile} already exists. Skipping\")\n",
    "        continue\n",
    "    #### FPI data\n",
    "    vars_tmp = pyspedas.mms.fpi(trange=times[i], probe='1', data_rate='brst', datatype='des-moms',\n",
    "                                time_clip=True, varnames=[\"mms1_des_numberdensity_brst\"])    \n",
    "    if vars_tmp is None: \n",
    "        print(f\"Missing FPI data for interval {times[i]}\")\n",
    "        i = loop_end(i)\n",
    "        continue\n",
    "    #### MEC data \n",
    "    mec_datarate = 'brst'  # pull this into its own variable in case we want to record whether we used brst or srvy\n",
    "    vars_tmp = np.empty(4, dtype=object)\n",
    "    for j in range(1,5):\n",
    "        vars_tmp[j-1] = pyspedas.mms.mec(trange=times[i], probe=str(j), data_rate=mec_datarate, time_clip=False,\n",
    "                                    varnames=[f\"mms{j}_mec_r_gse\",])\n",
    "    if np.any(vars_tmp == None):  # no burst data\n",
    "        print(f\"Missing some MEC burst data, interval {times[i]}. Trying survey data\")\n",
    "        mec_datarate = 'srvy'\n",
    "        vars_tmp = np.empty(4, dtype=object)   \n",
    "        for j in range(1,5):  # try srvy data\n",
    "            try:\n",
    "                vars_tmp[j-1] = pyspedas.mms.mec(trange=times[i], probe=str(j), data_rate=mec_datarate, time_clip=False,\n",
    "                                                 varnames=[f\"mms{j}_mec_r_gse\",])\n",
    "            except:\n",
    "                vars_tmp = np.empty(4, dtype=object)\n",
    "        if np.any(vars_tmp == None):  # no burst data \n",
    "            print(f\"Missing some MEC data, interval {times[i]}\")\n",
    "            i = loop_end(i)\n",
    "            continue\n",
    "    #### FGM data    \n",
    "    vars_tmp = np.empty(4, dtype=object)\n",
    "    for j in range(1,5):\n",
    "        vars_tmp[j-1] = pyspedas.mms.fgm(trange=times[i], probe=str(j), data_rate='brst', time_clip=True,\n",
    "                                    varnames=[f\"mms{j}_fgm_b_gsm_brst_l2\",])\n",
    "    if np.any(vars_tmp == None):\n",
    "        print(f\"Missing some FGM data, interval {times[i]}\")\n",
    "        i = loop_end(i)\n",
    "        continue\n",
    "    #### EDP data\n",
    "    vars_tmp = pyspedas.mms.edp(trange=times[i], probe='1', data_rate='brst', time_clip=True,\n",
    "                                varnames=[\"mms1_edp_dce_gse_brst_l2\"]) \n",
    "    if vars_tmp is None:\n",
    "        print(f\"Missing EDP data for interval {times[i]}\")\n",
    "        i = loop_end(i)\n",
    "        continue\n",
    "    \n",
    "    print(pytplot.data_quants.keys())\n",
    "    print(pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2\"].shape, pytplot.data_quants[\"mms1_edp_dce_gse_brst_l2\"].shape)\n",
    "    print(pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2\"])\n",
    "    print(pytplot.data_quants[\"mms1_edp_dce_gse_brst_l2\"])\n",
    "    \n",
    "    # remove duplicates from data \n",
    "    pytplot.data_quants[\"mms1_edp_dce_gse_brst_l2\"] = pytplot.data_quants[\"mms1_edp_dce_gse_brst_l2\"].drop_duplicates(dim='time', keep='first')\n",
    "    for j in range(1,5):\n",
    "        pytplot.data_quants[f\"mms{j}_fgm_b_gsm_brst_l2_bvec\"] = pytplot.data_quants[f\"mms{j}_fgm_b_gsm_brst_l2_bvec\"].drop_duplicates(dim='time', keep='first')\n",
    "        pytplot.data_quants[f\"mms{j}_mec_r_gse\"] = pytplot.data_quants[f\"mms{j}_mec_r_gse\"].drop_duplicates(dim='time', keep='first')\n",
    "\n",
    "    \n",
    "    # Find curlometer j- need to move to GSE and interpolate r for this\n",
    "    fields=[]\n",
    "    pos = []\n",
    "    for j in range(1,5):\n",
    "        pyspedas.cotrans(name_in = f\"mms{j}_fgm_b_gsm_brst_l2_bvec\", name_out = f\"mms{j}_fgm_b_gse_brst_l2_bvec\",\n",
    "                         coord_in='gsm', coord_out='gse')\n",
    "        fields.append(f\"mms{j}_fgm_b_gse_brst_l2_bvec\")\n",
    "        pos.append(f\"mms{j}_mec_r_gse\")\n",
    "    pyspedas.mms.curlometer(fields=fields, positions=pos) # jtotal in A/m^2\n",
    "    print(pytplot.data_quants['jtotal'])\n",
    "\n",
    "    # Interpolate E (and possibly B if we want to) to a lower data rate\n",
    "    pytplot.data_quants[\"mms1_edp_dce_gse_brst_l2\"] = \\\n",
    "        pytplot.data_quants[\"mms1_edp_dce_gse_brst_l2\"].interp(method=\"linear\", assume_sorted=False,\n",
    "                                                               time=pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2_bvec\"].time)\n",
    "    \n",
    "    # use pyspedas to transform E field and j data to GSM coordinates\n",
    "    pyspedas.cotrans(name_in=\"mms1_edp_dce_gse_brst_l2\", name_out=\"mms1_edp_dce_gsm_brst_l2\", coord_in='gse', coord_out='gsm')\n",
    "    pyspedas.cotrans(name_in=\"jtotal\", name_out=\"jtotal_gsm\", coord_in='gse', coord_out='gsm')\n",
    "    \n",
    "    # Convert E, B, J to typical PIC units e = 1, m_e = 1, c = 1, d_e = 1, w_pe = 1\n",
    "    pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2_bvec\"] *= 10**(-9)/m_e*e/plasma_freq  # T/nT*m_e/kg*C/e*(wpe^-1*s) -> units of m_e wpe / e #TODO FIX THESE\n",
    "    pytplot.data_quants[\"mms1_edp_dce_gsm_brst_l2\"] *= 10**(-3)/m_e*e/plasma_freq/c  # V/mV*m_e/kg*C/e*(wpe^-1*s)*(c / m/s) -> units of m_e wpe c / e \n",
    "    pytplot.data_quants[\"jtotal_gsm\"] *= c*c/plasma_freq**3/e  # units of e wpe^3/c^2 or e wpe / de^2\n",
    "    # Scale E and j based on parameter differences between PIC and MMS data\n",
    "    pytplot.data_quants[\"mms1_edp_dce_gsm_brst_l2\"] *= E_corr_factor\n",
    "    pytplot.data_quants[\"jtotal_gsm\"] *= j_corr_factor\n",
    "    \n",
    "    # group the data to get rid of data gaps\n",
    "    next_time_interval = np.diff(pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2_bvec\"].time)\n",
    "    timestep_max = 1.1*np.median(next_time_interval) # bigger than a timestep to avoid float inaccuracy nonsense\n",
    "    pre_gap_idxs = np.nonzero(next_time_interval > timestep_max)\n",
    "    bin_idxs = [0,] + list(pre_gap_idxs[0]) + [len(pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2_bvec\"].time)-1,]  # using real idx instead of -1 to avoid sorting issues\n",
    "    bin_idxs = np.unique(bin_idxs)\n",
    "    groups_B_cots = pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2_bvec\"].groupby_bins(\"time\", bins=pytplot.data_quants[\"mms1_fgm_b_gsm_brst_l2_bvec\"].time[bin_idxs].sortby(\"time\"),\n",
    "                                                                              include_lowest=True)\n",
    "    groups_E_cots = pytplot.data_quants[\"mms1_edp_dce_gsm_brst_l2\"].groupby_bins(\"time\", bins=pytplot.data_quants[\"mms1_edp_dce_gsm_brst_l2\"].time[bin_idxs].sortby(\"time\"),\n",
    "                                                                              include_lowest=True)\n",
    "    groups_j_cots = pytplot.data_quants[\"jtotal_gsm\"].groupby_bins(\"time\", bins=pytplot.data_quants[\"jtotal\"].time[bin_idxs].sortby(\"time\"), include_lowest=True)\n",
    "    # make the data into slices\n",
    "    sliced_B_list=[]\n",
    "    sliced_E_list=[]\n",
    "    sliced_j_list=[]\n",
    "    sliced_time_list=[]\n",
    "    for B_arr, E_arr, j_arr in zip(groups_B_cots, groups_E_cots, groups_j_cots): \n",
    "        if len(B_arr[1].time) > window:\n",
    "            B_slices = np.lib.stride_tricks.sliding_window_view(B_arr[1].values, window, axis=0)[::stride,:,:].copy()\n",
    "            E_slices = np.lib.stride_tricks.sliding_window_view(E_arr[1].values, window, axis=0)[::stride,:].copy()\n",
    "            j_slices = np.lib.stride_tricks.sliding_window_view(j_arr[1].values, window, axis=0)[::stride,:].copy()                                                                                                    \n",
    "            time_slices = np.lib.stride_tricks.sliding_window_view(B_arr[1].time.values, window, axis=0)[::stride,:].copy()\n",
    "\n",
    "            sliced_B_list.append(B_slices)\n",
    "            sliced_E_list.append(E_slices)\n",
    "            sliced_j_list.append(j_slices)\n",
    "            sliced_time_list.append(time_slices)\n",
    "        else:\n",
    "            print(f\"Segment too short for sliding window view with length {len(B_arr[1].time)}\")\n",
    "    # save sliced data\n",
    "    sliced_B = np.concatenate(sliced_B_list, axis=0)\n",
    "    sliced_E = np.concatenate(sliced_E_list, axis=0)\n",
    "    sliced_j = np.concatenate(sliced_j_list, axis=0)\n",
    "    sliced_time = np.concatenate(sliced_time_list, axis=0, dtype='datetime64[us]')\n",
    "    sliced_time = sliced_time.astype(object)\n",
    "    sliced_time = np.vectorize(lambda x: x.strftime('%Y-%m-%dT%H:%M:%S.%f').encode('ascii'))(sliced_time)\n",
    "    # remove data that has nans in it\n",
    "    # check j and E then filter all\n",
    "    clean_j_idxs = ~np.isnan(sliced_j).any(axis=(1,2))\n",
    "    clean_E_idxs = ~np.isnan(sliced_E).any(axis=(1,2))\n",
    "    clean_idxs = np.logical_and(clean_j_idxs, clean_E_idxs)\n",
    "    print(f\"Samples: {sliced_j.shape[0]}  Samples with no NaNs: {np.sum(clean_idxs)}\")\n",
    "\n",
    "    with h5py.File(outfile,'w') as file:\n",
    "        file.create_dataset('B', data=sliced_B[clean_idxs])\n",
    "        file.create_dataset('E', data=sliced_E[clean_idxs])\n",
    "        file.create_dataset('j', data=sliced_j[clean_idxs])\n",
    "        file.create_dataset('time', data=sliced_time[clean_idxs])\n",
    "    \n",
    "    i = loop_end(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432e312-e1e0-4144-b1c6-c8ab4f345253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d01114-fc36-4f42-99b4-8034b581bdee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13639f02-7e2b-4845-9b57-f420cc144bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspedas [~/.conda/envs/pyspedas/]",
   "language": "python",
   "name": "conda_pyspedas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
