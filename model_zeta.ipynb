{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frozen-affiliate",
   "metadata": {},
   "source": [
    "## Simple model using Bx, By, Bz, jy, vz\n",
    " classifications: current sheets, o_structures, null\n",
    " \n",
    " fixed length timeseries informed by d_per_de ~ 4\n",
    " \n",
    "model epsilon but as a binary problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib\n",
    "matplotlib.use('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "# get functions from other notebooks\n",
    "%run /tigress/kendrab/analysis-notebooks/loss_fns.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/metrics.ipynb\n",
    "%run /tigress/kendrab/analysis-notebooks/preproc_utils.ipynb\n",
    "start = dt.datetime.now(dt.timezone.utc)  # for timing\n",
    "startstr = start.strftime(\"%d-%m-%y_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-factory",
   "metadata": {},
   "source": [
    "### Assemble a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"epsilon\"\n",
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "filters = 8\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "padding_length = 10  # amount of data on each side of each segment for additional info\n",
    "stride = 10  # size (and therefore spacing) of each segment\n",
    "input_length = stride + 2*padding_length\n",
    "mask_value = int(-10.0)\n",
    "epochs = 10\n",
    "thinning_factor = [0.99, None]\n",
    "hyperparams = {'learning_rate':learning_rate, 'filters':filters, 'kernel_size':kernel_size, 'pool_size':pool_size,\n",
    "              'padding_length':padding_length, 'stride':stride, 'input_length':input_length, 'epochs':epochs,\n",
    "               'thinning_factor':thinning_factor}\n",
    "\n",
    "# input\n",
    "bx_input = keras.Input(shape=(input_length,1), name=\"bx\") \n",
    "by_input = keras.Input(shape=(input_length,1), name=\"by\") \n",
    "bz_input = keras.Input(shape=(input_length,1), name=\"bz\") \n",
    "jy_input = keras.Input(shape=(input_length,1), name=\"jy\") \n",
    "vz_input = keras.Input(shape=(input_length,1), name=\"vz\") \n",
    "\n",
    "\n",
    "# convolve and pool separately\n",
    "bx_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(bx_input)\n",
    "by_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(by_input)\n",
    "bz_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(bz_input)\n",
    "jy_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(jy_input)\n",
    "vz_conv = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')(vz_input)\n",
    "\n",
    "bx_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(bx_conv)\n",
    "by_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(by_conv)\n",
    "bz_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(bz_conv)\n",
    "jy_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(jy_conv)\n",
    "vz_pool = keras.layers.MaxPooling1D(pool_size=pool_size)(vz_conv)\n",
    "\n",
    "# merge the layers together\n",
    "avg = keras.layers.Average()([bx_pool, by_pool, bz_pool, jy_pool, vz_pool])\n",
    "# convolve and pool\n",
    "avg_conv = keras.layers.Conv1D(filters=2*filters, kernel_size=kernel_size, padding='valid')(avg)\n",
    "avg_pool = keras.layers.MaxPooling1D(pool_size=2)(avg_conv)\n",
    "\n",
    "\n",
    "# use dense layer to output\n",
    "flat_pool = keras.layers.Flatten()(avg_pool)\n",
    "flat_logits = keras.layers.Dense(stride*2, activation='relu')(flat_pool)\n",
    "logits = keras.layers.Reshape((stride, 2))(flat_logits)\n",
    "probs = keras.layers.Softmax()(logits)\n",
    "# throw together the model\n",
    "model = keras.Model(\n",
    "    inputs=[bx_input, by_input, bz_input, jy_input, vz_input],\n",
    "    outputs=[probs])\n",
    "\n",
    "# show the model\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, \"/scratch/gpfs/kendrab/model_\"+model_name+\".png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-justice",
   "metadata": {},
   "source": [
    "### Get 1d sampling (If training/testing only, not building!)\n",
    "Generated by [1d_sampling](./1d_sampling.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use command line args or someting easier than throwing it here\n",
    "readpaths = ['/tigress/kendrab/03082021/'+\"1000samples_idx22_bxbybzjyvz.hdf5\",\n",
    "             '/tigress/kendrab/03082021/'+\"1000samples_idx18_bxbybzjyvz.hdf5\",\n",
    "             '/tigress/kendrab/03082021/'+\"1000samples_idx15_bxbybzjyvz.hdf5\"]\n",
    "\n",
    "bx_list = []\n",
    "by_list = []\n",
    "bz_list = []\n",
    "jy_list = []\n",
    "vz_list = []\n",
    "topo_list = []\n",
    "\n",
    "for filepath in readpaths:\n",
    "    file = h5py.File(filepath, 'r')\n",
    "    bx_list += list(file['bx_smooth'][:])\n",
    "    by_list += list(file['by'][:])\n",
    "    bz_list += list(file['bz_smooth'][:])\n",
    "    jy_list += list(file['jy'][:])\n",
    "    vz_list += list(file['vz'][:])    \n",
    "    topo_list_tmp = list(file['topo'][:])\n",
    "    for i in range(len(topo_list_tmp)):  # I tried to vectorize this but I didn't get it to work\n",
    "        topo_list_tmp[i] = topo_list_tmp[i] % 2  # cat 0,2 are not plasmoids, cat 1,3 are\n",
    "        topo_list_tmp[i] = keras.utils.to_categorical(topo_list_tmp[i], num_classes=2)\n",
    "    topo_list += topo_list_tmp\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-programming",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk into sliding windows (put fn into preproc_utils)\n",
    "# NOTE TOPO HAS DIFFERENT SEGMENT LENGTHS THAN THE INPUTS (stride vs. 2*padding+stride)\n",
    "bx_segs = batch_subsects(bx_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "by_segs = batch_subsects(by_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "bz_segs = batch_subsects(bz_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "jy_segs = batch_subsects(jy_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "vz_segs = batch_subsects(vz_list, input_length, stride).reshape(-1, input_length, 1)\n",
    "topo_segs = batch_unpadded_subsects(topo_list, padding_length, stride)\n",
    "\n",
    "(bx_train, bx_test, by_train, by_test, bz_train, bz_test, jy_train, jy_test, vz_train, vz_test, \n",
    "     topo_train, topo_test) = \\\n",
    "                       train_test_split(bx_segs, by_segs, bz_segs, jy_segs, vz_segs, topo_segs)\n",
    "# try to do some rebalancing in the training set\n",
    "# model is struggling on plasmoids, which are underrepresented\n",
    "[bx_train, by_train, bz_train, jy_train, vz_train], topo_train = \\\n",
    "    rebalance_ctrl_group([bx_train, by_train, bz_train, jy_train, vz_train], topo_train, \n",
    "                         null_label=[1,0], thinning_factor = thinning_factor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-syndication",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# loss_fn = tfa.losses.SigmoidFocalCrossEntropy(gamma=10)  # gamma must be an integer apparently (in int form)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "loss = gen_loss_per_pt(loss_fn=loss_fn)\n",
    "metric = gen_metric_per_cat()\n",
    "metrics = [\"acc\"]  # loss_fn keyword left default\n",
    "# for i in range(4):\n",
    "#     metrics.append(gen_metric_per_cat(mask_layer=mask_layer, cat_idx=i))\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics,\n",
    "             run_eagerly = True)  # run eagerly to get .numpy() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x={'bx': bx_train, 'by': by_train, 'bz': bz_train, 'jy': jy_train, 'vz': vz_train},\n",
    "          y = topo_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-contact",
   "metadata": {},
   "source": [
    "### Observe the results, dump information to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = open(\"/scratch/gpfs/kendrab/model_outs/\"+model_name+'log'+startstr, 'w')\n",
    "\n",
    "with open(\"/scratch/gpfs/kendrab/model_outs/\"+model_name+'log'+startstr, 'w') as log:\n",
    "    log.write(f\"Model {model_name} trained on {startstr}\\n\")\n",
    "    log.write(f\"loss function \\t\\t{loss.name}\\n\")\n",
    "    log.write(\"Hyperparameters:\\n\")\n",
    "    for key in hyperparams.keys():\n",
    "        log.write(f\"{key}\\t\\t{hyperparams[key]}\\n\")\n",
    "        \n",
    "    log.write(\"Training performance\\n\")        \n",
    "    print(\"Training performance\")\n",
    "    topo_pred = model(inputs={'bx': bx_train, 'by': by_train, 'bz': bz_train, 'jy': jy_train, 'vz': vz_train}, training=False)\n",
    "    train_1d = np.argmax(topo_train.reshape(-1,2), axis=1) # for confusion matrix\n",
    "    train_1d_pred = np.argmax(topo_pred.numpy().reshape(-1,2), axis=1)  \n",
    "    num_per_cat = [np.sum(topo_train[...,i] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "    print([np.max(topo_train[:,:,i]) for i in range(2)])\n",
    "    print([np.max(topo_pred[:,:,i]) for i in range(2)])\n",
    "    for i in range(2):\n",
    "        acc = gen_metric_per_cat(cat_idx=i)(tf.convert_to_tensor(topo_train), topo_pred)\n",
    "        log.write(f\"cat{i}recall\\t\\t{acc}\\n\")\n",
    "        print(f\"Category {i} had recall {acc}\")\n",
    "\n",
    "    log.write(\"Testing performance\\n\")\n",
    "    print(\"Testing performance\")\n",
    "    topo_pred = model(inputs={'bx': bx_test, 'by': by_test, 'bz': bz_test, 'jy': jy_test, 'vz': vz_test}, training=False)\n",
    "    test_1d = np.argmax(topo_test.reshape(-1,2), axis=1) # for confusion matrix\n",
    "    test_1d_pred = np.argmax(topo_pred.numpy().reshape(-1,2), axis=1)  \n",
    "    num_per_cat = [np.sum(topo_test[...,i] == 1) for i in range(2)]\n",
    "    log.write(f\"cat_breakdown\\t\\t{num_per_cat}\\n\")\n",
    "    print(f\"cat_breakdown\\t\\t{num_per_cat}\")\n",
    "    print([np.max(topo_test[:,:,i]) for i in range(2)])\n",
    "    print([np.max(topo_pred[:,:,i]) for i in range(2)])\n",
    "    for i in range(2):\n",
    "        acc = gen_metric_per_cat(cat_idx=i)(tf.convert_to_tensor(topo_test), topo_pred)\n",
    "        log.write(f\"cat{i}recall\\t\\t{acc}\\n\")\n",
    "        print(f\"Category {i} had recall {acc}\")    \n",
    "    end = dt.datetime.now(dt.timezone.utc)    \n",
    "    log.write(f\"runtime_seconds\\t\\t{(end-start).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-banner",
   "metadata": {},
   "source": [
    "### Save confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].set(title=\"Training Confusion\")\n",
    "ax[1].set(title=\"Testing Confusion\")\n",
    "cf_train = ConfusionMatrixDisplay(confusion_matrix(train_1d, train_1d_pred, normalize='true'))\n",
    "cf_test = ConfusionMatrixDisplay(confusion_matrix(test_1d, test_1d_pred, normalize='true'))\n",
    "cf_train.plot(ax=ax[0])\n",
    "cf_test.plot(ax=ax[1])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"/scratch/gpfs/kendrab/model_outs/\"+model_name+'cfmatrix'+startstr+\".svg\")\n",
    "plt.close(fig='all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
