{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "under-rebel",
   "metadata": {},
   "source": [
    "### Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "underlying-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvpic\n",
    "from dataframework.src.datasets.vpicdataset import VPICDataset\n",
    "from skimage import measure  # for finding contours\n",
    "from scipy.ndimage import gaussian_filter   # for smoothing/filtering\n",
    "from scipy.spatial import Delaunay, ConvexHull\n",
    "import scipy.interpolate as interp\n",
    "from numpy.polynomial import Polynomial\n",
    "import sys  # for debugging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-automation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PARAMS ADDED, FUNCTIONALIITY NOT ADDED YET!!!! SORRY\n",
      "Added bx Variable\n",
      "Added by Variable\n",
      "Added bz Variable\n",
      "Added jy Variable\n"
     ]
    }
   ],
   "source": [
    "files = ['/tigress/kendrab/03082021/data.h5','tigress/kendrab/03082021/info']\n",
    "kwargs = {'get_vars' : ['bx', 'by', 'bz', 'jy']}\n",
    "data_03082021 = VPICDataset(vpicfiles=files, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "permanent-capacity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PARAMS ADDED, FUNCTIONALIITY NOT ADDED YET!!!! SORRY\n",
      "Added bx Variable\n",
      "Added by Variable\n",
      "Added bz Variable\n",
      "Added jy Variable\n"
     ]
    }
   ],
   "source": [
    "files = ['/tigress/kendrab/02232021/data.h5','tigress/kendrab/02232021/info']\n",
    "data_02232021 = VPICDataset(vpicfiles=files, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-design",
   "metadata": {},
   "source": [
    "### Function definitons ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polyphonic-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PULL MORE STUFF OVER HERE TO STOP REPEATING SO MUCH GD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "saved-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(data):\n",
    "    \"\"\" Flatten the data as structured in datasets to be used with an ND Interpolator from scipy\"\"\"\n",
    "    if len(data.shape) == 1:  # already flat\n",
    "        return data\n",
    "    elif len(data.shape) == 2:  # need to flatten everything\n",
    "        return data.flatten()\n",
    "    elif len(data.shape) == 3:  # need second dimension to be time, first dimension has length (number of points)\n",
    "        return np.column_stack(tuple(data[i].flatten() for i in range(data.shape[0])))\n",
    "    else:\n",
    "        raise ValueError(f\"Data has unacceptable dimension {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "average-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccw(A,B,C):\n",
    "    \"\"\" Test whether the three points are listed in a counterclockwise order, but ~vectorized~\n",
    "    Can't handle colinear points because I'm not handling edge cases rn\n",
    "    A- array, shape (n_pts,2)\n",
    "    B- array, shape (n_pts,2)\n",
    "    C- array, shape (n_pts,2)\n",
    "    \"\"\"\n",
    "    return (C[:,1]-A[:,1])*(B[:,0] - A[:,0]) > (B[:,1] - A[:,1])*(C[:,0] - A[:,0])\n",
    "\n",
    "def intersect_true(A, B, C, D):\n",
    "    \"\"\" Determine whether two line segments AB and CD intersect\n",
    "    A- array, shape (n_pts,2)\n",
    "    B- array, shape (n_pts,2)\n",
    "    C- array, shape (n_pts,2) (optional n_pts = 1)\n",
    "    D- array, shape (n_pts,2) (optional n_pts = 1)\n",
    "    \"\"\"\n",
    "    cond1 = np.logical_not(ccw(A, C, D) == ccw(B, C, D))\n",
    "    cond2 = np.logical_not(ccw(A, B, C) == ccw(A, B, D))\n",
    "    return np.logical_and(cond1, cond2)\n",
    "\n",
    "def line_intersect(A,B,C,D):\n",
    "    \"\"\" Finds the intersection of the lines AB and CD, if it exists\n",
    "    Using https://en.wikipedia.org/wiki/Line%E2%80%93line_intersection#Given_two_points_on_each_line_segment\n",
    "    A- array, shape (n_pts,2)\n",
    "    B- array, shape (n_pts,2)\n",
    "    C- array, shape (n_pts,2)\n",
    "    D- array, shape (n_pts,2)\n",
    "    Typically N_PTS = 1\n",
    "    \"\"\"\n",
    "    denominator = (A[:, 0] - B[:, 0])*(C[:, 1] - D[:, 1]) - (A[:, 1] - B[:, 1])*(C[:, 0] - D[:, 0])\n",
    "    px = ((A[:, 0]*B[:, 1] - A[:, 1]*B[:, 0])*(C[:, 0] - D[:, 0]) \n",
    "          - (A[:, 0] - B[:, 0])*(C[:, 0]*D[:, 1] - C[:, 1]*D[:, 0]))/denominator\n",
    "    py = ((A[:, 0]*B[:, 1] - A[:, 1]*B[:, 0])*(C[:, 1] - D[:, 1]) \n",
    "          - (A[:, 1] - B[:, 1])*(C[:, 0]*D[:, 1] - C[:, 1]*D[:, 0]))/denominator\n",
    "    p = np.stack([px,py], axis=1)\n",
    "    return p\n",
    "\n",
    "def in_idxs(pts, var):\n",
    "    \"\"\" Checks if the indices in pts are with in the Variable var's mesh, for interpolation\n",
    "    E.g. if x mesh has seven values, don't have pts at -0.5 or 6.2\n",
    "    pts- array, shape (n_pts,2)\n",
    "    var- variable whose mesh to use\n",
    "    \n",
    "    Returns:\n",
    "    mask- bool array, shape (n_pts)\n",
    "    \"\"\"\n",
    "    max0 = len(var.mesh[0])\n",
    "    max1 = len(var.mesh[1])\n",
    "    mask = (0 < pts[:,0] < max0) and (0 < pts[:,1] < max1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-preliminary",
   "metadata": {},
   "source": [
    "## X and O point finding ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "computational-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "zooms = [[-np.inf,np.inf], [-np.inf,np.inf]]\n",
    "time_idx = 31 #the time index we are processing rn (testing, will be reformatted nicer someday)\n",
    "smoothing = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-clearing",
   "metadata": {},
   "source": [
    "### Smoothing Bx and Bz ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_03082021.variables['bx'].data = gaussian_filter(data_03082021.variables['bx'].data,[0,smoothing, smoothing])\n",
    "data_03082021.variables['bz'].data = gaussian_filter(data_03082021.variables['bz'].data,[0,smoothing, smoothing])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-formation",
   "metadata": {},
   "source": [
    "### 0 contours of Bx and Bz, zoomed in ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "behind-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate number of grid spacings per 1 de: 4\n"
     ]
    }
   ],
   "source": [
    "bx = data_03082021.variables['bx'].ndslice(zooms=zooms)\n",
    "bz = data_03082021.variables['bz'].ndslice(zooms=zooms)\n",
    "jy = data_03082021.variables['jy'].ndslice(zooms=zooms)\n",
    "smooth_bx_data = gaussian_filter(bx.data[time_idx], smoothing)  # try gaussian filtering in space\n",
    "smooth_bz_data = gaussian_filter(bz.data[time_idx], smoothing)\n",
    "\n",
    "dbx_dz, dbx_dx = np.gradient(smooth_bx_data, *bx.mesh)\n",
    "dbz_dz, dbz_dx = np.gradient(smooth_bz_data, *bz.mesh)\n",
    "fluxfn_hessian_det = dbz_dx*(-dbx_dz) - (-dbx_dx)*dbz_dz\n",
    "topology = np.sign(fluxfn_hessian_det)  # 1 for max or min, -1 for saddle\n",
    "dz_per_de = 1/(bz.mesh[0][1]-bz.mesh[0][0])\n",
    "dx_per_de = 1/(bz.mesh[1][1]-bz.mesh[1][0])\n",
    "d_per_de = int((dz_per_de + dx_per_de)/2)\n",
    "print(\"Approximate number of grid spacings per 1 de:\", d_per_de)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-understanding",
   "metadata": {},
   "source": [
    "### Calculating flux function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "analyzed-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added flux_fn Variable\n"
     ]
    }
   ],
   "source": [
    "data_03082021.calc_fluxfn(b1_name='bz', b2_name='bx')\n",
    "# TODO: make sure you don't have a sign error in the flux function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-drilling",
   "metadata": {},
   "source": [
    "### show those flux contours ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "random-large",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868521e7f1be483c9808bb0da7e33cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e964427ddac24f76bbabfededc2d533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.8080933e-04  8.6337328e-04 -5.4651499e-04 ...  5.4532290e-04\n",
      "   1.0798126e-03 -2.8099120e-04]\n",
      " [ 1.6085505e-03  1.1733770e-03  7.5745583e-04 ...  2.8645992e-04\n",
      "   9.1467798e-04 -1.1689961e-04]\n",
      " [ 1.3196319e-03 -8.1360340e-05  1.0846853e-03 ...  3.6540627e-04\n",
      "   1.8509477e-03  5.3822994e-05]\n",
      " ...\n",
      " [-2.3946166e-05 -7.5009465e-04 -1.2922287e-03 ...  5.0997734e-04\n",
      "   1.1360645e-04 -8.3997846e-05]\n",
      " [-8.0190599e-04 -6.2519312e-04 -7.3575974e-04 ...  8.2826614e-04\n",
      "   3.7474930e-04 -1.7195940e-05]\n",
      " [ 2.1241605e-04 -3.4245849e-04 -9.5653534e-04 ...  6.3806772e-04\n",
      "   3.1952560e-04  2.8610229e-06]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flux_fn = np.zeros_like(bz.data)[0]\n",
    "\n",
    "flux_fn_zoomed = data_03082021.variables['flux_fn'].ndslice(zooms=zooms)\n",
    "X,Y = np.meshgrid(*bx.mesh, indexing='ij')\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "ax.contour(X,Y,flux_fn, levels=400)\n",
    "fig_orig, ax_orig = plt.subplots(figsize=(10,2))\n",
    "ax_orig.contour(X,Y,flux_fn_zoomed.data[time_idx], levels=400)\n",
    "print(flux_fn_zoomed.data[time_idx]-flux_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fluid-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c388a3276e421489007910d3a03d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contour finding using skimage.measure.find_contours\n",
    "zeros_bx = measure.find_contours(smooth_bx_data,0)  # this alg uses LINEAR interpolation, so we will too\n",
    "zeros_bz = measure.find_contours(smooth_bz_data,0)\n",
    "# Plot to visualize\n",
    "print(len(zeros_bx), len(zeros_bz))\n",
    "fig2, ax2 = plt.subplots(3)\n",
    "for contour in zeros_bx:\n",
    "    ax2[0].plot(contour[:,0],contour[:,1])\n",
    "    ax2[2].plot(contour[:,0],contour[:,1], color='r')\n",
    "for contour in zeros_bz:\n",
    "    ax2[1].plot(contour[:,0],contour[:,1])\n",
    "    ax2[2].plot(contour[:,0],contour[:,1], color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-cliff",
   "metadata": {},
   "source": [
    "### Setting up dictionary to hold interpolators of key things ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sweet-precipitation",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There are 2 point arrays, but values has 0 dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e6aef5cc6882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minterps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_pts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegularGridInterpolator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minterps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fluxfn_hessian_det'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegularGridInterpolator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfluxfn_hessian_det\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0minterps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flux_fn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegularGridInterpolator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflux_fn_zoomed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/analysis/lib/python3.7/site-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, points, values, method, bounds_error, fill_value)\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m             raise ValueError(\"There are %d point arrays, but values has %d \"\n\u001b[0;32m-> 2422\u001b[0;31m                              \"dimensions\" % (len(points), values.ndim))\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'astype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: There are 2 point arrays, but values has 0 dimensions"
     ]
    }
   ],
   "source": [
    "default_meshgrid = np.meshgrid(*bx.mesh, indexing='ij')  # needed for LinearNDInterpolator\n",
    "#idx_meshgrid = np.mgrid[0:bx.shape[0], 0:bx.shape[1]]\n",
    "#all_idxs = np.column_stack(tuple(idx_meshgrid[i].flatten() for i in range(len(idx_meshgrid))))\n",
    "#all_pts = np.column_stack(tuple(default_meshgrid[i].flatten() for i in range(len(default_meshgrid))))\n",
    "all_pts = np.stack(default_meshgrid, axis=2) # smush together for interpolating as the data\n",
    "interps = {}\n",
    "idx_mesh = (np.array(range(len(bx.mesh[0]))), np.array(range(len(bx.mesh[1]))))\n",
    "interps['all_pts'] = interp.RegularGridInterpolator(idx_mesh, all_pts)\n",
    "interps['fluxfn_hessian_det'] = interp.RegularGridInterpolator(idx_mesh, fluxfn_hessian_det)\n",
    "interps['flux_fn'] = interp.RegularGridInterpolator(idx_mesh, flux_fn_zoomed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-bidder",
   "metadata": {},
   "source": [
    "### Finding intersections of 0 contours ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chubby-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "[[  46.87904234   81.20138185]\n",
      " [ 114.97886056   67.42360713]\n",
      " [ 318.49994352   75.67419156]\n",
      " [ 210.30322402   56.5862849 ]\n",
      " [ 442.87898093   71.16726903]\n",
      " [ 639.73057951   42.27858263]\n",
      " [ 588.28429281   54.17121359]\n",
      " [ 833.6854744    42.18075265]\n",
      " [ 883.82992453   62.84579568]\n",
      " [1021.79858523   34.09320903]\n",
      " [1071.11759467   34.50547115]\n",
      " [1273.45605448   53.75710861]\n",
      " [1280.51253178   54.40093509]\n",
      " [1379.07706725   94.0737245 ]\n",
      " [1430.31336341   39.78887147]\n",
      " [1542.02943115   79.36864923]\n",
      " [1469.65852019   42.4039745 ]\n",
      " [1464.09212635   39.45679052]\n",
      " [1616.09703992   62.80010873]\n",
      " [1512.6354686   117.5921268 ]\n",
      " [1550.82189342  105.56050568]\n",
      " [4391.58249611   62.56437832]\n",
      " [5379.34847954   66.025706  ]\n",
      " [5487.93240191   65.58871828]\n",
      " [7200.93944316   64.89243169]\n",
      " [7191.34066627   64.71995566]\n",
      " [7189.80073756   64.65643862]\n",
      " [7206.04018472   64.90010716]\n",
      " [7212.80146102   64.86381673]\n",
      " [7239.92367879   60.97792374]\n",
      " [7278.36943798   60.89045593]\n",
      " [7294.40415335   59.48507449]\n",
      " [7412.31079268   93.37724601]\n",
      " [7438.98319144   76.63852332]\n",
      " [7513.60099443   53.10370621]\n",
      " [7551.93398553   63.02999643]\n",
      " [7730.79265274   80.32104545]\n",
      " [7680.90346511   64.46977207]\n",
      " [8055.25046227   85.7431513 ]\n",
      " [7984.99199787   75.96790442]\n",
      " [8273.85461172  104.79643651]\n",
      " [8260.20594843  100.42358049]\n",
      " [8586.85965862   56.91299759]\n",
      " [8518.20169824   31.66567482]\n",
      " [8668.10188393   56.34534265]\n",
      " [8804.45675899   74.04524977]]\n"
     ]
    }
   ],
   "source": [
    "# each contour is an m x 2 array for m some other number depending on the number of points in the contour\n",
    "# break up each contour into m-1 line segments and check if they intersect each other\n",
    "nulls_list = []\n",
    "for contour_x in zeros_bx:  # sigh there probably isn't a better way to do this\n",
    "    endpt_x_1 = contour_x[:-1]\n",
    "    endpt_x_2 = contour_x[1:]\n",
    "    for contour_z in zeros_bz:\n",
    "        endpt_z_1 = contour_z[:-1]\n",
    "        endpt_z_2 = contour_z[1:]\n",
    "        for i in range(endpt_x_1.shape[0]):  # check if the x contour line segments intersect any of the z contour ones\n",
    "            endpt_x_1i = endpt_x_1[i].reshape(-1,2)\n",
    "            endpt_x_2i = endpt_x_2[i].reshape(-1,2)\n",
    "            intersects = np.nonzero(intersect_true(endpt_z_1, endpt_z_2, endpt_x_1i, endpt_x_2i))[0]  # get indices of contour_x which intercept\n",
    "            if len(intersects) != 0:  # only add in points that exist\n",
    "                intersect_pt = line_intersect(endpt_z_1[intersects], endpt_z_2[intersects], endpt_x_1i, endpt_x_2i)\n",
    "                nulls_list.append(intersect_pt)  # DO NOT round intersections to nearest integer\n",
    "        \n",
    "nulls = np.concatenate(nulls_list, axis=0)\n",
    "m_nulls = tuple(np.sign(interps['fluxfn_hessian_det'](nulls[i]))[0] == -1 \n",
    "                for i in range(nulls.shape[0]))  # need to put this here to account for changing topo later\n",
    "p_nulls = tuple(np.sign(interps['fluxfn_hessian_det'](nulls[i]))[0] == 1\n",
    "                for i in range(nulls.shape[0]))\n",
    "\n",
    "ax2[2].scatter(nulls[m_nulls,0], nulls[m_nulls,1], color='black', marker='$m$')\n",
    "ax2[2].scatter(nulls[p_nulls,0], nulls[p_nulls,1], color='black', marker='$p$')\n",
    "print(len(nulls))\n",
    "print(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-office",
   "metadata": {},
   "source": [
    "### Reducing the null points to their clusters and reducing each of those to their median value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exposed-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls after numerical combination: 46\n",
      "Number of nulls after physical combination: 46\n"
     ]
    }
   ],
   "source": [
    "blobs = []  # will hold numpy arrays, each array the coordinates of one 'blob' of nulls\n",
    "adj_idxs = []\n",
    "\"\"\" DON'T USE, OUTDATED!!! NOT GOOD WITH LINEAR INTERPOLATION !!!!\"\"\"\n",
    "\"\"\" Combine the numerically adjacent nulls\"\"\" # NOT NEEDED if sufficiently smooth \n",
    "for new_coord in nulls:  # iterates along the first coordinate, yay\n",
    "    adj_idxs = []  # holds indices of null coordinates directly adjacent to new_coord\n",
    "    for i, blob in enumerate(blobs): \n",
    "        for coord in blob:  # check if new_coord is adjacent to coord in blob\n",
    "            diff = coord - new_coord\n",
    "            if (np.linalg.norm(diff) < 0.5  # make note of adjacent blob (adj. points probably numerical issue)\n",
    "                and np.sign(interps['fluxfn_hessian_det'](coord)) == \n",
    "                np.sign(interps['fluxfn_hessian_det'](new_coord))):  # see if both O or X-points, to combine\n",
    "                adj_idxs.append(i)                                                    \n",
    "                break  # don't want to append i multiple times if it touches multiple coords in a blob  \n",
    "    if len(adj_idxs) > 1:  # adjacent to more than one blob- need to combine blobs!\n",
    "        big_blob = [new_coord.reshape(-1,2)]\n",
    "        for idx in adj_idxs:\n",
    "            big_blob.append(blobs[idx])  # making up the new big blob\n",
    "        np.delete(np.array(blobs, dtype=object), adj_idxs).tolist()\n",
    "        big_blob_arr = np.concatenate(big_blob)\n",
    "        blobs.append(big_blob_arr)\n",
    "    elif len(adj_idxs) == 0:  # not adjacent to any existing blob, makes new blob\n",
    "        blobs.append(new_coord.reshape(-1,2))\n",
    "    elif len(adj_idxs) == 1:  # adjacent to exactly one blob\n",
    "        blobs[adj_idxs[0]] = np.concatenate((blobs[adj_idxs[0]], new_coord.reshape(-1,2)), axis=0)\n",
    "    else:\n",
    "        print(f\"bad size of adj_coords {len(adj_idxs)}\")\n",
    "print(f\"Number of nulls after numerical combination: {len(blobs)}\")\n",
    "\n",
    "\"\"\" Combine physically adjacent blobs, with topology cancellation\"\"\"\n",
    "for i in range(len(blobs)): \n",
    "    for j in range(i+1, len(blobs)):  # no repeats thank you very much\n",
    "        adjacent = False\n",
    "        for coord_i in blobs[i]:\n",
    "            for coord_j in blobs[j]:  # ewwwwwwwww please make this less awful\n",
    "                diff = coord_j - coord_i\n",
    "                if (np.linalg.norm(diff) < 0.5):  # blobs are touching\n",
    "                    adjacent = True\n",
    "                    break\n",
    "            if adjacent:  # already know blobs i and j are touching\n",
    "                break\n",
    "        if adjacent:  # combine the blobs and update the topology\n",
    "            new_topology = (np.sign(interps['fluxfn_hessian_det'](blobs[i][0]))[0] \n",
    "                            + np.sign(interps['fluxfn_hessian_det'](blobs[j][0]))[0])\n",
    "            blobs[i] = np.concatenate([blobs[i], blobs[j]])  # pool all the touching blob points into the first blob\n",
    "            blobs[j] = []  # second blob is redundant now but don't want to mess up for loop\n",
    "trimmed_blobs = list(blob for blob in blobs if not len(blob) == 0)  # get rid of redundant empty blobs   \n",
    "print(\"Number of nulls after physical combination:\", len(trimmed_blobs))\n",
    "for i, blob in enumerate(trimmed_blobs): #take integer version of the average value and have that be the center for now\n",
    "    avg_place = np.average(blob, axis=0)\n",
    "    closest_idx = np.argmin(np.sum((blob - avg_place)**2, axis=1))\n",
    "    trimmed_blobs[i] = blob[closest_idx].reshape(-1,2)  # update list\n",
    "\n",
    "blobs_arr = np.concatenate(trimmed_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-issue",
   "metadata": {},
   "source": [
    "### Separating X and O points ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "democratic-dressing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 2)\n",
      "(23, 2)\n",
      "(0, 2)\n",
      "(0, 2)\n",
      "0.0\n",
      "8879 132\n"
     ]
    }
   ],
   "source": [
    "o_idxs = [np.sign(interps['fluxfn_hessian_det'](blobs_arr[i])[0]) == 1 for i in range(blobs_arr.shape[0])]\n",
    "x_idxs = [np.sign(interps['fluxfn_hessian_det'](blobs_arr[i])[0]) == -1 for i in range(blobs_arr.shape[0])]\n",
    "inconcl_idxs = [np.sign(interps['fluxfn_hessian_det'](blobs_arr[i])[0]) == 0 for i in range(blobs_arr.shape[0])]\n",
    "o_coords = blobs_arr[o_idxs]\n",
    "x_coords = blobs_arr[x_idxs]\n",
    "inconcl_coords = blobs_arr[inconcl_idxs]\n",
    "other_coords = blobs_arr[np.logical_not(np.logical_or.reduce((o_idxs, x_idxs, inconcl_idxs)))]\n",
    "\n",
    "print(o_coords.shape)\n",
    "print(x_coords.shape)\n",
    "print(inconcl_coords.shape)\n",
    "print(other_coords.shape)\n",
    "print(np.max(np.abs(o_coords - nulls[p_nulls,:])))\n",
    "print(max(idx_mesh[0]), max(idx_mesh[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-enlargement",
   "metadata": {},
   "source": [
    "### Plot X and O points on flux contour ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "suitable-breath",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b01ef423cd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLEASE throw these into a function or something, this is nasty :(\n",
    "ax.scatter(*interps['all_pts'](nulls[m_nulls,:]).T, marker='$m$', color='black')\n",
    "ax.scatter(*interps['all_pts'](nulls[p_nulls,:]).T, marker='$p$', color='black')\n",
    "X_opts = interps['all_pts'](o_coords)[:,0]\n",
    "Y_opts = interps['all_pts'](o_coords)[:, 1]\n",
    "X_xpts = interps['all_pts'](x_coords)[:, 0]\n",
    "Y_xpts = interps['all_pts'](x_coords)[:, 1]\n",
    "X_inconclpts = interps['all_pts'](inconcl_coords)[:, 0]\n",
    "Y_inconclpts = interps['all_pts'](inconcl_coords)[:, 1]\n",
    "X_otherpts = interps['all_pts'](other_coords)[:, 0]\n",
    "Y_otherpts = interps['all_pts'](other_coords)[:, 1]\n",
    "ax.scatter(X_opts, Y_opts, marker='o', facecolor='none', edgecolors='r')\n",
    "ax.scatter(X_xpts, Y_xpts, color='r', marker='x')\n",
    "ax.scatter(X_inconclpts, Y_inconclpts, color='r', marker='_')\n",
    "ax.scatter(X_otherpts, Y_otherpts, color='r', marker='$?$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-sculpture",
   "metadata": {},
   "source": [
    "## Designating regions around X and O points, testing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-explanation",
   "metadata": {},
   "source": [
    "### Look at plots with topology / flux function hessian ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latin-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_range = [np.quantile(fluxfn_hessian_det, .01, interpolation='midpoint'),\n",
    "#               np.quantile(fluxfn_hessian_det, .99, interpolation='midpoint')]\n",
    "# fluxfn_hessian_det_nulls = [fluxfn_hessian_det[tuple(blobs_arr[i])] for i in range(blobs_arr.shape[0])]\n",
    "\n",
    "# figtest, axtest = plt.subplots(2, figsize=(8, 6))\n",
    "# axtest[0].imshow(topology.T, aspect='auto', interpolation='none')\n",
    "# hess_img = axtest[1].imshow(fluxfn_hessian_det.T, aspect='auto', interpolation='none', vmin=value_range[0],\n",
    "#                 vmax = value_range[1])\n",
    "# figtest.colorbar(hess_img)\n",
    "# for axes in axtest:\n",
    "#     axes.scatter(o_coords[:,0], o_coords[:,1], color='r', marker='o')\n",
    "#     axes.scatter(x_coords[:,0], x_coords[:,1], color='r', marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-lambda",
   "metadata": {},
   "source": [
    "### Try constant flux contours from x points and overplot w/ B, current density ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "healthy-roman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9bede29ea042d390a34d561d13895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boundaries = []\n",
    "for i in range(x_coords.shape[0]):  # I love nested for loops...\n",
    "    xline_contours = measure.find_contours(flux_fn_zoomed.data[time_idx], \n",
    "                                           level=interps['flux_fn'](x_coords[i]))\n",
    "    # find_contours returns list, let's make a list of lists\n",
    "    boundaries.append(xline_contours)\n",
    "\n",
    "\n",
    "fig_j, ax_j = plt.subplots()    \n",
    "ax_j.imshow(jy.data[time_idx].T, aspect='auto', interpolation='none')\n",
    "for level in boundaries:\n",
    "    for contour in level:\n",
    "        bdy_points = interps['all_pts'](contour)\n",
    "        ax.plot(bdy_points[:,0], bdy_points[:,1], linestyle='dashed')\n",
    "        ax_j.plot(contour[:,0], contour[:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-thirty",
   "metadata": {},
   "source": [
    "### Try flux fn. Hessian structure def ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "artificial-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_types = [o_coords, x_coords]\n",
    "# colors = ['black','blue']\n",
    "# paths = [[1,0], [1,1], [0,1], [1,-1]]\n",
    "# paths = [np.array(paths[i]) for i in range(len(paths))]  # make np arrays\n",
    "# meshgrids = np.mgrid[0:topology.shape[0], 0:topology.shape[1]]\n",
    "\n",
    "# for k, null_type in enumerate(null_types):\n",
    "#     structure_masks = []\n",
    "#     point_list = []\n",
    "#     for null in null_type:\n",
    "#         print('begin', null)\n",
    "#         pts = np.empty((2*len(paths), 2))\n",
    "#         no_steps = True\n",
    "#         for i, path in enumerate(paths): \n",
    "#             idx = null.copy()\n",
    "#             max_idx0 = topology.shape[0]\n",
    "#             max_idx1 = topology.shape[1]\n",
    "#             l=0\n",
    "#             while ((topology[tuple(idx)] == topology[tuple(null)])\n",
    "#                    and max(-path[0], 0) <= idx[0] < min(max_idx0 - path[0], max_idx0)\n",
    "#                    and max(-path[1], 0) <= idx[1] < min(max_idx1 - path[1], max_idx1)):  # wont run off the grid\n",
    "#                 no_steps = False\n",
    "#                 idx += path\n",
    "#                 if l < 10000:\n",
    "#                    l += 1\n",
    "#                 else:\n",
    "#                    raise RuntimeError(\"Loop goes too long\")\n",
    "#             pts[i] = idx.reshape(-1,2).copy()\n",
    "#             idx = null.copy()\n",
    "#             l=0\n",
    "#             while ((topology[tuple(idx)] == topology[tuple(null)])\n",
    "#                    and max(path[0],0) <= idx[0] < min(max_idx0 + path[0], max_idx0)\n",
    "#                    and max(path[1],0) <= idx[1] < min(max_idx1 + path[1], max_idx1)):\n",
    "#                 no_steps = False\n",
    "#                 idx -= path\n",
    "#                 if l < 10000:\n",
    "#                    l += 1\n",
    "#                 else:\n",
    "#                    raise RuntimeError(\"Loop goes too long\")\n",
    "#             pts[-1-i] = idx.reshape(-1,2).copy()\n",
    "#         _, degeneracies = np.unique(pts, return_counts = True)\n",
    "#         degenerate = max(degeneracies) >= len(paths)*2\n",
    "#         if not (no_steps or degenerate):  # we went somewhere yay\n",
    "#             hull = Delaunay(pts)\n",
    "#             point_list.append(pts)\n",
    "#             all_pts = np.stack(meshgrids, axis=2)\n",
    "#             structure_mask = np.logical_and(hull.find_simplex(all_pts) > 0, topology == topology[tuple(null)])\n",
    "#             structure_masks.append(structure_mask) \n",
    "#         print('end ', null)  # need to visualize that this thing is actually doing something\n",
    "    \n",
    "#     structure_masks_arr = np.stack(structure_masks, axis=2)\n",
    "#     all_structure_map = np.sign(np.sum(structure_masks_arr, axis=2))\n",
    "#     print(all_structure_map.shape)\n",
    "#     for pt in point_list:\n",
    "#         pt_approx = pt.astype(int)\n",
    "#         hull = ConvexHull(pt_approx)\n",
    "#         vertices = pt_approx[hull.vertices]\n",
    "#         x_border = X[tuple(vertices.T)]\n",
    "#         y_border = Y[tuple(vertices.T)]\n",
    "#         ax.fill(x_border, y_border, color = colors[k], alpha=0.3)\n",
    "#     print(\"plotted\")\n",
    "# plt.show() \n",
    "# fig.savefig(\"outs/2d_test.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-cinema",
   "metadata": {},
   "source": [
    "### Testing are for structure finding methods etc. implemented into the VPICDataset object ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pretty-equation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added bz_smooth Variable\n",
      "Added bx_smooth Variable\n",
      "Original number of nulls:  46\n",
      "Number of nulls after numerical combination: 46\n",
      "Number of nulls after physical combination: 45\n",
      "parameter x_coords = [[ 114   67]\n",
      " [ 318   75]\n",
      " [ 588   54]\n",
      " [ 833   42]\n",
      " [1021   34]\n",
      " [1273   53]\n",
      " [1379   94]\n",
      " [1542   79]\n",
      " [1464   39]\n",
      " [1550  105]\n",
      " [4391   62]\n",
      " [5487   65]\n",
      " [7206   64]\n",
      " [7239   60]\n",
      " [7294   59]\n",
      " [7438   76]\n",
      " [7551   63]\n",
      " [7730   80]\n",
      " [8055   85]\n",
      " [8273  104]\n",
      " [8586   56]\n",
      " [8804   74]]\n",
      "parameter o_coords = [[  46   81]\n",
      " [ 210   56]\n",
      " [ 442   71]\n",
      " [ 639   42]\n",
      " [ 883   62]\n",
      " [1071   34]\n",
      " [1280   54]\n",
      " [1430   39]\n",
      " [1469   42]\n",
      " [1616   62]\n",
      " [1512  117]\n",
      " [5379   66]\n",
      " [7200   64]\n",
      " [7212   64]\n",
      " [7278   60]\n",
      " [7412   93]\n",
      " [7513   53]\n",
      " [7680   64]\n",
      " [7984   75]\n",
      " [8260  100]\n",
      " [8518   31]\n",
      " [8668   56]]\n",
      "Added topology Variable\n",
      "NOTE: Not actually calculating the structures yet!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "desired_time = data_03082021.timeseries[time_idx]\n",
    "dt = data_03082021.timeseries[1] - data_03082021.timeseries[0]\n",
    "test_dset = data_03082021.ndslice(timelims=[desired_time - dt/2, desired_time + dt/2], zooms=zooms)\n",
    "test_dset.find_structures(b1_name='bz', b2_name='bx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-edinburgh",
   "metadata": {},
   "source": [
    "## Try slapdash ML thing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-substance",
   "metadata": {},
   "source": [
    "### Handpicking slices through structures (bad practice, just for this test) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before making each slice randomly shuffle the endpoints\n",
    "used_time = 31  # in case I change it up above, make sure I remember what I used here\n",
    "slices_dict = {'cs':[[[-40,-5],[20,4]],\n",
    "                     [[-10,10],[-12,-7]],\n",
    "                     [[225,1,],[245,1]],\n",
    "                     [[230,9],[235,-8]],\n",
    "                     [[227,-6],[234,-5]],\n",
    "                     [[-985,-2],[-965,5]],\n",
    "                     [[-982,3],[-965,-1]],\n",
    "                     [[-975,-9],[-971,9]],\n",
    "                     [[-935,-5],[-933,9]],\n",
    "                     [[-940,5],[-920,4]],\n",
    "                     [[-925,6],[-930,-5]],\n",
    "                     [[-860,-5],[-870,-4]],\n",
    "                     [[-867,-7],[-865,4]],\n",
    "                     [[-816,0],[-808,1]],\n",
    "                     [[-813,8],[-814,-5]],\n",
    "                     [[-778,-5],[-774,7]],\n",
    "                     [[-764,-6],[-773,-5]],\n",
    "                     [[-697,-3],[-682,2]],\n",
    "                     [[-690,8],[-689,0]],\n",
    "                     [[-687,-2],[-685,7]],\n",
    "                     [[-675,-6],[-670,-6]],\n",
    "                     [[630,-5],[632,3]],\n",
    "                     [[627,0],[633,-1]],\n",
    "                     [[640,7],[644,-8]],\n",
    "                     [[642,5],[647,6]],\n",
    "                     [[670,3],[681,-3]],\n",
    "                     [[674,-4],[675,8]],\n",
    "                     [[706,-3],[702,1]],\n",
    "                     [[735,-5],[750,0]],\n",
    "                     [[735,0],[747,5]],\n",
    "                     [[738,8],[744,-7]],\n",
    "                     [[863,5],[869,9]],\n",
    "                     [[927,-7],[940,4]],\n",
    "                     [[940,-5],[932,7]],\n",
    "                     [[975,3],[990,8]],\n",
    "                     [[985,7],[989,-8]]],\n",
    "              'fr':[[[208,-6],[217,-2]],\n",
    "                    [[196,-7],[221,7]],\n",
    "                    [[211,-8],[213,-7]],\n",
    "                    [[207,9],[210,-6]],\n",
    "                    [[-994,7],[-978,-3]],\n",
    "                    [[-993,-6],[-986,9]],\n",
    "                    [[-965,-5],[-940,4]],\n",
    "                    [[-964,0],[-945,8]],\n",
    "                    [[-955,7],[-950,-8]],\n",
    "                    [[-900,-5],[-896,7]],\n",
    "                    [[-902,7],[-897,-6]],\n",
    "                    [[-862,-7],[-848,-6]],\n",
    "                    [[-858,5],[-852,-8]],\n",
    "                    [[-808,0],[-786,1]],\n",
    "                    [[-790,8],[-794,-7]],\n",
    "                    [[-685,-1],[-675,-9]],\n",
    "                    [[-670,-9],[-662,1]],\n",
    "                    [[-645,0],[-625,-1]],\n",
    "                    [[610,-6],[623,6]],\n",
    "                    [[609,0],[623,-3]],\n",
    "                    [[640,-7],[636,6]],\n",
    "                    [[635,-1],[641,1]],\n",
    "                    [[688,2],[700,-7]],\n",
    "                    [[690,-5],[695,3]],\n",
    "                    [[715,1],[755,-7]],\n",
    "                    [[725,-8],[721,6]],\n",
    "                    [[787,0],[800,3]],\n",
    "                    [[865,2],[855,9]],\n",
    "                    [[850,3],[864,4]],\n",
    "                    [[948,-3],[965,-4]],\n",
    "                    [[950,-9],[954,7]]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-dynamics",
   "metadata": {},
   "source": [
    "### Taking slices, trying various features, conglomerating them into datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2d = data_03082021.ndslice(zooms=zooms)\n",
    "dx = data_2d.default_mesh[0][1] - data_2d.default_mesh[0][0]\n",
    "eps = dx/2  # to make sure we have some data that exists\n",
    "full_dset=[]\n",
    "values=[]\n",
    "for key in slices_dict.keys():\n",
    "    for ndslice in slices_dict[key]:\n",
    "        print(ndslice)\n",
    "        values.append(key)\n",
    "        window_0 = np.array([min(ndslice[0][0], ndslice[1][0])-eps, max(ndslice[0][0], ndslice[1][0])+eps])\n",
    "        window_1 = np.array([min(ndslice[0][1], ndslice[1][1])-eps, max(ndslice[0][1], ndslice[1][1])+eps])\n",
    "        set_pts = [np.array(ndslice[0]), np.array(ndslice[1])]\n",
    "        zoom_sample = [window_0, window_1]\n",
    "        sample = data_2d.ndslice(zooms=zoom_sample, set_pts=set_pts)\n",
    "        sample_len = max(sample.default_mesh[0]) - min(sample.default_mesh[0])\n",
    "        mesh_normed = (sample.default_mesh[0] - sample.default_mesh[0][0])/(sample_len/2) - 1\n",
    "        \"\"\"calculate some features to use in the classifier\"\"\"\n",
    "        bx = sample.variables['bx'].data[used_time]\n",
    "        by = sample.variables['by'].data[used_time]\n",
    "        bz = sample.variables['bz'].data[used_time]\n",
    "        jy_avg = np.average(sample.variables['jy'].data[used_time])\n",
    "        b_mag_avg = np.average(np.sqrt(bx*bx+by*by+bz*bz))\n",
    "        \n",
    "        bx_norm_fit = Polynomial.fit(mesh_normed, bx/b_mag_avg, deg=5)\n",
    "        by_norm_fit = Polynomial.fit(mesh_normed, by/b_mag_avg, deg=5)\n",
    "        bz_norm_fit = Polynomial.fit(mesh_normed, bz/b_mag_avg, deg=5)\n",
    "        \n",
    "        features = np.array([jy_avg] + [b_mag_avg] + list(bx_norm_fit) + list(by_norm_fit) + list(bz_norm_fit))\n",
    "        full_dset.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dset_arr = np.vstack(full_dset)\n",
    "print(full_dset_arr.shape)\n",
    "print(len(values))\n",
    "np.savetxt(\"testdata.txt\", np.array([list(full_dset_arr[i])+[values[i]] for i in range(len(values))]), fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-retailer",
   "metadata": {},
   "source": [
    "### Splitting the data up and trying out the RFC ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_dset_arr, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10000, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-import",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
