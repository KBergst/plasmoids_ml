{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "productive-appreciation",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "Going to put any custom loss functions I need to make over the course of building models for this research here. If it gets to be too many, I'll keep them in separate notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh(obj):  # clear the state of the loss function / metric if it holds onto it for some damn reason\n",
    "    try:\n",
    "        obj.reset_state()\n",
    "        print('reset state')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        obj.reset_states()\n",
    "        print('reset state')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-professional",
   "metadata": {},
   "source": [
    "### CategoricalCrossEntropy for each point in a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catcrossentropy_per_pt(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    A metric for a series of datapoints, each of which needs classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: tensorflow tensor of shape (batch size, series length, num_categories)\n",
    "        The true values to compare with. For datapoint in the series,\n",
    "        the category information should be one-hot encoded\n",
    "    y_pred: tensorflow tensor of shape (batch size, series length, num_categories)\n",
    "        The predicted values. For datapoint in the series,\n",
    "        the category information should be expressed in probabilities (fractions of 1)    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensorflow tensor of shape (1,)\n",
    "        The categorical cross entropy for each datapoint in each series, summed\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_fn = CategoricalCrossentropy()\n",
    "    \n",
    "    losses = tf.zeros(shape=(1,))\n",
    "\n",
    "    for i in range(int(tf.shape(y_true)[1])):  # loop over every datapoint in the series\n",
    "        loss = loss_fn(y_true[:,i,:], y_pred[:,i,:])\n",
    "        losses += loss\n",
    "        \n",
    "    losses /= int(tf.shape(y_true)[1])\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-championship",
   "metadata": {},
   "source": [
    "### generator function to handle masked sequential data (OUT OF DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss_per_pt(loss_fn=CategoricalCrossentropy(), mask_layer=None):\n",
    "    \"\"\"\n",
    "    A generator function for series' of datapoints that returns a loss function\n",
    "    which takes into account a mask of the inputs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_fn : loss-type class object\n",
    "        loss function to use per each data point\n",
    "    mask_layer : layer.Masking object\n",
    "        masking layer used to throw out padding datapoints\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss_per_pt : function\n",
    "        The generated loss function taking into account the values of loss_fn and mask_layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_per_pt(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        A metric for a series of datapoints, each of which needs its own separate evaluation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: tensorflow tensor of shape (batch size, series length, num_categories)\n",
    "            The true values to compare with. For datapoint in the series,\n",
    "            the category information should be one-hot encoded\n",
    "            SHOULD BE MASKED AS PER MASK_LAYER'S EXPECTATIONS\n",
    "        y_pred: tensorflow tensor of shape (batch size, series length, num_categories)\n",
    "            The predicted values. For datapoint in the series,\n",
    "            the category information should be expressed in probabilities (fractions of 1)    \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: tensorflow tensor of shape (1,)\n",
    "            The loss for each datapoint in each series, summed\n",
    "\n",
    "        \"\"\"\n",
    "        losses = tf.zeros(shape=(1,))\n",
    "        n_points = tf.shape(y_true)[1]\n",
    "        if mask_layer is not None:\n",
    "            mask = mask_layer.compute_mask(y_true)\n",
    "            for i in range(n_points):  # loop over every datapoint in the series\n",
    "                y_t = tf.boolean_mask(y_true[:,i,:], mask[:,i])\n",
    "                y_p = tf.boolean_mask(y_pred[:,i,:], mask[:,i]) \n",
    "                refresh(loss_fn)\n",
    "                loss = loss_fn(y_t, y_p)\n",
    "                if not loss.shape: # fn returned loss for each point\n",
    "                    loss = sum(loss)\n",
    "                    # we need to do it this way because scalar tensors apparently have a __len__\n",
    "                    # but you can't call len(scalar_tensor)\n",
    "                losses += loss        \n",
    "        else:\n",
    "            for i in range(n_points):  # loop over every datapoint in the series\n",
    "                refresh(loss_fn)\n",
    "                loss = loss_fn(y_true[:,i,:], y_pred[:,i,:])\n",
    "                if not loss.shape:\n",
    "                    loss = sum(loss)\n",
    "                losses += loss\n",
    "        # normalize by number of datapoints\n",
    "        losses /= tf.cast(n_points, losses.dtype)\n",
    "        \n",
    "        return losses\n",
    "    loss_per_pt.name = loss_fn.name + \"_per_pt\"\n",
    "    \n",
    "    return loss_per_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-florence",
   "metadata": {},
   "source": [
    "### Loss per point class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPerPt:\n",
    "    \"\"\"\n",
    "    A class for losses for series' of datapoints\n",
    "    which take into account a mask of the inputs and possible weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_fn : loss-type class object\n",
    "        loss function to use per each data point\n",
    "    mask_layer : layer.Masking object\n",
    "        masking layer used to throw out padding datapoints\n",
    "    class_weights : dict, default None\n",
    "        dictionary specifying weights for the different categories. Keys should be the one-hot index of the\n",
    "        category (e.g. 0, 1, 2, ...), as an integer. Values should be floats.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn=CategoricalCrossentropy(), mask_layer=None, class_weights=None):\n",
    "        \"\"\" Initialize the class. NOTE tensors >2 dim do NOT support class_weights called from model.fit\n",
    "        So we initialize them here specifically.\"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.mask_layer = mask_layer\n",
    "        self.class_weights = class_weights\n",
    "        self.__name__ = loss_fn.name + \"_per_pt\"\n",
    "    \n",
    "    def __call__(self, y_true, y_pred, sample_weights = None):\n",
    "        \"\"\"\n",
    "        A metric for a series of datapoints, each of which needs its own separate evaluation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: tensorflow tensor of shape (batch size, series length, num_categories)\n",
    "            The true values to compare with. For datapoint in the series,\n",
    "            the category information should be one-hot encoded\n",
    "            SHOULD BE MASKED AS PER MASK_LAYER'S EXPECTATIONS\n",
    "        y_pred: tensorflow tensor of shape (batch size, series length, num_categories)\n",
    "            The predicted values. For datapoint in the series,\n",
    "            the category information should be expressed in probabilities (fractions of 1)  \n",
    "        sample_weight: the samplewise weighting desired. Scalar or (batch_size,...)\n",
    "        Returns\n",
    "        -------\n",
    "        loss: tensorflow tensor of shape (1,)\n",
    "            The loss for each datapoint in each series, summed\n",
    "\n",
    "        \"\"\"\n",
    "        losses = tf.zeros(shape=(1,))\n",
    "        n_points = tf.shape(y_true)[1]\n",
    "        full_weights = self._full_sample_weights(y_true, sample_weights)\n",
    "        \n",
    "        if self.mask_layer is not None:\n",
    "            mask = self.mask_layer.compute_mask(y_true)\n",
    "            for i in range(n_points):  # loop over every datapoint in the series\n",
    "                y_t = tf.boolean_mask(y_true[:,i,:], mask[:,i])\n",
    "                y_p = tf.boolean_mask(y_pred[:,i,:], mask[:,i]) \n",
    "                wts = tf.boolean_mask(wts[:,i], mask[:,i]) \n",
    "                refresh(self.loss_fn)\n",
    "                loss = self.loss_fn(y_t, y_p, sample_weight = wts)\n",
    "                if not loss.shape: # fn returned loss for each point\n",
    "                    loss = sum(loss)\n",
    "                    # we need to do it this way because scalar tensors apparently have a __len__\n",
    "                    # but you can't call len(scalar_tensor)\n",
    "                losses += loss        \n",
    "        else:\n",
    "            for i in range(n_points):  # loop over every datapoint in the series\n",
    "                refresh(self.loss_fn)\n",
    "                loss = self.loss_fn(y_true[:,i,:], y_pred[:,i,:], sample_weight = full_weights[:,i])\n",
    "                if not loss.shape:\n",
    "                    loss = sum(loss)\n",
    "                losses += loss\n",
    "        # normalize by number of datapoints\n",
    "        losses /= tf.cast(n_points, losses.dtype)\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    \n",
    "    def _full_sample_weights(self, y_true, sample_weights):\n",
    "        \"\"\" Should work for nd tensors, because numpy\"\"\"\n",
    "        \n",
    "        full_weights = np.ones_like(y_true[...,0]) # default even weights\n",
    "\n",
    "        if sample_weights is not None:  # use sample weights\n",
    "            if not hasattr(sample_weights, \"__len__\"):  # scalar boye to batch-length array\n",
    "                sample_weights = np.full(y_true.shape[0], sample_weights)\n",
    "            new_dims = y_true.ndim - sample_weights.ndim - 1  # don't need one-hot dimension\n",
    "            # reshape to correct number of dimensions\n",
    "            sample_weights_nd = sample_weights.copy().reshape(sample_weights.shape + tuple(1 for i in range(new_dims)))\n",
    "            # tile to correct number of reps per dimension\n",
    "            sample_weights_nd = np.tile(sample_weights_nd, tuple(1  for i in range(sample_weights.ndim)) + y_true.shape[sample_weights.ndim:-1])\n",
    "            full_weights *= sample_weights_nd\n",
    "            \n",
    "        if self.class_weights is not None:  # use class weights\n",
    "            class_weights_nd = np.zeros_like(full_weights)\n",
    "            for cls_num, weight in self.class_weights.items():\n",
    "                class_weights_nd += weight*y_true[..., cls_num] \n",
    "            full_weights *= class_weights_nd\n",
    "            \n",
    "        return full_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
